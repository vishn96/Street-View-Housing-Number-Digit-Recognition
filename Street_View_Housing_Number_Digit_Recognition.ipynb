{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Street View Housing Number Digit Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOp3avihmzx6D4nGqm2Mv6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishn96/Street-View-Housing-Number-Digit-Recognition/blob/main/Street_View_Housing_Number_Digit_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cKm6LfN_J_J",
        "outputId": "e130bd42-cae2-4ca8-e537-53f56b44d3c9"
      },
      "source": [
        "# Mounting Google Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awwQsqSb_ovm"
      },
      "source": [
        "# Setting the current working directory\r\n",
        "import os; os.chdir('drive/My Drive/PGP/Neural_Network')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaHX3pUgACQd",
        "outputId": "2e781fdb-61d2-4bd0-ed39-a47fcd45e501"
      },
      "source": [
        "# Imports\r\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, h5py\r\n",
        "import matplotlib.style as style; style.use('fivethirtyeight')\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Metrics and preprocessing\r\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn import preprocessing\r\n",
        "\r\n",
        "# TF and Keras\r\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\r\n",
        "from tensorflow.keras.layers import Activation, Dense\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras import optimizers\r\n",
        "\r\n",
        "# Checking if GPU is found\r\n",
        "import tensorflow as tf\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  raise SystemError('GPU device not found')\r\n",
        "print('Found GPU at: {}'.format(device_name))\r\n",
        "\r\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnLNoWkiAOR7",
        "outputId": "2ab58486-fe35-4ef1-98cf-8f9df156ee78"
      },
      "source": [
        "!ls '/content/drive/My Drive/PGP/Neural_Network'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVHN_single_grey1.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwRM3ky9A9pa",
        "outputId": "d8d6fe87-a4fd-48a4-d888-b4a3013e6267"
      },
      "source": [
        "# Read the h5 file\r\n",
        "h5_SVH = h5py.File('SVHN_single_grey1.h5', 'r')\r\n",
        "\r\n",
        "# Load the training, validation and test sets\r\n",
        "X_train = h5_SVH['X_train'][:]\r\n",
        "y_train_o = h5_SVH['y_train'][:]\r\n",
        "X_val = h5_SVH['X_val'][:]\r\n",
        "y_val_o = h5_SVH['y_val'][:]\r\n",
        "X_test = h5_SVH['X_test'][:]\r\n",
        "y_test_o = h5_SVH['y_test'][:]\r\n",
        "\r\n",
        "# Close this file\r\n",
        "\r\n",
        "h5_SVH.close()\r\n",
        "\r\n",
        "print('Training set', X_train.shape, y_train_o.shape)\r\n",
        "print('Validation set', X_val.shape, y_val_o.shape)\r\n",
        "print('Test set', X_test.shape, y_test_o.shape)\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "print('Unique labels in y_train:', np.unique(y_train_o))\r\n",
        "print('Unique labels in y_val:', np.unique(y_val_o))\r\n",
        "print('Unique labels in y_test:', np.unique(y_test_o))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (42000, 32, 32) (42000,)\n",
            "Validation set (60000, 32, 32) (60000,)\n",
            "Test set (18000, 32, 32) (18000,)\n",
            "\n",
            "\n",
            "Unique labels in y_train: [0 1 2 3 4 5 6 7 8 9]\n",
            "Unique labels in y_val: [0 1 2 3 4 5 6 7 8 9]\n",
            "Unique labels in y_test: [0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "GKJcb1dbBD0K",
        "outputId": "1d041e9b-f2d1-43b0-9207-f9d8578d439a"
      },
      "source": [
        "# Visualizing first 10 images in the dataset and their labels\r\n",
        "plt.figure(figsize = (15, 4.5))\r\n",
        "for i in range(10):  \r\n",
        "    plt.subplot(1, 10, i+1)\r\n",
        "    plt.imshow(X_train[i].reshape((32, 32)),cmap = plt.cm.binary)\r\n",
        "    plt.axis('off')\r\n",
        "\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print('Label for each of the above image: %s' % (y_train_o[0 : 10]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x324 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "N6pMUqJeCz77",
        "outputId": "55d242d2-d723-44a1-8853-fea2d5ec5a65"
      },
      "source": [
        "print('Checking first image and label in training set'); print('--'*40)\r\n",
        "plt.imshow(X_train[0], cmap = plt.cm.binary)    \r\n",
        "plt.show()\r\n",
        "print('Label:', y_train_o[0])\r\n",
        "\r\n",
        "print('Checking first image and label in validation set'); print('--'*40)\r\n",
        "plt.imshow(X_val[0], cmap = plt.cm.binary)    \r\n",
        "plt.show()\r\n",
        "print('Label:', y_val_o[0])\r\n",
        "\r\n",
        "print('Checking first image and label in test set'); print('--'*40)\r\n",
        "plt.imshow(X_test[0], cmap = plt.cm.binary)    \r\n",
        "plt.show()\r\n",
        "print('Label:', y_test_o[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking first image and label in training set\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label: 2\n",
            "Checking first image and label in validation set\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label: 0\n",
            "Checking first image and label in test set\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAczElEQVR4nO2dfYxc1XnGnxdjPuK1BzDYWWwLO2ShcZziEkONbBFIbAQIySSKolCFJApVo4pISaB/oCC1tClS0hb4C/UjMgqQNCltSEAWtNjIwTIqCRgbWKDx2msXvKx38dfahtiAOf1j7pnOzp732Zm7O7Pk5PlJq73znjn3nj33vntn3ue+77EQAoQQeXLSVA9ACNE+5OBCZIwcXIiMkYMLkTFycCEy5uR27XhkZETheSE6SKVSsUbbhO7gZna1mf3GzHaY2W0T2ZcQYvIp7eBmNg3AvQCuAbAYwA1mtniyBiaEmDgT+Yh+KYAdIYR+ADCznwJYA+CVxje+/vrrAIBjx47htNNOAwAcPHjQ3fE777yTtLM+O3fudNuOHj066vWaNWvwyCOPAABmz57t9otjbWRwcLDpY9VzzjnnjHq9atUqbNiwYdxxLFy40G0zG/OpDEB1rj327ds36vWSJUvQ29sLAJg2bZrbz5sPADh+/PiEx1E/H9u3b3f77dq1y207/fTTk/YTJ064ffbu3Tvq9dq1a3HTTTcBAIaHh91+3t8MAL/97W/dNu+cvf/++6Neb968GStXrgQAnHRS+l7MrkUAsLJPspnZ5wFcHUL40+L1jQD+OITwDWD0d/C+vr5SxxBCcHp6emrbqe/gbQuy1RP/8+sOrjs4G4fu4JN/B59IkG0AwIK61/MLmxDiA8JE7uDPAugxs0WoOvYXAfxJ8iAnnzxm+9RTT3V37H1tqN9PI95/biD93zu+/5RTTnH7vffee0n74cOH3T6HDh1y2z70oQ+Nsb311lsAgDlz5rj92Fy9/fbbSfvQ0JDbp/GutGTJktqdYObMmW6/6dOnu20e7777bst9xoNdB96dzjuX48E+0bA2dl218rU47ocdi1HawUMI75nZNwD8F4BpAO4LIbxcdn9CiMlnQt/BQwiPAXhsksYihJhk9KiqEBkjBxciY+TgQmSMHFyIjOnIgy710kXc9uQMwJdWmNTBJJyUzBRtTILyxsHG7j3EAKTlumhrfMihHiareHPCHrQYGRlxbUxuZH+bNyds7Cm5K9qYFDZr1iy3zXtIyrMD6bmPtlbPZ6TMXKWI8hjbH0N3cCEyRg4uRMbIwYXIGDm4EBkjBxciY6Ysis6i1150kiUusIhmKoIabSzlj+3Tg0WNU9HTaGOpmCzRwEvHZEkvqQSVaGN/MxtHmbliUXTGjBkz3DZvjEeOHCk1jjLJSACfD+8aSfWJ0XOmsjB0BxciY+TgQmSMHFyIjJGDC5ExcnAhMkYOLkTGdLSqav02C/t7dcFSSRIRJq+wZBMmg3hS3plnnun2YVJSqu5atDVWXK2HyTGeHMZkoVgHLmVj88jkKW+MLMmDyXXsb2ZtXiIHS6JJSaXxOi0j/43Xz7v2U/ZYx4/JuQzdwYXIGDm4EBkjBxciY+TgQmSMHFyIjJGDC5ExHZHJ6iWKuM1khLlz5ybtb775ptuHLUyYyt6JWVgf/vCH3X7eon+sPtYbb7zhtqXqxkUbk9fKZLwxSZHJU+xvq1QqbpuX1cbGzsbB5Dq2T0+WY/PLarIxyiy/xdpScx8l47JLQE3Iwc1sN4AjAE4AeC+EsGwi+xNCTC6TcQe/MoSwb/y3CSE6jb6DC5Ex1spSpmM6m+0CcBBAAPDPIYR/iW0jIyO1Hff19U1kjEIIh56entp2pVIZ8yV+oh/RV4YQBsxsDoD1ZvY/IYRNjW9atGgRAGDXrl217aNHj7o79QJHvb29bp+XX/ZXLm78J3bVVVfhiSeeAOAH0lhbf3+/24cF2Rqfsb/kkkvw7LPPAhh9ohphAcndu3cn7Vu3bnX7NAbgbr31Vtx1110AgMsuu8ztt2yZH2Lx1iPfvn2726fxefnrr78ev/jFLwCMXcO8nv3797ttXpCN3cgag7f33nsvbr75ZgB8AQn2TPxkBNkefPBB3HjjjQCAAwcOJPuw+QUm+BE9hDBQ/B4G8HMAl05kf0KIyaX0HdzMZgA4KYRwpNi+CsDfpN5bn70Ut5mM47WxPkzeSbXFrCMmx3hSCcuqYsUTU1ly0dbV1eX2YxlZ3vjZ8jipzKpoY9IPmytv6Sg2V6lPJjF7imX5McnLG0erRRCjjUlyrK3M0kWp+Y3XYCvLHY3aZ6leVeYC+Hnxh5wM4F9DCP85gf0JISaZ0g4eQugHcNEkjkUIMclIJhMiY+TgQmSMHFyIjJGDC5ExHckmq5c14jaTYzx5isk0TJ5KZeLEcbCsoVRxQoDLVp5MA6SzsaKNyWQsU86Tf9g4UnMfx8EKSrJsMm+MrZ4XVhwxwh4wYcfzYEU52fVRdl07TwJk13cz85JCd3AhMkYOLkTGyMGFyBg5uBAZIwcXImM6EkWvjyjGbRbtjAkHjbDIMHu4PxXRbGZJGu897FhMHUgtTxRtZ511ltsvVbss4iUhzJo1y+2TIkbxWbSW1QUro3wwWISajcM7ZyxZIxWVjzaW2MISYtg580iNMdrYOOg+S/USQvxOIAcXImPk4EJkjBxciIyRgwuRMXJwITKmIzJZvVTSjGziSU1MJmMyQqouWLSVqf3FjsXkqcYlmfbv31+znXvuuW6/w4cPu21nn3120s4qj6YkqDi3LJGDyVNeG0vWYLXQWD+W7ONVLG211ly0eZIt0HpCT8ST8lLXVTNJUQzdwYXIGDm4EBkjBxciY+TgQmSMHFyIjJGDC5ExU5ZNxiSXY8eOjbufRlj2TkoKi/ti+/SkCZbpxGSm1EJ2bHG7CJNcvCw0Jv+ljhn/ViZBMbwMO5Z5lxpjtLEadUyK9I7HshdTMlmUH1PLTUXYAoPeNczaUuOINfIaF2pslnHv4GZ2n5kNm1lvne0sM1tvZn3Fb79SnxBiymjmI/oPAVzdYLsNwJMhhB4ATxavhRAfMMZ18GK978bFidcAuL/Yvh/A9ZM8LiHEJGDse0TtTWYLAawLISwpXh8KIZxRbBuAg/F1ZGRkpLbjvr6+SRyyECLS09NT265UKmMCEBMOsoUQgpnR/xLxeeuhoaHaNguy7du3L2nfs2eP22fHjh1uW2MwZ+XKldi8eTMAYN68eW4/L9DDFiJgQbZly5aNev3222/XnnVevHix22/Xrl1u2+uvv560b9myxe3TuM76ddddh3Xr1gEAPvaxj7n9LrjgArfNOzfeuQTGLiyxfPlyPPPMMwD4TeGNN95w28oE2RoDWLfffjvuvPNOADyg2u4g2z333INvf/vbyTFGnnrqKfc4QHmZbMjMugGg+D1ccj9CiDZS9g7+KICvAPhe8fsR9ub6YnJxu0wmDiucx+SdVNvRo0cBcJnKuxuwTC32n3t4ePT/wa6urppt0aJFbj929/HmhM0vg8laZc4Z2x/L4jrjjDPGtEVYZpV3vFaXvYrFMNknTQbLAGxGGo1EmY7NI6MZmewnAP4bwIVmtsfMbkLVsVebWR+AVcVrIcQHjHHv4CGEG5ymz0zyWIQQk4weVRUiY+TgQmSMHFyIjJGDC5ExHckmq5dX4jaTOrxMKCYVRNkrRUpKitln7EEGTxZix2JZP43SSVdXV81WJqsN8MfPZCEmTzEprMw6Y2x+U5JitLFzXaYQIpNYGx+4qT8+Gz87L0wKSx0PSI8x7oeNg6E7uBAZIwcXImPk4EJkjBxciIyRgwuRMXJwITJmymQyljftSSQss4fJIKlMp2hjRfW8fPDTTz/d7cPkkUaZ7Nxzz63ZWPYRk668v5vNVUreibYykhzgy3wsy4/JZAx2rr3MOzb2lLQZbSyfncmlrGaAVyA0NfaY+162GKbu4EJkjBxciIyRgwuRMXJwITJGDi5ExkzZ0kUs2nzgQGMZ9iosMhxraKVIRexjwkKlUnH7LViwIGlnNdkOHTrktqUistHGorXz589321jU2yM1j9HG1A2WEOMlorDEkBQzZswA4CdkAOUSLxorydaTimpHGzvX7Jyx43nnbNq0aWNssWZf2Rp7uoMLkTFycCEyRg4uRMbIwYXIGDm4EBkjBxciYzoik9XLMnGbSV5lJBeWgMBgdca82nBsKaEo86RgyRWNyxrVw5bx8SQjVtMsJXdFWxnZDUhLPIA/h0A6WSMenyVyMAnKGweTu1KybLS1UmOvHiYpllmGqJ1LF91nZsNm1ltnu8PMBsxsW/FzbamjCyHaSjO3vR8CuDphvyeEsLT4eWxyhyWEmAzGdfAQwiYA6UfLhBAfaIwtYl57k9lCAOtCCEuK13cA+CqAwwCeA3BrCGFUhvvIyEhtx2wxdyFEeXp6emrblUplzBf1sg4+F8A+AAHAdwF0hxC+Vt+n3sHjc70DAwOYN2/eKFsKLyAyODjo9tm9e7fb1lgN48orr8TGjRsBAEuXLnX7nXfeeUn71q1b3T6vvfaa29YYeFm1ahU2bNgAAFi2bJnb74ILLnDbent7k/b+/n63T6wSEvnc5z6Hhx9+GADwiU98wu134YUXum3eM/gDAwNun8bg1ooVK/D0008D4OdzsoNsjc+U33333bjlllsAAHv37nX7vfnmm25bmYpFjWNfv349Vq9eDcAPVm7fvr22nXLwUqHnEMJQCOFECOF9AD8AcGmZ/Qgh2kspmczMukMI8Xb6WQDp20hB/X8z9p8t4slQs2bNcvt4/7m9/UUbk7y82mvxU0gKJtft2bNnjC3WhmOSC8ueKlO/LnUOmjkvTPLy5EGWBZU6Z9HGxs8yzcr0SWU2RhsbRzOffierX9ljjevgZvYTAFcAONvM9gD4KwBXmNlSVD+i7wbw9VJHF0K0lXEdPIRwQ8K8tg1jEUJMMnpUVYiMkYMLkTFycCEyRg4uRMZ0JJssyltDQ0O1bVZ00ZMEWB8mT6XkmCj5MAnKk47mzJnj9mGyUEoKi/PBlqZhGV6ePNgOmSy1BNR4/VkGYKot2tixmCTqyWFMJktlrkUbW0qpbIFK7xpJXffRxq59hu7gQmSMHFyIjJGDC5ExcnAhMkYOLkTGyMGFyJiOyGT1ucJxu0wxO7YWFJOSUm1RRmI5vV6GVFdXl9uHZacxmOTCpDyvaCTL/GIweYeN0WtjxQJTbdHGpDBWKNODXR+sCCWTL8tIYYAv6aaOFW3sWAzdwYXIGDm4EBkjBxciY+TgQmSMHFyIjOlIFL0+Ahi3WXKIl6DAlgViUVcWgWRL5KRqqAF8KSEWYS+bTMAi897x2HykIrzRVjaBwos2s+g1mw8WvWZ41xVLXkn1iTZWC43NMVMPyiyzVbYmm+7gQmSMHFyIjJGDC5ExcnAhMkYOLkTGyMGFyJiOyGT19bDiNnsY30smYNJDq7WzoixVRhZiyR+tjiPa2LJMTB70xl+pVNw+qYX4vGWa6mF13rzFJNkikyy5gp0XVl/NS7JpNfkj2pgUxiRAdjxvjCkpLMp7bO4Z497BzWyBmW00s1fM7GUz+2ZhP8vM1ptZX/H7zFIjEEK0jWY+or+H6vrfiwEsB3CzmS0GcBuAJ0MIPQCeLF4LIT5AjOvgIYTBEMLzxfYRAK8CmAdgDYD7i7fdD+D6dg1SCFEOa+URODNbCGATgCUAXgshnFHYDcDB+BoARkZGajvu6+ubpOEKIerp6empbVcqlTFBqqaDbGbWBeBnAL4VQjhcH/AKIQQzc/9TxEDB8ePHa9ssCOEFWIaHh90+L7zwQtP7u/LKK7Fx48Zxx+E9A86eN2fBrYGBgVGvly1bhueeew4AD7J96lOfcttGRkaS9rjfFP39/aNeX3fddVi3bh0AYOHChW6/iy66yG1LBe4AXjGncez189Hb6y85Pzg46LZ5AdqDBw+6fRpzDh544AF8+ctfBsDHz4J97Nl3r60xeLt+/XqsXr0agJ8zwaocAU3KZGY2HVXn/nEI4eHCPGRm3UV7NwDf+4QQU8K4d/Di4/daAK+GEO6ua3oUwFcAfK/4/Yi3j3iXPH78eG27TEYNk6dYphOr/cXGUWYJJZb5lZJHoo19KigzV6wPGwcbP7sreRmAZbO4GCzTzPtExuSuVBt7f4TJZK3OP5C+huMnkmaWlkrRzEf0FQBuBPCSmW0rbN9B1bEfMrObAPwvgC+UGoEQom2M6+AhhM0AvCdMPjO5wxFCTCZ6VFWIjJGDC5ExcnAhMkYOLkTGdCSbLEooR48erW0zicGTocouI5OSaqL8wLKoPMmFSSDswZnUQzDRxh6QYZKN9wAEk/JScx9tZaQwb5/A6GWrGmHyFBsHy67zxsGuN5blx64r9hQoW17JO59MzmXXFUN3cCEyRg4uRMbIwYXIGDm4EBkjBxciY+TgQmRMR2SyelkpbrPihF6BOSadMJgMwvCKPLKxs4y3lNRx5MgRADybjOX8esdjMk1qHqONZZOxNdk8OYnNMyu6WFaui/PZCCtaWFae8rLCgHLXd8oe5c62FV0UQvzuIgcXImPk4EJkjBxciIyRgwuRMR2Jotc/6B+3y9RXa7XeWSSV8BCjra3WDAP8CqIAX6pnwYIFY2zNRI1Z4kiZZZ4YLLmCJb2w+fdIVYSNtlaTQyJeQhLbXyppJNrKRtHZddAK8W8tM7+A7uBCZI0cXIiMkYMLkTFycCEyRg4uRMbIwYXImI7IZPXSS9z2kgIAv86YZ288RiMpKYklY0Q8aYUdi7UxWALFnDlz3DZvHpmklapDF22sH5M2PemK1bw788wzXRuTmVjihSeTsbGn2qKNXaesRiCjmWWRGt9bVvYc9w5uZgvMbKOZvWJmL5vZNwv7HWY2YGbbip9rS41ACNE2mrmDvwfg1hDC82Y2E8AWM1tftN0TQviH9g1PCDERmlmbbBDAYLF9xMxeBTCv3QMTQkwcY7Wdx7zZbCGATQCWALgFwFcBHAbwHKp3+doq6yMjI7Ud9/X1TcpghRCj6enpqW1XKpUxX9SbdnAz6wLwFIA7QwgPm9lcAPsABADfBdAdQvhafH+9gx84cABA9Rnu2bNnAwAGBwfdY5UJsvX397ttjc9yX3PNNXj88ccB8Col3rPvQ0NDbh8WHFq4cOGo1xdffDGef/55AMDSpUvdft3d3W6bFwTasmWL26fxGfDly5fjmWeeAQCcf/75br9LLrmk5XHs2LHD7dN4zj75yU/Wxj0wMOD2YzcMb6EFtgBD4/l86KGH8IUvVFfDZnkH7Q6y/fKXv8QVV1wBwA/41vtRysGbksnMbDqAnwH4cQjhYQAIIQyFEE6EEN4H8AMAlzY1aiFExxj3O7hV4/NrAbwaQri7zt5dfD8HgM8C6HUPUidJxW2WHeNlcTGpgC0nxGQyli3k7ZN96mFyTOruHm0sQ6rM392KFFMPqyVWJkOKZQCmpMFoYzImO2fe8VrNRIw2dl2Vxbsbp87ZRJcuaiaKvgLAjQBeMrNthe07AG4ws6WofkTfDeDrpUYghGgbzUTRNwNI3UIem/zhCCEmEz2qKkTGyMGFyBg5uBAZIwcXImM6kk1WL0PE7ZkzZ7rv92QolqnFlv5JZTTFB1zmzp3bUj+ASycse6pSqbg2Nh9sn550xcaYkmmi7a233nL7MQnNk+VYsUOWxcWOxfDmismQM2bMcG2ptgj721hbK9JbfG9ZuU53cCEyRg4uRMbIwYXIGDm4EBkjBxciY+TgQmTMlK1NxrJ7PInh8OHDbp+DBw+6balsrJi/XEYmSxULjLCCgKm/OdqYBMiyuLy5Ymudsay2lJTXDN7xWHZd6nxGG+vH8LKuWHYayzZkmXwsq7DV7EavT5SVy86H7uBCZIwcXIiMkYMLkTFycCEyRg4uRMbIwYXImClbm6xMdgyTkpiMkJJO4r6YfOKtCcZKLbOstpSUNH/+fABcnmKSlyfLsSwoVuyQrZHGCmV62WTsnKXGHm3sWGw+GktCRxpLZ9eTyqCLNiZRsn2y68qbq1TGW7Qx+ZWhO7gQGSMHFyJj5OBCZIwcXIiMkYMLkTFTlmzCHuL3Iq9ssTe2v1Qbe3/Ei+SyCDVra4ye7t+/vxZFnzVrltuPjdWLyLKEmEWLFrk2Fs1n4/AWH2RRdJZ8w6LQbJ9eLTdW441Fr9k1V3a5Ka9fyh7VobILHY57Bzez08zs12b2gpm9bGZ/XdgXmdmvzGyHmf2bmfm6hhBiSmjmI/pxAJ8OIVwEYCmAq81sOYDvA7gnhPBRAAcB3NS+YQohyjCug4cqcWHu6cVPAPBpAP9R2O8HcH1bRiiEKI2xpPXam8ymAdgC4KMA7gXw9wCeKe7eMLMFAB4PISyJfUZGRmo7Zgu2CyHK09PTU9uuVCpjvvg3FWQLIZwAsNTMzgDwcwB/0Mog4iOfw8PDtW0WhPAeN9y9e7fbZ+vWrW5b46Oqq1atwoYNGwAAixcvdvt9/OMfb2p/9bBgSCrINnv2bADlg2xecGt4eNjts3fv3lGvu7u7MThYXeqdBdnOO+88t817VHj79u1un507d456vWLFCjz99NMAgH379rn9BgYG3DavH6sG1DgfP/rRj/ClL31p3GMdPXrUbSuzhnljkG3Tpk24/PLLAfjze+DAAfc4QIsyWQjhEICNAC4DcIaZxX8Q8wH4MyGEmBLGvYOb2TkA3g0hHDKz0wGsRjXAthHA5wH8FMBXADzi7aP+a0DcbkamYvtphC0Vk7qrRhuTT7zjsf/OTDpJSVrRxqQf1uaNkd2JG2vNHTt2rCaTMXmK4c2jJ+MB6eWaoo3dmVgCiHddseSmVFu0sXPN9sn+bq8tdS7je8suXdTM2ewGcH/xPfwkAA+FENaZ2SsAfmpmfwtgK4C1pUYghGgb4zp4COFFAH+UsPcDuLQdgxJCTA56VFWIjJGDC5ExcnAhMqapB13KUP+gixCi/aQedNEdXIiMkYMLkTFt+4guhJh6dAcXImPk4EJkTEcc3MyuNrPfFNVfbuvEMZ1x7Dazl8xsm5k91+Fj32dmw2bWW2c7y8zWm1lf8duvs9TecdxhZgPFvGwzs2vbPIYFZrbRzF4pqgR9s7B3dD7IODo9H+2rmhRCaOsPgGkAdgL4CIBTALwAYHG7j+uMZTeAs6fo2JcDuBhAb53t7wDcVmzfBuD7UzSOOwD8RQfnohvAxcX2TADbASzu9HyQcXR6PgxAV7E9HcCvACwH8BCALxb2fwLw563uuxN38EsB7Agh9IcQ3kE1+2xNB477gSKEsAlAY4rUGlSr4QAdqorjjKOjhBAGQwjPF9tHALwKYB46PB9kHB0lVGlL1aROOPg8AK/Xvd6DKZjEggDgCTPbYmZ/NkVjqGduCGGw2N4LYO4UjuUbZvZi8RG+7V8VIma2ENVkpl9hCuejYRxAh+fDzKaZ2TYAwwDWo/qp91AIIeYfl/Kb37cg28oQwsUArgFws5ldPtUDioTq57Cp0iz/EcD5qBbVHARwVycOamZdAH4G4FshhFElVzo5H4lxdHw+QggnQghLUS2ecilarJrk0QkHHwCwoO71lFV/CSEMFL+HUS09NdXprkNm1g0AxW+/zlIbCSEMFRfY+wB+gA7Mi5lNR9WpfhxCeLgwd3w+UuOYivmIhEmumtQJB38WQE8RETwFwBcBPNqB447CzGaY2cy4DeAqAL28V9t5FNVqOMA4VXHaSXSqgs+izfNi1bIrawG8GkK4u66po/PhjWMK5uOcot4h6qomvYr/r5oElJ2PDkUJr0U1QrkTwO2dik42jOEjqEbwXwDwcqfHAeAnqH7cexfV71M3AZgN4EkAfQA2ADhrisbxIICXALyIqpN1t3kMK1H9+P0igG3Fz7Wdng8yjk7Pxx+iWhXpRVT/mfxl3TX7awA7APw7gFNb3bceVRUiY37fgmxC/F4hBxciY+TgQmSMHFyIjJGDC5ExcnAhMkYOLkTG/B/8DPIBs7FotQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43LroPAQDIvN",
        "outputId": "c05c2e94-ee2c-4399-99c0-06c586395eff"
      },
      "source": [
        "print('Reshaping X data: (n, 32, 32) => (n, 1024)'); print('--'*45)\r\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))\r\n",
        "X_val = X_val.reshape((X_val.shape[0], -1))\r\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))\r\n",
        "\r\n",
        "print('Making sure that the values are float so that we can get decimal points after division'); print('--'*45)\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_val = X_val.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "\r\n",
        "print('Normalizing the RGB codes by dividing it to the max RGB value'); print('--'*45)\r\n",
        "X_train /= 255\r\n",
        "X_val /= 255\r\n",
        "X_test /= 255\r\n",
        "\r\n",
        "print('Converting y data into categorical (one-hot encoding)'); print('--'*45)\r\n",
        "y_train = to_categorical(y_train_o)\r\n",
        "y_val = to_categorical(y_val_o)\r\n",
        "y_test = to_categorical(y_test_o)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaping X data: (n, 32, 32) => (n, 1024)\n",
            "------------------------------------------------------------------------------------------\n",
            "Making sure that the values are float so that we can get decimal points after division\n",
            "------------------------------------------------------------------------------------------\n",
            "Normalizing the RGB codes by dividing it to the max RGB value\n",
            "------------------------------------------------------------------------------------------\n",
            "Converting y data into categorical (one-hot encoding)\n",
            "------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeN_V0f4Decq",
        "outputId": "f3da1f8c-5992-4770-98a3-89f339a51e9a"
      },
      "source": [
        "print('X_train shape:', X_train.shape)\r\n",
        "print('X_val shape:', X_val.shape)\r\n",
        "print('X_test shape:', X_test.shape)\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "print('y_train shape:', y_train.shape)\r\n",
        "print('y_val shape:', y_val.shape)\r\n",
        "print('y_test shape:', y_test.shape)\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "print('Number of images in X_train :', X_train.shape[0])\r\n",
        "print('Number of images in X_val :', X_val.shape[0])\r\n",
        "print('Number of images in X_test :', X_test.shape[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (42000, 1024)\n",
            "X_val shape: (60000, 1024)\n",
            "X_test shape: (18000, 1024)\n",
            "\n",
            "\n",
            "y_train shape: (42000, 10)\n",
            "y_val shape: (60000, 10)\n",
            "y_test shape: (18000, 10)\n",
            "\n",
            "\n",
            "Number of images in X_train : 42000\n",
            "Number of images in X_val : 60000\n",
            "Number of images in X_test : 18000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn57uQqjFloU"
      },
      "source": [
        "## **Modelling - Baby sitting the learning process**\r\n",
        "**Fully connected linear layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZHnhyd2FOTy"
      },
      "source": [
        "class Linear():\r\n",
        "    def __init__(self, in_size, out_size):\r\n",
        "        self.W = np.random.randn(in_size, out_size) * 0.01\r\n",
        "        self.b = np.zeros((1, out_size))\r\n",
        "        self.params = [self.W, self.b]\r\n",
        "        self.gradW = None\r\n",
        "        self.gradB = None\r\n",
        "        self.gradInput = None        \r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        self.X = X\r\n",
        "        self.output = np.dot(X, self.W) + self.b\r\n",
        "        return self.output\r\n",
        "\r\n",
        "    def backward(self, nextgrad):\r\n",
        "        self.gradW = np.dot(self.X.T, nextgrad)\r\n",
        "        self.gradB = np.sum(nextgrad, axis=0)\r\n",
        "        self.gradInput = np.dot(nextgrad, self.W.T)\r\n",
        "        return self.gradInput, [self.gradW, self.gradB]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61KklMJmFklf"
      },
      "source": [
        "**ReLU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RYF_064FbxU"
      },
      "source": [
        "class ReLU():\r\n",
        "    def __init__(self):\r\n",
        "        self.params = []\r\n",
        "        self.gradInput = None\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        self.output = np.maximum(X, 0)\r\n",
        "        return self.output\r\n",
        "\r\n",
        "    def backward(self, nextgrad):\r\n",
        "        self.gradInput = nextgrad.copy()\r\n",
        "        self.gradInput[self.output <=0] = 0\r\n",
        "        return self.gradInput, []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP_whwmj_gC6"
      },
      "source": [
        "**Softmax function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iXkifrk_pkd"
      },
      "source": [
        "def softmax(x):\r\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\r\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3RLPiKH_vkl"
      },
      "source": [
        "**Cross entropy loss** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5PuEunA_sHj"
      },
      "source": [
        "class CrossEntropy:\r\n",
        "    def forward(self, X, y):\r\n",
        "        self.m = y.shape[0]\r\n",
        "        self.p = softmax(X)\r\n",
        "        cross_entropy = -np.log(self.p[range(self.m), y]+1e-16)\r\n",
        "        loss = np.sum(cross_entropy) / self.m\r\n",
        "        return loss\r\n",
        "    \r\n",
        "    def backward(self, X, y):\r\n",
        "        y_idx = y.argmax()        \r\n",
        "        grad = softmax(X)\r\n",
        "        grad[range(self.m), y] -= 1\r\n",
        "        grad /= self.m\r\n",
        "        return grad"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrA8AwME_5xS"
      },
      "source": [
        "class NN():\r\n",
        "    def __init__(self, lossfunc = CrossEntropy(), mode = 'train'):\r\n",
        "        self.params = []\r\n",
        "        self.layers = []\r\n",
        "        self.loss_func = lossfunc\r\n",
        "        self.grads = []\r\n",
        "        self.mode = mode\r\n",
        "        \r\n",
        "    def add_layer(self, layer):\r\n",
        "        self.layers.append(layer)\r\n",
        "        self.params.append(layer.params)\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        for layer in self.layers:\r\n",
        "            X = layer.forward(X)\r\n",
        "        return X\r\n",
        "    \r\n",
        "    def backward(self, nextgrad):\r\n",
        "        self.clear_grad_param()\r\n",
        "        for layer in reversed(self.layers):\r\n",
        "            nextgrad, grad = layer.backward(nextgrad)\r\n",
        "            self.grads.append(grad)\r\n",
        "        return self.grads\r\n",
        "    \r\n",
        "    def train_step(self, X, y):\r\n",
        "        out = self.forward(X)\r\n",
        "        loss = self.loss_func.forward(out,y)\r\n",
        "        nextgrad = self.loss_func.backward(out,y)\r\n",
        "        grads = self.backward(nextgrad)\r\n",
        "        return loss, grads\r\n",
        "    \r\n",
        "    def predict(self, X):\r\n",
        "        X = self.forward(X)\r\n",
        "        p = softmax(X)\r\n",
        "        return np.argmax(p, axis=1)\r\n",
        "    \r\n",
        "    def predict_scores(self, X):\r\n",
        "        X = self.forward(X)\r\n",
        "        p = softmax(X)\r\n",
        "        return p\r\n",
        "    \r\n",
        "    def clear_grad_param(self):\r\n",
        "        self.grads = []"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJbW4S29AR3q"
      },
      "source": [
        "**Update function SGD with momentum**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9oZOuTXAFVh"
      },
      "source": [
        "def update(velocity, params, grads, learning_rate=0.01, mu=0.9):\r\n",
        "    for v, p, g, in zip(velocity, params, reversed(grads)):\r\n",
        "        for i in range(len(g)):\r\n",
        "            v[i] = (mu * v[i]) - (learning_rate * g[i])\r\n",
        "            p[i] += v[i]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Q2r2cXAs1K"
      },
      "source": [
        "**minibatches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2ma1sXNAXbk"
      },
      "source": [
        "def minibatch(X, y, minibatch_size):\r\n",
        "    n = X.shape[0]\r\n",
        "    minibatches = []\r\n",
        "    permutation = np.random.permutation(X.shape[0])\r\n",
        "    X = X[permutation]\r\n",
        "    y = y[permutation]\r\n",
        "    \r\n",
        "    for i in range(0, n , minibatch_size):\r\n",
        "        X_batch = X[i:i + minibatch_size, :]\r\n",
        "        y_batch = y[i:i + minibatch_size, ]\r\n",
        "        minibatches.append((X_batch, y_batch))\r\n",
        "        \r\n",
        "    return minibatches"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBc6uaHdAyoi"
      },
      "source": [
        "**training loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No_9r-duA2VE"
      },
      "source": [
        "def train(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu = 0.9, X_val = None, y_val = None, Lambda = 0, verb = True):\r\n",
        "    val_loss_epoch = []\r\n",
        "    minibatches = minibatch(X_train, y_train, minibatch_size)\r\n",
        "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\r\n",
        "    \r\n",
        "    for i in range(epoch):\r\n",
        "        loss_batch = []\r\n",
        "        val_loss_batch = []\r\n",
        "        velocity = []\r\n",
        "        for param_layer in net.params:\r\n",
        "            p = [np.zeros_like(param) for param in list(param_layer)]\r\n",
        "            velocity.append(p)\r\n",
        "            \r\n",
        "        # iterate over mini batches\r\n",
        "        for X_mini, y_mini in minibatches:\r\n",
        "            loss, grads = net.train_step(X_mini, y_mini)\r\n",
        "            loss_batch.append(loss)\r\n",
        "            update(velocity, net.params, grads, learning_rate=learning_rate, mu=mu)\r\n",
        "\r\n",
        "        for X_mini_val, y_mini_val in minibatches_val:\r\n",
        "            val_loss, _ = net.train_step(X_mini, y_mini)\r\n",
        "            val_loss_batch.append(val_loss)\r\n",
        "        \r\n",
        "        # accuracy of model at end of epoch after all mini batch updates\r\n",
        "        m_train = X_train.shape[0]\r\n",
        "        m_val = X_val.shape[0]\r\n",
        "        y_train_pred = []\r\n",
        "        y_val_pred = []\r\n",
        "        y_train1 = []\r\n",
        "        y_vall = []\r\n",
        "        for ii in range(0, m_train, minibatch_size):\r\n",
        "            X_tr = X_train[ii:ii + minibatch_size, : ]\r\n",
        "            y_tr = y_train[ii:ii + minibatch_size,]\r\n",
        "            y_train1 = np.append(y_train1, y_tr)\r\n",
        "            y_train_pred = np.append(y_train_pred, net.predict(X_tr))\r\n",
        "\r\n",
        "        for ii in range(0, m_val, minibatch_size):\r\n",
        "            X_va = X_val[ii:ii + minibatch_size, : ]\r\n",
        "            y_va = y_val[ii:ii + minibatch_size,]\r\n",
        "            y_vall = np.append(y_vall, y_va)\r\n",
        "            y_val_pred = np.append(y_val_pred, net.predict(X_va))\r\n",
        "            \r\n",
        "        train_acc = check_accuracy(y_train1, y_train_pred)\r\n",
        "        val_acc = check_accuracy(y_vall, y_val_pred)\r\n",
        "        \r\n",
        "        ## weights\r\n",
        "        w = np.array(net.params[0][0])\r\n",
        "        \r\n",
        "        ## adding regularization to cost\r\n",
        "        mean_train_loss = (sum(loss_batch) / float(len(loss_batch)))\r\n",
        "        mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\r\n",
        "        \r\n",
        "        val_loss_epoch.append(mean_val_loss)\r\n",
        "        if verb:\r\n",
        "            if i%50==0:\r\n",
        "                print(\"Epoch {3}/{4}: Loss = {0} | Training Accuracy = {1}\".format(mean_train_loss, train_acc, val_acc, i, epoch))\r\n",
        "    return net, val_acc"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glA_H-03B790"
      },
      "source": [
        "**Checking the accuracy of the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyBDINRhB4fa"
      },
      "source": [
        "def check_accuracy(y_true, y_pred):\r\n",
        "    return np.mean(y_pred == y_true)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o595oSKTCE68"
      },
      "source": [
        "**Invoking all that we have created until now**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUR0TB3yCG8C"
      },
      "source": [
        "# Invoking the model\r\n",
        "## input size\r\n",
        "input_dim = X_train.shape[1]\r\n",
        "\r\n",
        "def train_and_test_loop(iterations, lr, Lambda, verb = True):\r\n",
        "    ## hyperparameters\r\n",
        "    iterations = iterations\r\n",
        "    learning_rate = lr\r\n",
        "    hidden_nodes1 = 10\r\n",
        "    output_nodes = 10\r\n",
        "\r\n",
        "    ## define neural net\r\n",
        "    nn = NN()\r\n",
        "    nn.add_layer(Linear(input_dim, hidden_nodes1))\r\n",
        "\r\n",
        "    nn, val_acc = train(nn, X_train, y_train_o, minibatch_size = 200, epoch = iterations, learning_rate = learning_rate,\\\r\n",
        "                      X_val = X_test, y_val = y_test_o, Lambda = Lambda, verb = verb)\r\n",
        "    return val_acc"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E44pF8-Cj86"
      },
      "source": [
        "**Double Check that the loss is reasonable : Disable the regularization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYSKcth2CJQB",
        "outputId": "e12298b1-1040-4913-b015-314ba94e0df4"
      },
      "source": [
        "lr = 0.00001\r\n",
        "Lambda = 0\r\n",
        "train_and_test_loop(1, lr, Lambda)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1: Loss = 2.3075708433795503 | Training Accuracy = 0.09271428571428571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08855555555555555"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl2wOEkfCqry"
      },
      "source": [
        "**Now, lets crank up the Lambda(Regularization) and check what it does to our loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-n1FD4zCyJL",
        "outputId": "bf561088-9c25-4c3a-8095-e68153844450"
      },
      "source": [
        "lr = 0.00001\r\n",
        "Lambda = 1e3\r\n",
        "train_and_test_loop(1, lr, Lambda)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1: Loss = 2.3143311346047173 | Training Accuracy = 0.10983333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11016666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey2h63n0DAvJ"
      },
      "source": [
        "**Now, lets overfit to a small subset of our dataset, in this case 20 images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9n76JC9DD78",
        "outputId": "91f918fb-4bab-4fff-a069-beb4ee90493a"
      },
      "source": [
        "X_train_subset = X_train[0:20]\r\n",
        "y_train_subset = y_train_o[0:20]\r\n",
        "\r\n",
        "X_train = X_train_subset\r\n",
        "y_train_o = y_train_subset\r\n",
        "\r\n",
        "X_train.shape, y_train_o.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20, 1024), (20,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRgsG0xEDZdp"
      },
      "source": [
        "**Make sure that you can overfit very small portion of the training data**\r\n",
        "\r\n",
        "So, set a small learning rate and turn regularization off In the code below:\r\n",
        "\r\n",
        "*   Take the first 20 examples\r\n",
        "*   turn off regularization(reg=0.0)\r\n",
        "*   use simple vanilla 'sgd'\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiU6c5NsEHGJ",
        "outputId": "38f72a26-0ca4-4c9c-a00b-b51fc2e52231"
      },
      "source": [
        "%time\r\n",
        "lr = 0.001\r\n",
        "Lambda = 0\r\n",
        "train_and_test_loop(5000, lr, Lambda)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 s, sys: 1 s, total: 4 s\n",
            "Wall time: 7.39 s\n",
            "Epoch 0/5000: Loss = 2.3376287833056706 | Training Accuracy = 0.1\n",
            "Epoch 50/5000: Loss = 1.9429845985995438 | Training Accuracy = 0.3\n",
            "Epoch 100/5000: Loss = 1.858145742182116 | Training Accuracy = 0.3\n",
            "Epoch 150/5000: Loss = 1.8043437499365211 | Training Accuracy = 0.45\n",
            "Epoch 200/5000: Loss = 1.7595187449974894 | Training Accuracy = 0.45\n",
            "Epoch 250/5000: Loss = 1.719100326717852 | Training Accuracy = 0.45\n",
            "Epoch 300/5000: Loss = 1.6814856488300904 | Training Accuracy = 0.5\n",
            "Epoch 350/5000: Loss = 1.645942736662724 | Training Accuracy = 0.55\n",
            "Epoch 400/5000: Loss = 1.612078995884033 | Training Accuracy = 0.55\n",
            "Epoch 450/5000: Loss = 1.5796567552770582 | Training Accuracy = 0.55\n",
            "Epoch 500/5000: Loss = 1.5485168703241747 | Training Accuracy = 0.6\n",
            "Epoch 550/5000: Loss = 1.51854349863012 | Training Accuracy = 0.6\n",
            "Epoch 600/5000: Loss = 1.4896465469738382 | Training Accuracy = 0.6\n",
            "Epoch 650/5000: Loss = 1.461752335669504 | Training Accuracy = 0.6\n",
            "Epoch 700/5000: Loss = 1.4347983326789637 | Training Accuracy = 0.6\n",
            "Epoch 750/5000: Loss = 1.4087300169127215 | Training Accuracy = 0.65\n",
            "Epoch 800/5000: Loss = 1.3834989124906745 | Training Accuracy = 0.65\n",
            "Epoch 850/5000: Loss = 1.3590612974701168 | Training Accuracy = 0.65\n",
            "Epoch 900/5000: Loss = 1.3353773181175175 | Training Accuracy = 0.65\n",
            "Epoch 950/5000: Loss = 1.3124103569044432 | Training Accuracy = 0.65\n",
            "Epoch 1000/5000: Loss = 1.29012656514456 | Training Accuracy = 0.65\n",
            "Epoch 1050/5000: Loss = 1.268494506098372 | Training Accuracy = 0.65\n",
            "Epoch 1100/5000: Loss = 1.24748487450289 | Training Accuracy = 0.65\n",
            "Epoch 1150/5000: Loss = 1.227070270483792 | Training Accuracy = 0.65\n",
            "Epoch 1200/5000: Loss = 1.20722501318428 | Training Accuracy = 0.7\n",
            "Epoch 1250/5000: Loss = 1.187924984108569 | Training Accuracy = 0.7\n",
            "Epoch 1300/5000: Loss = 1.1691474932031878 | Training Accuracy = 0.7\n",
            "Epoch 1350/5000: Loss = 1.1508711627080277 | Training Accuracy = 0.7\n",
            "Epoch 1400/5000: Loss = 1.1330758251715445 | Training Accuracy = 0.7\n",
            "Epoch 1450/5000: Loss = 1.115742432966618 | Training Accuracy = 0.75\n",
            "Epoch 1500/5000: Loss = 1.0988529773064242 | Training Accuracy = 0.75\n",
            "Epoch 1550/5000: Loss = 1.082390415233466 | Training Accuracy = 0.75\n",
            "Epoch 1600/5000: Loss = 1.0663386033983553 | Training Accuracy = 0.75\n",
            "Epoch 1650/5000: Loss = 1.0506822376971243 | Training Accuracy = 0.8\n",
            "Epoch 1700/5000: Loss = 1.035406798023169 | Training Accuracy = 0.8\n",
            "Epoch 1750/5000: Loss = 1.020498497530619 | Training Accuracy = 0.8\n",
            "Epoch 1800/5000: Loss = 1.0059442359126667 | Training Accuracy = 0.85\n",
            "Epoch 1850/5000: Loss = 0.9917315562802592 | Training Accuracy = 0.85\n",
            "Epoch 1900/5000: Loss = 0.9778486052899599 | Training Accuracy = 0.85\n",
            "Epoch 1950/5000: Loss = 0.9642840962195469 | Training Accuracy = 0.85\n",
            "Epoch 2000/5000: Loss = 0.9510272747293265 | Training Accuracy = 0.85\n",
            "Epoch 2050/5000: Loss = 0.9380678870789041 | Training Accuracy = 0.85\n",
            "Epoch 2100/5000: Loss = 0.9253961505950224 | Training Accuracy = 0.85\n",
            "Epoch 2150/5000: Loss = 0.9130027262075217 | Training Accuracy = 0.85\n",
            "Epoch 2200/5000: Loss = 0.9008786928885233 | Training Accuracy = 0.85\n",
            "Epoch 2250/5000: Loss = 0.8890155238453449 | Training Accuracy = 0.85\n",
            "Epoch 2300/5000: Loss = 0.8774050643310213 | Training Accuracy = 0.85\n",
            "Epoch 2350/5000: Loss = 0.8660395109480563 | Training Accuracy = 0.95\n",
            "Epoch 2400/5000: Loss = 0.8549113923314626 | Training Accuracy = 0.95\n",
            "Epoch 2450/5000: Loss = 0.8440135511065519 | Training Accuracy = 0.95\n",
            "Epoch 2500/5000: Loss = 0.8333391270254042 | Training Accuracy = 0.95\n",
            "Epoch 2550/5000: Loss = 0.8228815411937205 | Training Accuracy = 0.95\n",
            "Epoch 2600/5000: Loss = 0.812634481306819 | Training Accuracy = 0.95\n",
            "Epoch 2650/5000: Loss = 0.8025918878200654 | Training Accuracy = 0.95\n",
            "Epoch 2700/5000: Loss = 0.7927479409849998 | Training Accuracy = 0.95\n",
            "Epoch 2750/5000: Loss = 0.783097048687942 | Training Accuracy = 1.0\n",
            "Epoch 2800/5000: Loss = 0.7736338350329316 | Training Accuracy = 1.0\n",
            "Epoch 2850/5000: Loss = 0.7643531296155366 | Training Accuracy = 1.0\n",
            "Epoch 2900/5000: Loss = 0.7552499574383578 | Training Accuracy = 1.0\n",
            "Epoch 2950/5000: Loss = 0.7463195294230209 | Training Accuracy = 1.0\n",
            "Epoch 3000/5000: Loss = 0.7375572334770697 | Training Accuracy = 1.0\n",
            "Epoch 3050/5000: Loss = 0.7289586260775032 | Training Accuracy = 1.0\n",
            "Epoch 3100/5000: Loss = 0.7205194243357521 | Training Accuracy = 1.0\n",
            "Epoch 3150/5000: Loss = 0.7122354985116905 | Training Accuracy = 1.0\n",
            "Epoch 3200/5000: Loss = 0.7041028649468029 | Training Accuracy = 1.0\n",
            "Epoch 3250/5000: Loss = 0.6961176793889988 | Training Accuracy = 1.0\n",
            "Epoch 3300/5000: Loss = 0.6882762306836602 | Training Accuracy = 1.0\n",
            "Epoch 3350/5000: Loss = 0.6805749348074777 | Training Accuracy = 1.0\n",
            "Epoch 3400/5000: Loss = 0.6730103292233865 | Training Accuracy = 1.0\n",
            "Epoch 3450/5000: Loss = 0.6655790675365528 | Training Accuracy = 1.0\n",
            "Epoch 3500/5000: Loss = 0.6582779144328156 | Training Accuracy = 1.0\n",
            "Epoch 3550/5000: Loss = 0.6511037408823669 | Training Accuracy = 1.0\n",
            "Epoch 3600/5000: Loss = 0.6440535195926641 | Training Accuracy = 1.0\n",
            "Epoch 3650/5000: Loss = 0.6371243206957093 | Training Accuracy = 1.0\n",
            "Epoch 3700/5000: Loss = 0.6303133076558651 | Training Accuracy = 1.0\n",
            "Epoch 3750/5000: Loss = 0.6236177333853146 | Training Accuracy = 1.0\n",
            "Epoch 3800/5000: Loss = 0.617034936555154 | Training Accuracy = 1.0\n",
            "Epoch 3850/5000: Loss = 0.6105623380908932 | Training Accuracy = 1.0\n",
            "Epoch 3900/5000: Loss = 0.6041974378418866 | Training Accuracy = 1.0\n",
            "Epoch 3950/5000: Loss = 0.5979378114148827 | Training Accuracy = 1.0\n",
            "Epoch 4000/5000: Loss = 0.5917811071625103 | Training Accuracy = 1.0\n",
            "Epoch 4050/5000: Loss = 0.5857250433181032 | Training Accuracy = 1.0\n",
            "Epoch 4100/5000: Loss = 0.5797674052687849 | Training Accuracy = 1.0\n",
            "Epoch 4150/5000: Loss = 0.5739060429592445 | Training Accuracy = 1.0\n",
            "Epoch 4200/5000: Loss = 0.568138868419087 | Training Accuracy = 1.0\n",
            "Epoch 4250/5000: Loss = 0.562463853407072 | Training Accuracy = 1.0\n",
            "Epoch 4300/5000: Loss = 0.5568790271659475 | Training Accuracy = 1.0\n",
            "Epoch 4350/5000: Loss = 0.5513824742819698 | Training Accuracy = 1.0\n",
            "Epoch 4400/5000: Loss = 0.5459723326435257 | Training Accuracy = 1.0\n",
            "Epoch 4450/5000: Loss = 0.540646791493618 | Training Accuracy = 1.0\n",
            "Epoch 4500/5000: Loss = 0.5354040895712628 | Training Accuracy = 1.0\n",
            "Epoch 4550/5000: Loss = 0.5302425133371366 | Training Accuracy = 1.0\n",
            "Epoch 4600/5000: Loss = 0.5251603952790784 | Training Accuracy = 1.0\n",
            "Epoch 4650/5000: Loss = 0.5201561122932945 | Training Accuracy = 1.0\n",
            "Epoch 4700/5000: Loss = 0.515228084137353 | Training Accuracy = 1.0\n",
            "Epoch 4750/5000: Loss = 0.5103747719512721 | Training Accuracy = 1.0\n",
            "Epoch 4800/5000: Loss = 0.5055946768432136 | Training Accuracy = 1.0\n",
            "Epoch 4850/5000: Loss = 0.5008863385364861 | Training Accuracy = 1.0\n",
            "Epoch 4900/5000: Loss = 0.4962483340747415 | Training Accuracy = 1.0\n",
            "Epoch 4950/5000: Loss = 0.4916792765824328 | Training Accuracy = 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13694444444444445"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljBOk4NjER5i"
      },
      "source": [
        "**Loading the original dataset again**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY3KhCGmENTS",
        "outputId": "b2b16609-dc25-49ab-c06d-ec03726d6f3b"
      },
      "source": [
        "\r\n",
        "h5_SVH = h5py.File('SVHN_single_grey1.h5', 'r')\r\n",
        "# Load the training, validation and test sets\r\n",
        "X_train = h5_SVH['X_train'][:]\r\n",
        "y_train_o = h5_SVH['y_train'][:]\r\n",
        "X_val = h5_SVH['X_val'][:]\r\n",
        "y_val_o = h5_SVH['y_val'][:]\r\n",
        "X_test = h5_SVH['X_test'][:]\r\n",
        "y_test_o = h5_SVH['y_test'][:]\r\n",
        "\r\n",
        "print('Reshaping X data: (n, 32, 32) => (n, 1024)'); print('--'*40)\r\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))\r\n",
        "X_val = X_val.reshape((X_val.shape[0], -1))\r\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))\r\n",
        "\r\n",
        "print('Making sure that the values are float so that we can get decimal points after division'); print('--'*40)\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_val = X_val.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "\r\n",
        "print('Normalizing the RGB codes by dividing it to the max RGB value'); print('--'*40)\r\n",
        "X_train /= 255\r\n",
        "X_val /= 255\r\n",
        "X_test /= 255\r\n",
        "\r\n",
        "print('Converting y data into categorical (one-hot encoding)'); print('--'*40)\r\n",
        "y_train = to_categorical(y_train_o)\r\n",
        "y_val = to_categorical(y_val_o)\r\n",
        "y_test = to_categorical(y_test_o)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaping X data: (n, 32, 32) => (n, 1024)\n",
            "--------------------------------------------------------------------------------\n",
            "Making sure that the values are float so that we can get decimal points after division\n",
            "--------------------------------------------------------------------------------\n",
            "Normalizing the RGB codes by dividing it to the max RGB value\n",
            "--------------------------------------------------------------------------------\n",
            "Converting y data into categorical (one-hot encoding)\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8uu0W7MH0fM"
      },
      "source": [
        "**Start with small regularization and find learning rate that makes the loss go down.**\r\n",
        "\r\n",
        "*   we start with Lambda(small regularization) = 1e-7\r\n",
        "*   we start with a small learning rate = 1e-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-_PEpcsIBLV",
        "outputId": "6c6dee7f-7fd8-4576-d862-d573c56f5e4e"
      },
      "source": [
        "lr = 1e-7\r\n",
        "Lambda = 1e-7\r\n",
        "train_and_test_loop(500, lr, Lambda)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/500: Loss = 2.313430385002409 | Training Accuracy = 0.10026190476190476\n",
            "Epoch 50/500: Loss = 2.30966525334014 | Training Accuracy = 0.10042857142857142\n",
            "Epoch 100/500: Loss = 2.307249386713144 | Training Accuracy = 0.10154761904761905\n",
            "Epoch 150/500: Loss = 2.305690386678784 | Training Accuracy = 0.10264285714285715\n",
            "Epoch 200/500: Loss = 2.3046794984921735 | Training Accuracy = 0.10302380952380952\n",
            "Epoch 250/500: Loss = 2.3040206632550184 | Training Accuracy = 0.10376190476190476\n",
            "Epoch 300/500: Loss = 2.303588469207817 | Training Accuracy = 0.10497619047619047\n",
            "Epoch 350/500: Loss = 2.303302365026004 | Training Accuracy = 0.10688095238095238\n",
            "Epoch 400/500: Loss = 2.303110498199185 | Training Accuracy = 0.10819047619047618\n",
            "Epoch 450/500: Loss = 2.30297945188193 | Training Accuracy = 0.10842857142857143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10722222222222222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch4Ymx9tIO5B"
      },
      "source": [
        "**Lets try to train now with a value of learning rate 0.001**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAM8-G1mIDm5",
        "outputId": "4736e8dc-3316-4779-9383-081e5e2a8a40"
      },
      "source": [
        "lr = 0.001\r\n",
        "Lambda = 1e-7\r\n",
        "train_and_test_loop(500, lr, Lambda)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/500: Loss = 2.3043172853088545 | Training Accuracy = 0.11697619047619047\n",
            "Epoch 50/500: Loss = 2.259137893639948 | Training Accuracy = 0.21214285714285713\n",
            "Epoch 100/500: Loss = 2.2506358493130385 | Training Accuracy = 0.22083333333333333\n",
            "Epoch 150/500: Loss = 2.246322320436922 | Training Accuracy = 0.22652380952380952\n",
            "Epoch 200/500: Loss = 2.243437872527246 | Training Accuracy = 0.2288095238095238\n",
            "Epoch 250/500: Loss = 2.241269179951821 | Training Accuracy = 0.22990476190476192\n",
            "Epoch 300/500: Loss = 2.2395311747584348 | Training Accuracy = 0.2317857142857143\n",
            "Epoch 350/500: Loss = 2.238080495399565 | Training Accuracy = 0.23338095238095238\n",
            "Epoch 400/500: Loss = 2.236834647672908 | Training Accuracy = 0.23438095238095238\n",
            "Epoch 450/500: Loss = 2.2357418364572332 | Training Accuracy = 0.23561904761904762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21777777777777776"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-XpjhXTIVzA"
      },
      "source": [
        "**Hyperparameter Optimization**\r\n",
        "\r\n",
        "**Running a finer search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A65QfPnDIfqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352f97ba-9f5e-460f-b177-d02a5cf51255"
      },
      "source": [
        "import math\r\n",
        "for k in range(1, 10):\r\n",
        "    lr = math.pow(10, np.random.uniform(-3.0, -2.0))\r\n",
        "    Lambda = math.pow(10, np.random.uniform(-5, 2))\r\n",
        "    best_acc = train_and_test_loop(100, lr, Lambda, False)\r\n",
        "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 10, best_acc, lr, Lambda))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Try 1/10: Best_val_acc: 0.18594444444444444, lr: 0.0022445486695815554, Lambda: 4.468938038777701\n",
            "\n",
            "Try 2/10: Best_val_acc: 0.19394444444444445, lr: 0.0043192340925826194, Lambda: 3.265320449954064e-05\n",
            "\n",
            "Try 3/10: Best_val_acc: 0.19777777777777777, lr: 0.0011061360864759757, Lambda: 0.0022397801458532026\n",
            "\n",
            "Try 4/10: Best_val_acc: 0.20333333333333334, lr: 0.001846001016682408, Lambda: 11.143539156195237\n",
            "\n",
            "Try 5/10: Best_val_acc: 0.1935, lr: 0.0019245785424494785, Lambda: 5.8151293567523365e-05\n",
            "\n",
            "Try 6/10: Best_val_acc: 0.2078888888888889, lr: 0.0013219046416681544, Lambda: 3.5214253685826065\n",
            "\n",
            "Try 7/10: Best_val_acc: 0.2003888888888889, lr: 0.007110008628874481, Lambda: 0.8635572757207823\n",
            "\n",
            "Try 8/10: Best_val_acc: 0.2055, lr: 0.0012479182008206348, Lambda: 0.05217066718973961\n",
            "\n",
            "Try 9/10: Best_val_acc: 0.18855555555555556, lr: 0.0033892661998016363, Lambda: 0.0009983402985581941\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okhuvY2iImb7"
      },
      "source": [
        "Observation 2 - Baby sitting the neural network for SVHN\r\n",
        "Best accuracy achieved using this method after hyperparameter optimization: 21%.\r\n",
        "\r\n",
        "**Modelling - Neural Network API**\r\n",
        "\r\n",
        "**NN model, sigmoid activations, SGD optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIAga2uXIt4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b61255-ecba-4444-b87e-c79230542165"
      },
      "source": [
        "print('NN model with sigmoid activations'); print('--'*40)\r\n",
        "# Initialize the neural network classifier\r\n",
        "model1 = Sequential()\r\n",
        "\r\n",
        "# Input Layer - adding input layer and activation functions sigmoid\r\n",
        "model1.add(Dense(128, input_shape = (1024, )))\r\n",
        "# Adding activation function\r\n",
        "model1.add(Activation('sigmoid'))\r\n",
        "\r\n",
        "#Hidden Layer 1 - adding first hidden layer\r\n",
        "model1.add(Dense(64))\r\n",
        "# Adding activation function\r\n",
        "model1.add(Activation('sigmoid'))\r\n",
        "\r\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\r\n",
        "model1.add(Dense(10))\r\n",
        "# Adding activation function - softmax for multiclass classification\r\n",
        "model1.add(Activation('softmax'))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with sigmoid activations\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW6rp3MaIx8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8243b127-55e7-4bd1-d1ee-172f4e6b9284"
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 140,106\n",
            "Trainable params: 140,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phsVdYb_I0DJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3f5335-1288-4f6c-acd9-e31e81ea0989"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\r\n",
        "sgd = optimizers.SGD(lr = 0.01)\r\n",
        "model1.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model1.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 3s 6ms/step - loss: 2.3709 - accuracy: 0.0996 - val_loss: 2.3028 - val_accuracy: 0.1001\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0976 - val_loss: 2.3028 - val_accuracy: 0.1007\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.1001 - val_loss: 2.3025 - val_accuracy: 0.1059\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3025 - accuracy: 0.1052 - val_loss: 2.3025 - val_accuracy: 0.1060\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3025 - accuracy: 0.1024 - val_loss: 2.3024 - val_accuracy: 0.1010\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3024 - accuracy: 0.1043 - val_loss: 2.3025 - val_accuracy: 0.1017\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3024 - accuracy: 0.1060 - val_loss: 2.3024 - val_accuracy: 0.1058\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3023 - accuracy: 0.1055 - val_loss: 2.3024 - val_accuracy: 0.0980\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3023 - accuracy: 0.1030 - val_loss: 2.3023 - val_accuracy: 0.0981\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3024 - accuracy: 0.0989 - val_loss: 2.3022 - val_accuracy: 0.1064\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3022 - accuracy: 0.1055 - val_loss: 2.3025 - val_accuracy: 0.1030\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3023 - accuracy: 0.1007 - val_loss: 2.3021 - val_accuracy: 0.1103\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3022 - accuracy: 0.1009 - val_loss: 2.3022 - val_accuracy: 0.1080\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3020 - accuracy: 0.1059 - val_loss: 2.3021 - val_accuracy: 0.1094\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3022 - accuracy: 0.1090 - val_loss: 2.3022 - val_accuracy: 0.1043\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3021 - accuracy: 0.1080 - val_loss: 2.3021 - val_accuracy: 0.1007\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3018 - accuracy: 0.1076 - val_loss: 2.3021 - val_accuracy: 0.1056\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3022 - accuracy: 0.1022 - val_loss: 2.3019 - val_accuracy: 0.1119\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3021 - accuracy: 0.1109 - val_loss: 2.3019 - val_accuracy: 0.1033\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3019 - accuracy: 0.1064 - val_loss: 2.3020 - val_accuracy: 0.1162\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3021 - accuracy: 0.1101 - val_loss: 2.3019 - val_accuracy: 0.1027\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3018 - accuracy: 0.1083 - val_loss: 2.3017 - val_accuracy: 0.1129\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3017 - accuracy: 0.1050 - val_loss: 2.3018 - val_accuracy: 0.1119\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3020 - accuracy: 0.1076 - val_loss: 2.3018 - val_accuracy: 0.1080\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3019 - accuracy: 0.1095 - val_loss: 2.3017 - val_accuracy: 0.1184\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3019 - accuracy: 0.1125 - val_loss: 2.3018 - val_accuracy: 0.1019\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3017 - accuracy: 0.1120 - val_loss: 2.3018 - val_accuracy: 0.1054\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3019 - accuracy: 0.1083 - val_loss: 2.3016 - val_accuracy: 0.1049\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.3017 - accuracy: 0.1097 - val_loss: 2.3016 - val_accuracy: 0.1113\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3016 - accuracy: 0.1125 - val_loss: 2.3015 - val_accuracy: 0.1045\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3013 - accuracy: 0.1112 - val_loss: 2.3014 - val_accuracy: 0.1183\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3015 - val_accuracy: 0.1123\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3015 - accuracy: 0.1187 - val_loss: 2.3015 - val_accuracy: 0.1026\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3018 - accuracy: 0.1101 - val_loss: 2.3014 - val_accuracy: 0.1182\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3014 - accuracy: 0.1166 - val_loss: 2.3014 - val_accuracy: 0.1156\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3013 - accuracy: 0.1154 - val_loss: 2.3013 - val_accuracy: 0.1148\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.3013 - accuracy: 0.1164 - val_loss: 2.3013 - val_accuracy: 0.1144\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3013 - accuracy: 0.1144 - val_loss: 2.3012 - val_accuracy: 0.1210\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3012 - accuracy: 0.1194 - val_loss: 2.3012 - val_accuracy: 0.1126\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3012 - accuracy: 0.1194 - val_loss: 2.3014 - val_accuracy: 0.1144\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3014 - accuracy: 0.1158 - val_loss: 2.3010 - val_accuracy: 0.1240\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3012 - accuracy: 0.1177 - val_loss: 2.3011 - val_accuracy: 0.1216\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3011 - accuracy: 0.1192 - val_loss: 2.3011 - val_accuracy: 0.1201\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3011 - accuracy: 0.1157 - val_loss: 2.3010 - val_accuracy: 0.1239\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3010 - accuracy: 0.1195 - val_loss: 2.3009 - val_accuracy: 0.1111\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3010 - accuracy: 0.1148 - val_loss: 2.3009 - val_accuracy: 0.1233\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3009 - accuracy: 0.1182 - val_loss: 2.3009 - val_accuracy: 0.1156\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3008 - accuracy: 0.1204 - val_loss: 2.3009 - val_accuracy: 0.1182\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3010 - accuracy: 0.1181 - val_loss: 2.3009 - val_accuracy: 0.1196\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3007 - accuracy: 0.1206 - val_loss: 2.3008 - val_accuracy: 0.1212\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3006 - accuracy: 0.1210 - val_loss: 2.3007 - val_accuracy: 0.1162\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3008 - accuracy: 0.1206 - val_loss: 2.3008 - val_accuracy: 0.1213\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3005 - accuracy: 0.1218 - val_loss: 2.3007 - val_accuracy: 0.1266\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3007 - accuracy: 0.1189 - val_loss: 2.3007 - val_accuracy: 0.1148\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3006 - accuracy: 0.1246 - val_loss: 2.3005 - val_accuracy: 0.1259\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3006 - accuracy: 0.1200 - val_loss: 2.3005 - val_accuracy: 0.1265\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3004 - accuracy: 0.1243 - val_loss: 2.3007 - val_accuracy: 0.1040\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3003 - accuracy: 0.1167 - val_loss: 2.3005 - val_accuracy: 0.1190\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3007 - accuracy: 0.1185 - val_loss: 2.3004 - val_accuracy: 0.1208\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3003 - accuracy: 0.1235 - val_loss: 2.3004 - val_accuracy: 0.1269\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3003 - accuracy: 0.1258 - val_loss: 2.3004 - val_accuracy: 0.1206\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3005 - accuracy: 0.1196 - val_loss: 2.3003 - val_accuracy: 0.1123\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3005 - accuracy: 0.1192 - val_loss: 2.3003 - val_accuracy: 0.1297\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3005 - accuracy: 0.1231 - val_loss: 2.3002 - val_accuracy: 0.1267\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3004 - accuracy: 0.1262 - val_loss: 2.3005 - val_accuracy: 0.1106\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3001 - accuracy: 0.1161 - val_loss: 2.3002 - val_accuracy: 0.1224\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3001 - accuracy: 0.1230 - val_loss: 2.3003 - val_accuracy: 0.1215\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3001 - accuracy: 0.1206 - val_loss: 2.3001 - val_accuracy: 0.1182\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3001 - accuracy: 0.1234 - val_loss: 2.3002 - val_accuracy: 0.1263\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3004 - accuracy: 0.1224 - val_loss: 2.3000 - val_accuracy: 0.1311\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2999 - accuracy: 0.1313 - val_loss: 2.2999 - val_accuracy: 0.1309\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2999 - accuracy: 0.1267 - val_loss: 2.3000 - val_accuracy: 0.1350\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2999 - accuracy: 0.1284 - val_loss: 2.2999 - val_accuracy: 0.1383\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3000 - accuracy: 0.1283 - val_loss: 2.2999 - val_accuracy: 0.1164\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.3000 - accuracy: 0.1261 - val_loss: 2.3001 - val_accuracy: 0.1200\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2998 - accuracy: 0.1226 - val_loss: 2.2997 - val_accuracy: 0.1358\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2998 - accuracy: 0.1313 - val_loss: 2.2998 - val_accuracy: 0.1279\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2998 - accuracy: 0.1272 - val_loss: 2.2997 - val_accuracy: 0.1207\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2996 - accuracy: 0.1247 - val_loss: 2.2998 - val_accuracy: 0.1264\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2993 - accuracy: 0.1292 - val_loss: 2.2995 - val_accuracy: 0.1271\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2995 - accuracy: 0.1308 - val_loss: 2.2995 - val_accuracy: 0.1338\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2997 - accuracy: 0.1349 - val_loss: 2.2995 - val_accuracy: 0.1262\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2993 - accuracy: 0.1288 - val_loss: 2.2996 - val_accuracy: 0.1324\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2993 - accuracy: 0.1261 - val_loss: 2.2996 - val_accuracy: 0.1278\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2993 - accuracy: 0.1264 - val_loss: 2.2994 - val_accuracy: 0.1331\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2995 - accuracy: 0.1342 - val_loss: 2.2993 - val_accuracy: 0.1356\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2993 - accuracy: 0.1357 - val_loss: 2.2993 - val_accuracy: 0.1248\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2993 - accuracy: 0.1292 - val_loss: 2.2993 - val_accuracy: 0.1306\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2995 - accuracy: 0.1340 - val_loss: 2.2992 - val_accuracy: 0.1352\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2994 - accuracy: 0.1335 - val_loss: 2.2993 - val_accuracy: 0.1278\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2991 - accuracy: 0.1292 - val_loss: 2.2993 - val_accuracy: 0.1274\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2990 - accuracy: 0.1310 - val_loss: 2.2991 - val_accuracy: 0.1434\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2992 - accuracy: 0.1331 - val_loss: 2.2991 - val_accuracy: 0.1427\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2989 - accuracy: 0.1374 - val_loss: 2.2991 - val_accuracy: 0.1279\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2990 - accuracy: 0.1270 - val_loss: 2.2990 - val_accuracy: 0.1249\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2987 - accuracy: 0.1272 - val_loss: 2.2989 - val_accuracy: 0.1338\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2987 - accuracy: 0.1296 - val_loss: 2.2990 - val_accuracy: 0.1327\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2986 - accuracy: 0.1346 - val_loss: 2.2988 - val_accuracy: 0.1464\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2988 - accuracy: 0.1360 - val_loss: 2.2988 - val_accuracy: 0.1316\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2986 - accuracy: 0.1340 - val_loss: 2.2987 - val_accuracy: 0.1412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8dhiLqdI45h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b51ea73-6478-4a6e-ba27-8195acc143ef"
      },
      "source": [
        "print('Evaluate NN model with sigmoid activations'); print('--'*40)\r\n",
        "results1 = model1.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results1[1]*100, 2), '%'))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with sigmoid activations\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.2987 - accuracy: 0.1412\n",
            "Validation accuracy: 14.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2dRFvufI9h5"
      },
      "source": [
        "**NN model, sigmoid activations, SGD optimizer, changing learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klw_8nF3I8S9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64923016-3e70-45d6-fa62-26b6b9f581dc"
      },
      "source": [
        "print('NN model with sigmoid activations - changing learning rate'); print('--'*40)\r\n",
        "# compiling the neural network classifier, sgd optimizer\r\n",
        "sgd = optimizers.SGD(lr = 0.001)\r\n",
        "model1.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model1.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with sigmoid activations - changing learning rate\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2988 - accuracy: 0.1411 - val_loss: 2.2986 - val_accuracy: 0.1469\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1538 - val_loss: 2.2986 - val_accuracy: 0.1489\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1492 - val_loss: 2.2986 - val_accuracy: 0.1486\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1522 - val_loss: 2.2986 - val_accuracy: 0.1478\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1471 - val_loss: 2.2986 - val_accuracy: 0.1478\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2984 - accuracy: 0.1530 - val_loss: 2.2986 - val_accuracy: 0.1477\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1498 - val_loss: 2.2986 - val_accuracy: 0.1480\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1517 - val_loss: 2.2986 - val_accuracy: 0.1474\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2983 - accuracy: 0.1468 - val_loss: 2.2986 - val_accuracy: 0.1470\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2984 - accuracy: 0.1471 - val_loss: 2.2986 - val_accuracy: 0.1468\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2981 - accuracy: 0.1519 - val_loss: 2.2986 - val_accuracy: 0.1468\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2981 - accuracy: 0.1473 - val_loss: 2.2986 - val_accuracy: 0.1472\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2983 - accuracy: 0.1495 - val_loss: 2.2986 - val_accuracy: 0.1473\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1493 - val_loss: 2.2986 - val_accuracy: 0.1475\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2985 - accuracy: 0.1480 - val_loss: 2.2986 - val_accuracy: 0.1474\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1493 - val_loss: 2.2986 - val_accuracy: 0.1473\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1480 - val_loss: 2.2986 - val_accuracy: 0.1469\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1482 - val_loss: 2.2986 - val_accuracy: 0.1474\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2987 - accuracy: 0.1457 - val_loss: 2.2986 - val_accuracy: 0.1472\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2983 - accuracy: 0.1470 - val_loss: 2.2986 - val_accuracy: 0.1474\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2985 - accuracy: 0.1469 - val_loss: 2.2986 - val_accuracy: 0.1474\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2981 - accuracy: 0.1491 - val_loss: 2.2985 - val_accuracy: 0.1472\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2983 - accuracy: 0.1483 - val_loss: 2.2985 - val_accuracy: 0.1486\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2985 - accuracy: 0.1481 - val_loss: 2.2985 - val_accuracy: 0.1479\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1496 - val_loss: 2.2985 - val_accuracy: 0.1473\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2986 - accuracy: 0.1478 - val_loss: 2.2985 - val_accuracy: 0.1474\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1468 - val_loss: 2.2985 - val_accuracy: 0.1470\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2985 - accuracy: 0.1463 - val_loss: 2.2985 - val_accuracy: 0.1474\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2985 - accuracy: 0.1472 - val_loss: 2.2985 - val_accuracy: 0.1473\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2983 - accuracy: 0.1471 - val_loss: 2.2985 - val_accuracy: 0.1467\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2980 - accuracy: 0.1522 - val_loss: 2.2985 - val_accuracy: 0.1474\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2983 - accuracy: 0.1468 - val_loss: 2.2985 - val_accuracy: 0.1480\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1508 - val_loss: 2.2985 - val_accuracy: 0.1467\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2987 - accuracy: 0.1456 - val_loss: 2.2985 - val_accuracy: 0.1468\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2983 - accuracy: 0.1505 - val_loss: 2.2985 - val_accuracy: 0.1472\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1521 - val_loss: 2.2985 - val_accuracy: 0.1469\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2981 - accuracy: 0.1480 - val_loss: 2.2985 - val_accuracy: 0.1474\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1501 - val_loss: 2.2985 - val_accuracy: 0.1481\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1479 - val_loss: 2.2985 - val_accuracy: 0.1485\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1500 - val_loss: 2.2985 - val_accuracy: 0.1481\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2983 - accuracy: 0.1494 - val_loss: 2.2985 - val_accuracy: 0.1475\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1458 - val_loss: 2.2985 - val_accuracy: 0.1476\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1469 - val_loss: 2.2985 - val_accuracy: 0.1487\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1500 - val_loss: 2.2984 - val_accuracy: 0.1483\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1502 - val_loss: 2.2984 - val_accuracy: 0.1481\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1482 - val_loss: 2.2984 - val_accuracy: 0.1485\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1490 - val_loss: 2.2984 - val_accuracy: 0.1487\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2981 - accuracy: 0.1503 - val_loss: 2.2984 - val_accuracy: 0.1483\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1512 - val_loss: 2.2984 - val_accuracy: 0.1478\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2980 - accuracy: 0.1506 - val_loss: 2.2984 - val_accuracy: 0.1482\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2979 - accuracy: 0.1500 - val_loss: 2.2984 - val_accuracy: 0.1479\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2981 - accuracy: 0.1515 - val_loss: 2.2984 - val_accuracy: 0.1471\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2981 - accuracy: 0.1463 - val_loss: 2.2984 - val_accuracy: 0.1476\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1465 - val_loss: 2.2984 - val_accuracy: 0.1476\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2981 - accuracy: 0.1484 - val_loss: 2.2984 - val_accuracy: 0.1480\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2981 - accuracy: 0.1510 - val_loss: 2.2984 - val_accuracy: 0.1483\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2980 - accuracy: 0.1497 - val_loss: 2.2984 - val_accuracy: 0.1482\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2981 - accuracy: 0.1525 - val_loss: 2.2984 - val_accuracy: 0.1486\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1513 - val_loss: 2.2984 - val_accuracy: 0.1489\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2980 - accuracy: 0.1490 - val_loss: 2.2984 - val_accuracy: 0.1489\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2980 - accuracy: 0.1534 - val_loss: 2.2984 - val_accuracy: 0.1478\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1471 - val_loss: 2.2984 - val_accuracy: 0.1488\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2985 - accuracy: 0.1470 - val_loss: 2.2984 - val_accuracy: 0.1488\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2983 - accuracy: 0.1515 - val_loss: 2.2984 - val_accuracy: 0.1482\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2985 - accuracy: 0.1481 - val_loss: 2.2984 - val_accuracy: 0.1475\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2979 - accuracy: 0.1458 - val_loss: 2.2983 - val_accuracy: 0.1474\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2980 - accuracy: 0.1525 - val_loss: 2.2983 - val_accuracy: 0.1472\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2980 - accuracy: 0.1489 - val_loss: 2.2983 - val_accuracy: 0.1474\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1468 - val_loss: 2.2983 - val_accuracy: 0.1483\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2984 - accuracy: 0.1468 - val_loss: 2.2983 - val_accuracy: 0.1488\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2980 - accuracy: 0.1511 - val_loss: 2.2983 - val_accuracy: 0.1487\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2980 - accuracy: 0.1509 - val_loss: 2.2983 - val_accuracy: 0.1488\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1504 - val_loss: 2.2983 - val_accuracy: 0.1491\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1522 - val_loss: 2.2983 - val_accuracy: 0.1487\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1505 - val_loss: 2.2983 - val_accuracy: 0.1480\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2979 - accuracy: 0.1501 - val_loss: 2.2983 - val_accuracy: 0.1488\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1491 - val_loss: 2.2983 - val_accuracy: 0.1481\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2981 - accuracy: 0.1518 - val_loss: 2.2983 - val_accuracy: 0.1481\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2979 - accuracy: 0.1497 - val_loss: 2.2983 - val_accuracy: 0.1479\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2978 - accuracy: 0.1503 - val_loss: 2.2983 - val_accuracy: 0.1483\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2980 - accuracy: 0.1507 - val_loss: 2.2983 - val_accuracy: 0.1487\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2983 - accuracy: 0.1516 - val_loss: 2.2983 - val_accuracy: 0.1480\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2979 - accuracy: 0.1485 - val_loss: 2.2983 - val_accuracy: 0.1480\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2979 - accuracy: 0.1504 - val_loss: 2.2983 - val_accuracy: 0.1489\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2979 - accuracy: 0.1529 - val_loss: 2.2983 - val_accuracy: 0.1485\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.2981 - accuracy: 0.1482 - val_loss: 2.2983 - val_accuracy: 0.1482\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2981 - accuracy: 0.1507 - val_loss: 2.2982 - val_accuracy: 0.1497\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2980 - accuracy: 0.1529 - val_loss: 2.2982 - val_accuracy: 0.1497\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1490 - val_loss: 2.2982 - val_accuracy: 0.1493\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2982 - accuracy: 0.1492 - val_loss: 2.2982 - val_accuracy: 0.1491\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2980 - accuracy: 0.1490 - val_loss: 2.2982 - val_accuracy: 0.1493\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2979 - accuracy: 0.1505 - val_loss: 2.2982 - val_accuracy: 0.1500\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2981 - accuracy: 0.1509 - val_loss: 2.2982 - val_accuracy: 0.1502\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2979 - accuracy: 0.1563 - val_loss: 2.2982 - val_accuracy: 0.1496\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2981 - accuracy: 0.1512 - val_loss: 2.2982 - val_accuracy: 0.1488\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2978 - accuracy: 0.1474 - val_loss: 2.2982 - val_accuracy: 0.1495\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2979 - accuracy: 0.1540 - val_loss: 2.2982 - val_accuracy: 0.1491\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2979 - accuracy: 0.1508 - val_loss: 2.2982 - val_accuracy: 0.1491\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2980 - accuracy: 0.1475 - val_loss: 2.2982 - val_accuracy: 0.1498\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2977 - accuracy: 0.1487 - val_loss: 2.2982 - val_accuracy: 0.1508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HngOoMkGJKW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a21422-08b1-46cc-b6e8-61c37c9ea59f"
      },
      "source": [
        "print('Evaluate NN model with sigmoid activations - changing learning rate'); print('--'*40)\r\n",
        "results1 = model1.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results1[1]*100, 2), '%'))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with sigmoid activations - changing learning rate\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.2982 - accuracy: 0.1508\n",
            "Validation accuracy: 15.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-BKo4s3JMrz"
      },
      "source": [
        "**Observation 3 - NN model with sigmoid activations**\r\n",
        "\r\n",
        "*   Validation score is very low, changing learning rate further reduces it.\r\n",
        "*   Optimizing the network in order to better learn the patterns in the dataset.\r\n",
        "*   Best model out of the above is the one with lower learning rate using SGD optimizer and sigmoid activations. \r\n",
        "*   Next, let's use relu activations and see if the score improves.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**NN model, relu activations, SGD optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR2WncxNJjjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d53df0d-2412-4744-daa0-876e313e2276"
      },
      "source": [
        "%time\r\n",
        "print('NN model with relu activations and sgd optimizers'); print('--'*40)\r\n",
        "# Initialize the neural network classifier\r\n",
        "model2 = Sequential()\r\n",
        "\r\n",
        "# Input Layer - adding input layer and activation functions relu\r\n",
        "model2.add(Dense(128, input_shape = (1024, )))\r\n",
        "# Adding activation function\r\n",
        "model2.add(Activation('relu'))\r\n",
        "\r\n",
        "#Hidden Layer 1 - adding first hidden layer\r\n",
        "model2.add(Dense(64))\r\n",
        "# Adding activation function\r\n",
        "model2.add(Activation('relu'))\r\n",
        "\r\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\r\n",
        "model2.add(Dense(10))\r\n",
        "# Adding activation function - softmax for multiclass classification\r\n",
        "model2.add(Activation('softmax'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 s, sys: 0 ns, total: 2 s\n",
            "Wall time: 5.01 s\n",
            "NN model with relu activations and sgd optimizers\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjytN5JwJndD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac1cb648-2ff9-4fee-8003-fcc8d49021e2"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 140,106\n",
            "Trainable params: 140,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf88WxXhJrGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c47ead3-864e-4853-c283-f7545f395e12"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\r\n",
        "sgd = optimizers.SGD(lr = 0.01)\r\n",
        "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 2.3101 - accuracy: 0.1155 - val_loss: 2.2903 - val_accuracy: 0.1455\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2879 - accuracy: 0.1492 - val_loss: 2.2813 - val_accuracy: 0.1431\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2770 - accuracy: 0.1656 - val_loss: 2.2672 - val_accuracy: 0.1981\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2655 - accuracy: 0.1926 - val_loss: 2.2535 - val_accuracy: 0.2119\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2501 - accuracy: 0.2132 - val_loss: 2.2360 - val_accuracy: 0.2342\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2305 - accuracy: 0.2531 - val_loss: 2.2134 - val_accuracy: 0.2679\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2081 - accuracy: 0.2650 - val_loss: 2.1887 - val_accuracy: 0.2767\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.1789 - accuracy: 0.2893 - val_loss: 2.1535 - val_accuracy: 0.3088\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.1455 - accuracy: 0.3044 - val_loss: 2.1144 - val_accuracy: 0.3355\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.1024 - accuracy: 0.3329 - val_loss: 2.0700 - val_accuracy: 0.3456\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.0537 - accuracy: 0.3528 - val_loss: 2.0152 - val_accuracy: 0.3742\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.0029 - accuracy: 0.3752 - val_loss: 1.9622 - val_accuracy: 0.3867\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.9490 - accuracy: 0.3897 - val_loss: 1.9065 - val_accuracy: 0.3962\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.8928 - accuracy: 0.4141 - val_loss: 1.8400 - val_accuracy: 0.4293\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.8267 - accuracy: 0.4354 - val_loss: 1.8001 - val_accuracy: 0.4216\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.7772 - accuracy: 0.4514 - val_loss: 1.7332 - val_accuracy: 0.4653\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.7122 - accuracy: 0.4765 - val_loss: 1.6750 - val_accuracy: 0.4826\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.6669 - accuracy: 0.4921 - val_loss: 1.6547 - val_accuracy: 0.4771\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.6244 - accuracy: 0.5027 - val_loss: 1.5871 - val_accuracy: 0.5193\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.5840 - accuracy: 0.5177 - val_loss: 1.5438 - val_accuracy: 0.5311\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.5395 - accuracy: 0.5299 - val_loss: 1.5090 - val_accuracy: 0.5468\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.5098 - accuracy: 0.5465 - val_loss: 1.4884 - val_accuracy: 0.5560\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.4722 - accuracy: 0.5563 - val_loss: 1.4447 - val_accuracy: 0.5649\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.4410 - accuracy: 0.5725 - val_loss: 1.4145 - val_accuracy: 0.5763\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.4113 - accuracy: 0.5789 - val_loss: 1.3765 - val_accuracy: 0.5976\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.3809 - accuracy: 0.5891 - val_loss: 1.3578 - val_accuracy: 0.5893\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.3533 - accuracy: 0.5984 - val_loss: 1.3966 - val_accuracy: 0.5615\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.3356 - accuracy: 0.6027 - val_loss: 1.2955 - val_accuracy: 0.6210\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.3103 - accuracy: 0.6125 - val_loss: 1.3255 - val_accuracy: 0.5976\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.2766 - accuracy: 0.6239 - val_loss: 1.2521 - val_accuracy: 0.6356\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.2615 - accuracy: 0.6266 - val_loss: 1.2258 - val_accuracy: 0.6451\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.2492 - accuracy: 0.6279 - val_loss: 1.2662 - val_accuracy: 0.6173\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.2106 - accuracy: 0.6439 - val_loss: 1.2524 - val_accuracy: 0.6124\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1967 - accuracy: 0.6489 - val_loss: 1.1825 - val_accuracy: 0.6537\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1787 - accuracy: 0.6510 - val_loss: 1.1531 - val_accuracy: 0.6635\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1553 - accuracy: 0.6592 - val_loss: 1.1464 - val_accuracy: 0.6634\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1461 - accuracy: 0.6620 - val_loss: 1.1619 - val_accuracy: 0.6516\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1411 - accuracy: 0.6591 - val_loss: 1.1211 - val_accuracy: 0.6700\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1174 - accuracy: 0.6689 - val_loss: 1.1107 - val_accuracy: 0.6724\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1059 - accuracy: 0.6743 - val_loss: 1.0829 - val_accuracy: 0.6814\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.0867 - accuracy: 0.6781 - val_loss: 1.0625 - val_accuracy: 0.6891\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0790 - accuracy: 0.6783 - val_loss: 1.0578 - val_accuracy: 0.6879\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0638 - accuracy: 0.6850 - val_loss: 1.0562 - val_accuracy: 0.6899\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0529 - accuracy: 0.6858 - val_loss: 1.0378 - val_accuracy: 0.6935\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0443 - accuracy: 0.6887 - val_loss: 1.0144 - val_accuracy: 0.7031\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0380 - accuracy: 0.6876 - val_loss: 1.0096 - val_accuracy: 0.7039\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.0162 - accuracy: 0.7003 - val_loss: 1.0300 - val_accuracy: 0.6915\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0098 - accuracy: 0.6995 - val_loss: 0.9939 - val_accuracy: 0.7059\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9955 - accuracy: 0.7037 - val_loss: 1.0121 - val_accuracy: 0.6959\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9924 - accuracy: 0.7048 - val_loss: 0.9728 - val_accuracy: 0.7137\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9846 - accuracy: 0.7044 - val_loss: 0.9747 - val_accuracy: 0.7122\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9700 - accuracy: 0.7127 - val_loss: 0.9576 - val_accuracy: 0.7163\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9653 - accuracy: 0.7157 - val_loss: 0.9473 - val_accuracy: 0.7218\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9511 - accuracy: 0.7162 - val_loss: 0.9535 - val_accuracy: 0.7186\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9419 - accuracy: 0.7213 - val_loss: 0.9376 - val_accuracy: 0.7242\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9336 - accuracy: 0.7212 - val_loss: 0.9333 - val_accuracy: 0.7241\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9307 - accuracy: 0.7265 - val_loss: 0.9518 - val_accuracy: 0.7146\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9183 - accuracy: 0.7285 - val_loss: 0.9276 - val_accuracy: 0.7261\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9200 - accuracy: 0.7287 - val_loss: 0.9218 - val_accuracy: 0.7254\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9173 - accuracy: 0.7289 - val_loss: 0.9154 - val_accuracy: 0.7283\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9158 - accuracy: 0.7278 - val_loss: 0.8817 - val_accuracy: 0.7425\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8885 - accuracy: 0.7379 - val_loss: 0.8791 - val_accuracy: 0.7406\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8884 - accuracy: 0.7388 - val_loss: 0.8911 - val_accuracy: 0.7379\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8810 - accuracy: 0.7412 - val_loss: 0.8767 - val_accuracy: 0.7414\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8707 - accuracy: 0.7443 - val_loss: 0.9102 - val_accuracy: 0.7281\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8595 - accuracy: 0.7417 - val_loss: 0.8871 - val_accuracy: 0.7371\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8669 - accuracy: 0.7449 - val_loss: 0.8438 - val_accuracy: 0.7534\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8546 - accuracy: 0.7456 - val_loss: 0.8457 - val_accuracy: 0.7517\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8533 - accuracy: 0.7487 - val_loss: 0.8503 - val_accuracy: 0.7465\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8428 - accuracy: 0.7477 - val_loss: 0.8626 - val_accuracy: 0.7438\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8412 - accuracy: 0.7498 - val_loss: 0.8382 - val_accuracy: 0.7529\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8378 - accuracy: 0.7502 - val_loss: 0.8147 - val_accuracy: 0.7630\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8340 - accuracy: 0.7519 - val_loss: 0.8383 - val_accuracy: 0.7515\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8173 - accuracy: 0.7587 - val_loss: 0.8175 - val_accuracy: 0.7613\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8261 - accuracy: 0.7550 - val_loss: 0.8407 - val_accuracy: 0.7497\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8006 - accuracy: 0.7616 - val_loss: 0.8016 - val_accuracy: 0.7640\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8101 - accuracy: 0.7614 - val_loss: 0.8073 - val_accuracy: 0.7615\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8027 - accuracy: 0.7624 - val_loss: 0.7975 - val_accuracy: 0.7663\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7937 - accuracy: 0.7668 - val_loss: 0.8299 - val_accuracy: 0.7514\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7929 - accuracy: 0.7647 - val_loss: 0.7829 - val_accuracy: 0.7713\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7930 - accuracy: 0.7676 - val_loss: 0.7846 - val_accuracy: 0.7707\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7815 - accuracy: 0.7697 - val_loss: 0.7665 - val_accuracy: 0.7766\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7853 - accuracy: 0.7676 - val_loss: 0.7733 - val_accuracy: 0.7732\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7597 - accuracy: 0.7766 - val_loss: 0.7712 - val_accuracy: 0.7750\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7704 - accuracy: 0.7735 - val_loss: 0.7583 - val_accuracy: 0.7803\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7663 - accuracy: 0.7739 - val_loss: 0.8013 - val_accuracy: 0.7618\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7685 - accuracy: 0.7735 - val_loss: 0.7596 - val_accuracy: 0.7788\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7549 - accuracy: 0.7754 - val_loss: 0.7641 - val_accuracy: 0.7769\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7556 - accuracy: 0.7748 - val_loss: 0.7675 - val_accuracy: 0.7717\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7563 - accuracy: 0.7765 - val_loss: 0.7412 - val_accuracy: 0.7837\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7446 - accuracy: 0.7798 - val_loss: 0.8739 - val_accuracy: 0.7376\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7339 - accuracy: 0.7833 - val_loss: 0.7271 - val_accuracy: 0.7896\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7343 - accuracy: 0.7806 - val_loss: 0.7536 - val_accuracy: 0.7783\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7108 - accuracy: 0.7871 - val_loss: 0.7602 - val_accuracy: 0.7715\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7192 - accuracy: 0.7861 - val_loss: 0.7271 - val_accuracy: 0.7874\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7198 - accuracy: 0.7869 - val_loss: 0.7396 - val_accuracy: 0.7828\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7220 - accuracy: 0.7856 - val_loss: 0.7415 - val_accuracy: 0.7811\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7094 - accuracy: 0.7928 - val_loss: 0.7342 - val_accuracy: 0.7833\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7163 - accuracy: 0.7868 - val_loss: 0.7629 - val_accuracy: 0.7704\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7000 - accuracy: 0.7922 - val_loss: 0.7212 - val_accuracy: 0.7904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7NzaguYJ8tL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74197255-b899-460c-9edc-819d87c49890"
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\r\n",
        "results2 = model2.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7212 - accuracy: 0.7904\n",
            "Validation accuracy: 79.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbU4cVvsJ7gw"
      },
      "source": [
        "**NN model, relu activations, SGD optimizer, changing learning rate**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FilVpokKNbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717bd96d-49ba-479e-fe37-454928ae5122"
      },
      "source": [
        "%time\r\n",
        "print('NN model with relu activations and sgd optimizers - changing learning rate'); print('--'*40)\r\n",
        "# compiling the neural network classifier, sgd optimizer\r\n",
        "sgd = optimizers.SGD(lr = 0.001)\r\n",
        "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 s, sys: 0 ns, total: 2 s\n",
            "Wall time: 4.53 s\n",
            "NN model with relu activations and sgd optimizers - changing learning rate\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 0.6588 - accuracy: 0.8101 - val_loss: 0.6859 - val_accuracy: 0.8011\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6641 - accuracy: 0.8064 - val_loss: 0.6858 - val_accuracy: 0.8019\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6659 - accuracy: 0.8082 - val_loss: 0.6846 - val_accuracy: 0.8015\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6704 - accuracy: 0.8057 - val_loss: 0.6845 - val_accuracy: 0.8025\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6591 - accuracy: 0.8094 - val_loss: 0.6836 - val_accuracy: 0.8030\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6650 - accuracy: 0.8071 - val_loss: 0.6837 - val_accuracy: 0.8021\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6665 - accuracy: 0.8076 - val_loss: 0.6832 - val_accuracy: 0.8025\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6615 - accuracy: 0.8052 - val_loss: 0.6824 - val_accuracy: 0.8026\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6532 - accuracy: 0.8098 - val_loss: 0.6819 - val_accuracy: 0.8034\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6626 - accuracy: 0.8095 - val_loss: 0.6823 - val_accuracy: 0.8026\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6506 - accuracy: 0.8133 - val_loss: 0.6813 - val_accuracy: 0.8028\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6570 - accuracy: 0.8096 - val_loss: 0.6808 - val_accuracy: 0.8036\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6633 - accuracy: 0.8093 - val_loss: 0.6823 - val_accuracy: 0.8021\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6526 - accuracy: 0.8074 - val_loss: 0.6802 - val_accuracy: 0.8030\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6503 - accuracy: 0.8122 - val_loss: 0.6802 - val_accuracy: 0.8034\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6593 - accuracy: 0.8088 - val_loss: 0.6811 - val_accuracy: 0.8029\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6590 - accuracy: 0.8088 - val_loss: 0.6802 - val_accuracy: 0.8033\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6600 - accuracy: 0.8091 - val_loss: 0.6794 - val_accuracy: 0.8035\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6626 - accuracy: 0.8085 - val_loss: 0.6783 - val_accuracy: 0.8040\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6624 - accuracy: 0.8086 - val_loss: 0.6784 - val_accuracy: 0.8038\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6581 - accuracy: 0.8097 - val_loss: 0.6787 - val_accuracy: 0.8037\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6642 - accuracy: 0.8066 - val_loss: 0.6778 - val_accuracy: 0.8037\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6521 - accuracy: 0.8120 - val_loss: 0.6782 - val_accuracy: 0.8040\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6595 - accuracy: 0.8076 - val_loss: 0.6769 - val_accuracy: 0.8049\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6543 - accuracy: 0.8115 - val_loss: 0.6763 - val_accuracy: 0.8046\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6542 - accuracy: 0.8123 - val_loss: 0.6777 - val_accuracy: 0.8036\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6516 - accuracy: 0.8144 - val_loss: 0.6760 - val_accuracy: 0.8044\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6495 - accuracy: 0.8106 - val_loss: 0.6760 - val_accuracy: 0.8051\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6526 - accuracy: 0.8110 - val_loss: 0.6750 - val_accuracy: 0.8050\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6530 - accuracy: 0.8135 - val_loss: 0.6759 - val_accuracy: 0.8051\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6534 - accuracy: 0.8122 - val_loss: 0.6746 - val_accuracy: 0.8048\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6519 - accuracy: 0.8088 - val_loss: 0.6744 - val_accuracy: 0.8049\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6402 - accuracy: 0.8151 - val_loss: 0.6735 - val_accuracy: 0.8055\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6400 - accuracy: 0.8170 - val_loss: 0.6733 - val_accuracy: 0.8057\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6480 - accuracy: 0.8125 - val_loss: 0.6726 - val_accuracy: 0.8062\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6451 - accuracy: 0.8135 - val_loss: 0.6736 - val_accuracy: 0.8052\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6486 - accuracy: 0.8127 - val_loss: 0.6741 - val_accuracy: 0.8051\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6462 - accuracy: 0.8128 - val_loss: 0.6719 - val_accuracy: 0.8059\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6458 - accuracy: 0.8133 - val_loss: 0.6725 - val_accuracy: 0.8061\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6481 - accuracy: 0.8134 - val_loss: 0.6723 - val_accuracy: 0.8057\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6457 - accuracy: 0.8121 - val_loss: 0.6710 - val_accuracy: 0.8061\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6415 - accuracy: 0.8138 - val_loss: 0.6706 - val_accuracy: 0.8062\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6392 - accuracy: 0.8140 - val_loss: 0.6700 - val_accuracy: 0.8066\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6365 - accuracy: 0.8169 - val_loss: 0.6696 - val_accuracy: 0.8069\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6449 - accuracy: 0.8131 - val_loss: 0.6694 - val_accuracy: 0.8065\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6505 - accuracy: 0.8098 - val_loss: 0.6696 - val_accuracy: 0.8071\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6438 - accuracy: 0.8136 - val_loss: 0.6690 - val_accuracy: 0.8074\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6435 - accuracy: 0.8130 - val_loss: 0.6686 - val_accuracy: 0.8073\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6390 - accuracy: 0.8167 - val_loss: 0.6681 - val_accuracy: 0.8069\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6446 - accuracy: 0.8133 - val_loss: 0.6680 - val_accuracy: 0.8074\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6410 - accuracy: 0.8144 - val_loss: 0.6676 - val_accuracy: 0.8075\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6452 - accuracy: 0.8144 - val_loss: 0.6688 - val_accuracy: 0.8070\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6444 - accuracy: 0.8146 - val_loss: 0.6669 - val_accuracy: 0.8079\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6443 - accuracy: 0.8134 - val_loss: 0.6662 - val_accuracy: 0.8079\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6406 - accuracy: 0.8147 - val_loss: 0.6665 - val_accuracy: 0.8076\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6387 - accuracy: 0.8158 - val_loss: 0.6653 - val_accuracy: 0.8079\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6364 - accuracy: 0.8165 - val_loss: 0.6664 - val_accuracy: 0.8082\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6408 - accuracy: 0.8141 - val_loss: 0.6648 - val_accuracy: 0.8084\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6405 - accuracy: 0.8176 - val_loss: 0.6650 - val_accuracy: 0.8082\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6486 - accuracy: 0.8131 - val_loss: 0.6652 - val_accuracy: 0.8082\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6491 - accuracy: 0.8125 - val_loss: 0.6652 - val_accuracy: 0.8080\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6388 - accuracy: 0.8155 - val_loss: 0.6636 - val_accuracy: 0.8086\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6414 - accuracy: 0.8132 - val_loss: 0.6641 - val_accuracy: 0.8085\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6370 - accuracy: 0.8162 - val_loss: 0.6657 - val_accuracy: 0.8078\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6314 - accuracy: 0.8164 - val_loss: 0.6638 - val_accuracy: 0.8083\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6319 - accuracy: 0.8167 - val_loss: 0.6626 - val_accuracy: 0.8086\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6305 - accuracy: 0.8176 - val_loss: 0.6627 - val_accuracy: 0.8091\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.8182 - val_loss: 0.6618 - val_accuracy: 0.8091\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6433 - accuracy: 0.8129 - val_loss: 0.6618 - val_accuracy: 0.8098\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6333 - accuracy: 0.8165 - val_loss: 0.6614 - val_accuracy: 0.8093\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6423 - accuracy: 0.8132 - val_loss: 0.6608 - val_accuracy: 0.8089\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6422 - accuracy: 0.8114 - val_loss: 0.6602 - val_accuracy: 0.8098\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6417 - accuracy: 0.8126 - val_loss: 0.6621 - val_accuracy: 0.8092\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6352 - accuracy: 0.8172 - val_loss: 0.6606 - val_accuracy: 0.8099\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6420 - accuracy: 0.8163 - val_loss: 0.6598 - val_accuracy: 0.8096\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6307 - accuracy: 0.8171 - val_loss: 0.6602 - val_accuracy: 0.8098\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6350 - accuracy: 0.8153 - val_loss: 0.6596 - val_accuracy: 0.8100\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6397 - accuracy: 0.8135 - val_loss: 0.6596 - val_accuracy: 0.8104\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6326 - accuracy: 0.8185 - val_loss: 0.6599 - val_accuracy: 0.8094\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6364 - accuracy: 0.8155 - val_loss: 0.6587 - val_accuracy: 0.8104\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6374 - accuracy: 0.8171 - val_loss: 0.6581 - val_accuracy: 0.8105\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6359 - accuracy: 0.8166 - val_loss: 0.6574 - val_accuracy: 0.8108\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6357 - accuracy: 0.8153 - val_loss: 0.6572 - val_accuracy: 0.8106\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6229 - accuracy: 0.8206 - val_loss: 0.6574 - val_accuracy: 0.8105\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6369 - accuracy: 0.8152 - val_loss: 0.6567 - val_accuracy: 0.8106\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6370 - accuracy: 0.8158 - val_loss: 0.6571 - val_accuracy: 0.8100\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6342 - accuracy: 0.8185 - val_loss: 0.6556 - val_accuracy: 0.8110\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6327 - accuracy: 0.8150 - val_loss: 0.6573 - val_accuracy: 0.8110\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6336 - accuracy: 0.8165 - val_loss: 0.6557 - val_accuracy: 0.8109\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6357 - accuracy: 0.8161 - val_loss: 0.6550 - val_accuracy: 0.8113\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6289 - accuracy: 0.8188 - val_loss: 0.6559 - val_accuracy: 0.8109\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6205 - accuracy: 0.8208 - val_loss: 0.6543 - val_accuracy: 0.8115\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6266 - accuracy: 0.8194 - val_loss: 0.6561 - val_accuracy: 0.8109\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6147 - accuracy: 0.8227 - val_loss: 0.6547 - val_accuracy: 0.8121\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6221 - accuracy: 0.8212 - val_loss: 0.6553 - val_accuracy: 0.8106\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6256 - accuracy: 0.8188 - val_loss: 0.6541 - val_accuracy: 0.8114\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6306 - accuracy: 0.8173 - val_loss: 0.6536 - val_accuracy: 0.8124\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6239 - accuracy: 0.8186 - val_loss: 0.6534 - val_accuracy: 0.8118\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6272 - accuracy: 0.8181 - val_loss: 0.6521 - val_accuracy: 0.8123\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6176 - accuracy: 0.8217 - val_loss: 0.6523 - val_accuracy: 0.8121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw6blLAhKQuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ff85a5-dd33-46a2-ceab-c1cc36bcf8e0"
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\r\n",
        "results2 = model2.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6523 - accuracy: 0.8121\n",
            "Validation accuracy: 81.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi2ZE7xaDPY5"
      },
      "source": [
        "**NN model, relu activations, adam optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C69F3WX0KfSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3464d878-6e91-4c21-b824-b3b155ecd487"
      },
      "source": [
        "%time\r\n",
        "print('NN model with relu activations and adam optimizer'); print('--'*40)\r\n",
        "# compiling the neural network classifier, adam optimizer\r\n",
        "adam = optimizers.Adam(lr = 0.01)\r\n",
        "model2.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 s, sys: 1 s, total: 4 s\n",
            "Wall time: 7.87 s\n",
            "NN model with relu activations and adam optimizer\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 6.9578 - accuracy: 0.1306 - val_loss: 2.2255 - val_accuracy: 0.1825\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.1936 - accuracy: 0.1858 - val_loss: 2.0508 - val_accuracy: 0.2329\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.9863 - accuracy: 0.2703 - val_loss: 1.8088 - val_accuracy: 0.3566\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.6383 - accuracy: 0.4418 - val_loss: 1.4876 - val_accuracy: 0.4878\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.4055 - accuracy: 0.5370 - val_loss: 1.3395 - val_accuracy: 0.5571\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.2978 - accuracy: 0.5772 - val_loss: 1.2470 - val_accuracy: 0.5929\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.2761 - accuracy: 0.5841 - val_loss: 1.2402 - val_accuracy: 0.5984\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.2223 - accuracy: 0.6064 - val_loss: 1.2875 - val_accuracy: 0.5778\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.2199 - accuracy: 0.6069 - val_loss: 1.2317 - val_accuracy: 0.5969\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1659 - accuracy: 0.6255 - val_loss: 1.1790 - val_accuracy: 0.6337\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1628 - accuracy: 0.6316 - val_loss: 1.2276 - val_accuracy: 0.6087\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1348 - accuracy: 0.6400 - val_loss: 1.0818 - val_accuracy: 0.6632\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1160 - accuracy: 0.6508 - val_loss: 1.1608 - val_accuracy: 0.6319\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1274 - accuracy: 0.6432 - val_loss: 1.1174 - val_accuracy: 0.6517\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0808 - accuracy: 0.6621 - val_loss: 1.1174 - val_accuracy: 0.6492\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0645 - accuracy: 0.6679 - val_loss: 1.0940 - val_accuracy: 0.6582\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0584 - accuracy: 0.6715 - val_loss: 1.0668 - val_accuracy: 0.6678\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0415 - accuracy: 0.6783 - val_loss: 1.0880 - val_accuracy: 0.6546\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0670 - accuracy: 0.6699 - val_loss: 1.0124 - val_accuracy: 0.6878\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0314 - accuracy: 0.6811 - val_loss: 1.0404 - val_accuracy: 0.6820\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0258 - accuracy: 0.6860 - val_loss: 1.0045 - val_accuracy: 0.6912\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0288 - accuracy: 0.6807 - val_loss: 1.0439 - val_accuracy: 0.6728\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0009 - accuracy: 0.6870 - val_loss: 0.9661 - val_accuracy: 0.7094\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9835 - accuracy: 0.6955 - val_loss: 1.0148 - val_accuracy: 0.6868\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9783 - accuracy: 0.6996 - val_loss: 1.0067 - val_accuracy: 0.6885\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9578 - accuracy: 0.7061 - val_loss: 0.9867 - val_accuracy: 0.7020\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9537 - accuracy: 0.7056 - val_loss: 1.0462 - val_accuracy: 0.6717\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9628 - accuracy: 0.7009 - val_loss: 1.0158 - val_accuracy: 0.6802\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9462 - accuracy: 0.7064 - val_loss: 0.9771 - val_accuracy: 0.6969\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9443 - accuracy: 0.7060 - val_loss: 0.9768 - val_accuracy: 0.6979\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9523 - accuracy: 0.7063 - val_loss: 0.9348 - val_accuracy: 0.7127\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9330 - accuracy: 0.7129 - val_loss: 0.9444 - val_accuracy: 0.7080\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9273 - accuracy: 0.7146 - val_loss: 0.9608 - val_accuracy: 0.7052\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.9273 - accuracy: 0.7154 - val_loss: 0.9066 - val_accuracy: 0.7240\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.9033 - accuracy: 0.7231 - val_loss: 0.9283 - val_accuracy: 0.7150\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9036 - accuracy: 0.7189 - val_loss: 0.9470 - val_accuracy: 0.7090\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.9263 - accuracy: 0.7128 - val_loss: 0.9076 - val_accuracy: 0.7236\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.9355 - accuracy: 0.7090 - val_loss: 0.8605 - val_accuracy: 0.7400\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8967 - accuracy: 0.7262 - val_loss: 0.9742 - val_accuracy: 0.7040\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.9094 - accuracy: 0.7222 - val_loss: 1.0089 - val_accuracy: 0.6914\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9142 - accuracy: 0.7160 - val_loss: 0.9383 - val_accuracy: 0.7105\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9007 - accuracy: 0.7230 - val_loss: 0.9096 - val_accuracy: 0.7231\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8854 - accuracy: 0.7247 - val_loss: 0.8559 - val_accuracy: 0.7398\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8876 - accuracy: 0.7255 - val_loss: 0.9269 - val_accuracy: 0.7155\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8938 - accuracy: 0.7254 - val_loss: 0.8592 - val_accuracy: 0.7418\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8874 - accuracy: 0.7280 - val_loss: 0.8900 - val_accuracy: 0.7262\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8831 - accuracy: 0.7301 - val_loss: 0.8574 - val_accuracy: 0.7362\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8939 - accuracy: 0.7243 - val_loss: 0.9436 - val_accuracy: 0.7047\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8701 - accuracy: 0.7316 - val_loss: 0.9099 - val_accuracy: 0.7262\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8818 - accuracy: 0.7287 - val_loss: 0.8851 - val_accuracy: 0.7284\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8639 - accuracy: 0.7358 - val_loss: 0.8634 - val_accuracy: 0.7370\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8630 - accuracy: 0.7376 - val_loss: 0.8570 - val_accuracy: 0.7386\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8527 - accuracy: 0.7383 - val_loss: 0.8875 - val_accuracy: 0.7264\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8869 - accuracy: 0.7286 - val_loss: 0.8793 - val_accuracy: 0.7336\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8684 - accuracy: 0.7330 - val_loss: 0.8745 - val_accuracy: 0.7333\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8534 - accuracy: 0.7382 - val_loss: 0.9077 - val_accuracy: 0.7213\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8492 - accuracy: 0.7388 - val_loss: 0.9649 - val_accuracy: 0.7026\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8984 - accuracy: 0.7245 - val_loss: 0.8475 - val_accuracy: 0.7433\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8554 - accuracy: 0.7396 - val_loss: 0.8339 - val_accuracy: 0.7433\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8549 - accuracy: 0.7370 - val_loss: 0.8574 - val_accuracy: 0.7352\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8577 - accuracy: 0.7387 - val_loss: 0.8784 - val_accuracy: 0.7306\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8419 - accuracy: 0.7419 - val_loss: 0.8633 - val_accuracy: 0.7429\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8426 - accuracy: 0.7459 - val_loss: 0.8485 - val_accuracy: 0.7417\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8591 - accuracy: 0.7364 - val_loss: 0.8535 - val_accuracy: 0.7431\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8371 - accuracy: 0.7428 - val_loss: 0.8844 - val_accuracy: 0.7296\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8478 - accuracy: 0.7361 - val_loss: 0.8166 - val_accuracy: 0.7521\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8336 - accuracy: 0.7431 - val_loss: 0.8288 - val_accuracy: 0.7457\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8447 - accuracy: 0.7405 - val_loss: 0.8275 - val_accuracy: 0.7492\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8539 - accuracy: 0.7390 - val_loss: 0.8277 - val_accuracy: 0.7488\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8280 - accuracy: 0.7458 - val_loss: 0.8816 - val_accuracy: 0.7315\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8338 - accuracy: 0.7458 - val_loss: 0.8755 - val_accuracy: 0.7323\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8721 - accuracy: 0.7320 - val_loss: 0.8733 - val_accuracy: 0.7350\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8494 - accuracy: 0.7427 - val_loss: 0.8622 - val_accuracy: 0.7388\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8499 - accuracy: 0.7405 - val_loss: 0.8292 - val_accuracy: 0.7482\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8508 - accuracy: 0.7411 - val_loss: 0.8267 - val_accuracy: 0.7505\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8357 - accuracy: 0.7442 - val_loss: 0.8491 - val_accuracy: 0.7392\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8213 - accuracy: 0.7480 - val_loss: 0.9075 - val_accuracy: 0.7255\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8455 - accuracy: 0.7426 - val_loss: 0.8268 - val_accuracy: 0.7502\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8114 - accuracy: 0.7555 - val_loss: 0.8280 - val_accuracy: 0.7507\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8346 - accuracy: 0.7447 - val_loss: 0.8371 - val_accuracy: 0.7469\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8444 - accuracy: 0.7399 - val_loss: 0.8669 - val_accuracy: 0.7371\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8262 - accuracy: 0.7465 - val_loss: 0.8703 - val_accuracy: 0.7381\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8441 - accuracy: 0.7432 - val_loss: 0.8040 - val_accuracy: 0.7595\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8159 - accuracy: 0.7496 - val_loss: 0.8256 - val_accuracy: 0.7501\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8393 - accuracy: 0.7416 - val_loss: 0.8607 - val_accuracy: 0.7366\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8342 - accuracy: 0.7426 - val_loss: 0.8708 - val_accuracy: 0.7361\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8490 - accuracy: 0.7398 - val_loss: 0.8688 - val_accuracy: 0.7345\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8344 - accuracy: 0.7428 - val_loss: 0.8323 - val_accuracy: 0.7497\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8368 - accuracy: 0.7402 - val_loss: 0.8406 - val_accuracy: 0.7448\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8364 - accuracy: 0.7468 - val_loss: 0.8484 - val_accuracy: 0.7433\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8328 - accuracy: 0.7475 - val_loss: 0.8891 - val_accuracy: 0.7273\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8193 - accuracy: 0.7496 - val_loss: 0.8500 - val_accuracy: 0.7425\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8279 - accuracy: 0.7461 - val_loss: 0.8073 - val_accuracy: 0.7568\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8143 - accuracy: 0.7531 - val_loss: 0.8316 - val_accuracy: 0.7489\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8094 - accuracy: 0.7533 - val_loss: 0.8238 - val_accuracy: 0.7501\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8279 - accuracy: 0.7424 - val_loss: 0.8489 - val_accuracy: 0.7422\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8464 - accuracy: 0.7402 - val_loss: 0.8268 - val_accuracy: 0.7470\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8560 - accuracy: 0.7388 - val_loss: 0.8150 - val_accuracy: 0.7522\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8125 - accuracy: 0.7521 - val_loss: 0.8104 - val_accuracy: 0.7570\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8168 - accuracy: 0.7508 - val_loss: 0.8203 - val_accuracy: 0.7560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db_8YkntKh1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0656fd42-0c9c-405d-ccbe-d777ba71b2d1"
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\r\n",
        "results2 = model2.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8203 - accuracy: 0.7560\n",
            "Validation accuracy: 75.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0TndD8vKrBh"
      },
      "source": [
        "**NN model, relu activations, adam optimizer, changing learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWgr4oqtKlL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f8fb52c-e8f2-407b-c038-5c380c942fff"
      },
      "source": [
        "%time\r\n",
        "print('NN model with relu activations and adam optimizer'); print('--'*40)\r\n",
        "# compiling the neural network classifier, adam optimizer\r\n",
        "adam = optimizers.Adam(lr = 0.001)\r\n",
        "model2.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 s, sys: 2 s, total: 4 s\n",
            "Wall time: 7.87 s\n",
            "NN model with relu activations and adam optimizer\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 0.7230 - accuracy: 0.7810 - val_loss: 0.7481 - val_accuracy: 0.7775\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7087 - accuracy: 0.7837 - val_loss: 0.7564 - val_accuracy: 0.7740\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7142 - accuracy: 0.7838 - val_loss: 0.7378 - val_accuracy: 0.7789\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7098 - accuracy: 0.7845 - val_loss: 0.7536 - val_accuracy: 0.7745\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7060 - accuracy: 0.7857 - val_loss: 0.7439 - val_accuracy: 0.7779\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7103 - accuracy: 0.7861 - val_loss: 0.7323 - val_accuracy: 0.7824\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7056 - accuracy: 0.7884 - val_loss: 0.7350 - val_accuracy: 0.7793\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7053 - accuracy: 0.7857 - val_loss: 0.7375 - val_accuracy: 0.7796\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6997 - accuracy: 0.7887 - val_loss: 0.7267 - val_accuracy: 0.7840\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6958 - accuracy: 0.7912 - val_loss: 0.7303 - val_accuracy: 0.7828\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6924 - accuracy: 0.7910 - val_loss: 0.7319 - val_accuracy: 0.7810\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7028 - accuracy: 0.7874 - val_loss: 0.7285 - val_accuracy: 0.7832\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7010 - accuracy: 0.7874 - val_loss: 0.7306 - val_accuracy: 0.7810\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6975 - accuracy: 0.7898 - val_loss: 0.7306 - val_accuracy: 0.7821\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6950 - accuracy: 0.7898 - val_loss: 0.7269 - val_accuracy: 0.7828\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6924 - accuracy: 0.7887 - val_loss: 0.7322 - val_accuracy: 0.7813\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6993 - accuracy: 0.7866 - val_loss: 0.7232 - val_accuracy: 0.7849\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7013 - accuracy: 0.7888 - val_loss: 0.7288 - val_accuracy: 0.7821\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7086 - accuracy: 0.7877 - val_loss: 0.7250 - val_accuracy: 0.7835\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7017 - accuracy: 0.7895 - val_loss: 0.7258 - val_accuracy: 0.7822\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6994 - accuracy: 0.7890 - val_loss: 0.7211 - val_accuracy: 0.7846\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7072 - accuracy: 0.7868 - val_loss: 0.7254 - val_accuracy: 0.7838\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6956 - accuracy: 0.7862 - val_loss: 0.7240 - val_accuracy: 0.7845\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7028 - accuracy: 0.7867 - val_loss: 0.7289 - val_accuracy: 0.7817\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6955 - accuracy: 0.7903 - val_loss: 0.7226 - val_accuracy: 0.7837\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6901 - accuracy: 0.7906 - val_loss: 0.7386 - val_accuracy: 0.7789\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6958 - accuracy: 0.7908 - val_loss: 0.7208 - val_accuracy: 0.7852\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6923 - accuracy: 0.7903 - val_loss: 0.7217 - val_accuracy: 0.7845\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6926 - accuracy: 0.7911 - val_loss: 0.7206 - val_accuracy: 0.7853\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6921 - accuracy: 0.7927 - val_loss: 0.7224 - val_accuracy: 0.7842\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6911 - accuracy: 0.7918 - val_loss: 0.7193 - val_accuracy: 0.7858\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6972 - accuracy: 0.7888 - val_loss: 0.7208 - val_accuracy: 0.7846\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6800 - accuracy: 0.7951 - val_loss: 0.7248 - val_accuracy: 0.7847\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6834 - accuracy: 0.7930 - val_loss: 0.7235 - val_accuracy: 0.7831\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6872 - accuracy: 0.7925 - val_loss: 0.7163 - val_accuracy: 0.7869\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6832 - accuracy: 0.7933 - val_loss: 0.7277 - val_accuracy: 0.7834\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6933 - accuracy: 0.7916 - val_loss: 0.7261 - val_accuracy: 0.7825\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6845 - accuracy: 0.7922 - val_loss: 0.7212 - val_accuracy: 0.7853\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6863 - accuracy: 0.7934 - val_loss: 0.7203 - val_accuracy: 0.7856\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6919 - accuracy: 0.7925 - val_loss: 0.7197 - val_accuracy: 0.7851\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6874 - accuracy: 0.7933 - val_loss: 0.7192 - val_accuracy: 0.7863\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6861 - accuracy: 0.7939 - val_loss: 0.7184 - val_accuracy: 0.7849\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6797 - accuracy: 0.7957 - val_loss: 0.7222 - val_accuracy: 0.7848\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6808 - accuracy: 0.7946 - val_loss: 0.7174 - val_accuracy: 0.7864\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6885 - accuracy: 0.7939 - val_loss: 0.7170 - val_accuracy: 0.7852\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6919 - accuracy: 0.7900 - val_loss: 0.7152 - val_accuracy: 0.7865\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6868 - accuracy: 0.7911 - val_loss: 0.7193 - val_accuracy: 0.7861\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6877 - accuracy: 0.7912 - val_loss: 0.7134 - val_accuracy: 0.7870\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6802 - accuracy: 0.7929 - val_loss: 0.7147 - val_accuracy: 0.7869\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6885 - accuracy: 0.7932 - val_loss: 0.7155 - val_accuracy: 0.7864\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6854 - accuracy: 0.7943 - val_loss: 0.7251 - val_accuracy: 0.7840\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6869 - accuracy: 0.7920 - val_loss: 0.7191 - val_accuracy: 0.7844\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6868 - accuracy: 0.7916 - val_loss: 0.7248 - val_accuracy: 0.7828\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6880 - accuracy: 0.7922 - val_loss: 0.7134 - val_accuracy: 0.7861\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6852 - accuracy: 0.7923 - val_loss: 0.7147 - val_accuracy: 0.7879\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6877 - accuracy: 0.7962 - val_loss: 0.7193 - val_accuracy: 0.7852\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6836 - accuracy: 0.7926 - val_loss: 0.7153 - val_accuracy: 0.7862\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6802 - accuracy: 0.7931 - val_loss: 0.7136 - val_accuracy: 0.7859\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6879 - accuracy: 0.7929 - val_loss: 0.7221 - val_accuracy: 0.7837\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6897 - accuracy: 0.7914 - val_loss: 0.7139 - val_accuracy: 0.7866\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6936 - accuracy: 0.7923 - val_loss: 0.7115 - val_accuracy: 0.7884\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6782 - accuracy: 0.7970 - val_loss: 0.7210 - val_accuracy: 0.7851\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6785 - accuracy: 0.7957 - val_loss: 0.7250 - val_accuracy: 0.7838\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6850 - accuracy: 0.7933 - val_loss: 0.7169 - val_accuracy: 0.7864\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6759 - accuracy: 0.7959 - val_loss: 0.7180 - val_accuracy: 0.7860\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6777 - accuracy: 0.7941 - val_loss: 0.7120 - val_accuracy: 0.7877\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6753 - accuracy: 0.7960 - val_loss: 0.7110 - val_accuracy: 0.7887\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6798 - accuracy: 0.7945 - val_loss: 0.7073 - val_accuracy: 0.7890\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6854 - accuracy: 0.7925 - val_loss: 0.7114 - val_accuracy: 0.7883\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6767 - accuracy: 0.7959 - val_loss: 0.7095 - val_accuracy: 0.7887\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6797 - accuracy: 0.7936 - val_loss: 0.7109 - val_accuracy: 0.7878\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6805 - accuracy: 0.7949 - val_loss: 0.7163 - val_accuracy: 0.7861\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6895 - accuracy: 0.7935 - val_loss: 0.7143 - val_accuracy: 0.7860\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6773 - accuracy: 0.7967 - val_loss: 0.7175 - val_accuracy: 0.7856\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6837 - accuracy: 0.7935 - val_loss: 0.7112 - val_accuracy: 0.7888\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6718 - accuracy: 0.7959 - val_loss: 0.7185 - val_accuracy: 0.7855\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6752 - accuracy: 0.7934 - val_loss: 0.7137 - val_accuracy: 0.7875\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6845 - accuracy: 0.7919 - val_loss: 0.7170 - val_accuracy: 0.7860\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6713 - accuracy: 0.7987 - val_loss: 0.7114 - val_accuracy: 0.7889\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6752 - accuracy: 0.7976 - val_loss: 0.7250 - val_accuracy: 0.7839\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6854 - accuracy: 0.7949 - val_loss: 0.7150 - val_accuracy: 0.7865\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6819 - accuracy: 0.7949 - val_loss: 0.7194 - val_accuracy: 0.7848\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6828 - accuracy: 0.7927 - val_loss: 0.7210 - val_accuracy: 0.7842\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6685 - accuracy: 0.7969 - val_loss: 0.7138 - val_accuracy: 0.7875\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6890 - accuracy: 0.7923 - val_loss: 0.7061 - val_accuracy: 0.7906\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6863 - accuracy: 0.7937 - val_loss: 0.7128 - val_accuracy: 0.7893\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6797 - accuracy: 0.7956 - val_loss: 0.7075 - val_accuracy: 0.7895\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6803 - accuracy: 0.7931 - val_loss: 0.7106 - val_accuracy: 0.7897\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6785 - accuracy: 0.7945 - val_loss: 0.7068 - val_accuracy: 0.7895\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6811 - accuracy: 0.7929 - val_loss: 0.7148 - val_accuracy: 0.7859\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6736 - accuracy: 0.7975 - val_loss: 0.7121 - val_accuracy: 0.7875\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6739 - accuracy: 0.7954 - val_loss: 0.7146 - val_accuracy: 0.7857\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6696 - accuracy: 0.7960 - val_loss: 0.7109 - val_accuracy: 0.7880\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6657 - accuracy: 0.7994 - val_loss: 0.7093 - val_accuracy: 0.7885\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6754 - accuracy: 0.7966 - val_loss: 0.7034 - val_accuracy: 0.7914\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6749 - accuracy: 0.7979 - val_loss: 0.7099 - val_accuracy: 0.7886\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6818 - accuracy: 0.7934 - val_loss: 0.7060 - val_accuracy: 0.7908\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6721 - accuracy: 0.7951 - val_loss: 0.7082 - val_accuracy: 0.7892\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6765 - accuracy: 0.7964 - val_loss: 0.7072 - val_accuracy: 0.7895\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6635 - accuracy: 0.7997 - val_loss: 0.7048 - val_accuracy: 0.7902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmUXwnqKz8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afed3bc-886f-42df-d47a-5f81d8ba2a80"
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\r\n",
        "results2 = model2.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7048 - accuracy: 0.7902\n",
            "Validation accuracy: 79.02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD0dLWFCK2hN"
      },
      "source": [
        "**Observation 4 - NN model with relu activations**\r\n",
        "\r\n",
        "*   Best accuracy achieved till now is using relu activations, SGD optimizer, changing learning rate to 0.001.\r\n",
        "*   Improves the scores considerably.\r\n",
        "*  Next, let's try and change the number of activators and see if the score improves.\r\n",
        "\r\n",
        "\r\n",
        "**NN model, relu activations, changing number of activators, SGD optimizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXxvJf35LLGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0c346e-d53f-4058-ffb4-8679d272e0ad"
      },
      "source": [
        "print('NN model with relu activations and changing number of activators'); print('--'*40)\r\n",
        "# Initialize the neural network classifier\r\n",
        "model3 = Sequential()\r\n",
        "\r\n",
        "# Input Layer - adding input layer and activation functions relu\r\n",
        "model3.add(Dense(256, input_shape = (1024, )))\r\n",
        "# Adding activation function\r\n",
        "model3.add(Activation('relu'))\r\n",
        "\r\n",
        "#Hidden Layer 1 - adding first hidden layer\r\n",
        "model3.add(Dense(128))\r\n",
        "# Adding activation function\r\n",
        "model3.add(Activation('relu'))\r\n",
        "\r\n",
        "#Hidden Layer 2 - Adding second hidden layer\r\n",
        "model3.add(Dense(64))\r\n",
        "# Adding activation function\r\n",
        "model3.add(Activation('relu'))\r\n",
        "\r\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\r\n",
        "model3.add(Dense(10))\r\n",
        "# Adding activation function - softmax for multiclass classification\r\n",
        "model3.add(Activation('softmax'))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with relu activations and changing number of activators\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQzEx2mNLODJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e33193-43d5-47e7-b8b5-2d465d6f799c"
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 304,202\n",
            "Trainable params: 304,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptutt4ltLRaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1bbe187-c509-46ce-b42d-58dd601278e2"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\r\n",
        "sgd = optimizers.SGD(lr = 0.01)\r\n",
        "model3.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 2.3083 - accuracy: 0.0986 - val_loss: 2.2938 - val_accuracy: 0.1208\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2910 - accuracy: 0.1301 - val_loss: 2.2824 - val_accuracy: 0.1472\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2795 - accuracy: 0.1609 - val_loss: 2.2685 - val_accuracy: 0.1941\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2653 - accuracy: 0.1973 - val_loss: 2.2517 - val_accuracy: 0.2176\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2461 - accuracy: 0.2218 - val_loss: 2.2280 - val_accuracy: 0.2630\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2211 - accuracy: 0.2657 - val_loss: 2.1943 - val_accuracy: 0.2930\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.1846 - accuracy: 0.3036 - val_loss: 2.1501 - val_accuracy: 0.2870\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.1329 - accuracy: 0.3321 - val_loss: 2.0807 - val_accuracy: 0.3627\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.0600 - accuracy: 0.3571 - val_loss: 1.9968 - val_accuracy: 0.3783\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.9739 - accuracy: 0.3886 - val_loss: 1.9000 - val_accuracy: 0.4103\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.8770 - accuracy: 0.4197 - val_loss: 1.8092 - val_accuracy: 0.4490\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.7822 - accuracy: 0.4440 - val_loss: 1.7237 - val_accuracy: 0.4676\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.7061 - accuracy: 0.4631 - val_loss: 1.6382 - val_accuracy: 0.4924\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.6274 - accuracy: 0.4980 - val_loss: 1.5702 - val_accuracy: 0.5189\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.5584 - accuracy: 0.5182 - val_loss: 1.5369 - val_accuracy: 0.5244\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.5025 - accuracy: 0.5387 - val_loss: 1.4623 - val_accuracy: 0.5494\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.4500 - accuracy: 0.5518 - val_loss: 1.3941 - val_accuracy: 0.5704\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.4146 - accuracy: 0.5651 - val_loss: 1.3974 - val_accuracy: 0.5631\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.3674 - accuracy: 0.5777 - val_loss: 1.3251 - val_accuracy: 0.5930\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.3329 - accuracy: 0.5865 - val_loss: 1.2835 - val_accuracy: 0.6098\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.2840 - accuracy: 0.6045 - val_loss: 1.2384 - val_accuracy: 0.6271\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.2611 - accuracy: 0.6108 - val_loss: 1.2375 - val_accuracy: 0.6186\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.2263 - accuracy: 0.6228 - val_loss: 1.1963 - val_accuracy: 0.6405\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1987 - accuracy: 0.6349 - val_loss: 1.1679 - val_accuracy: 0.6409\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.1724 - accuracy: 0.6373 - val_loss: 1.1376 - val_accuracy: 0.6550\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.1446 - accuracy: 0.6503 - val_loss: 1.1565 - val_accuracy: 0.6335\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.1271 - accuracy: 0.6512 - val_loss: 1.1448 - val_accuracy: 0.6415\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1050 - accuracy: 0.6606 - val_loss: 1.0915 - val_accuracy: 0.6682\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0892 - accuracy: 0.6682 - val_loss: 1.0701 - val_accuracy: 0.6741\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0640 - accuracy: 0.6734 - val_loss: 1.0473 - val_accuracy: 0.6825\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0676 - accuracy: 0.6728 - val_loss: 1.0303 - val_accuracy: 0.6902\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0459 - accuracy: 0.6775 - val_loss: 1.0579 - val_accuracy: 0.6690\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0118 - accuracy: 0.6909 - val_loss: 1.0393 - val_accuracy: 0.6783\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0030 - accuracy: 0.6936 - val_loss: 0.9973 - val_accuracy: 0.6973\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9911 - accuracy: 0.6961 - val_loss: 0.9847 - val_accuracy: 0.6984\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9740 - accuracy: 0.7045 - val_loss: 0.9592 - val_accuracy: 0.7109\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9727 - accuracy: 0.7024 - val_loss: 0.9661 - val_accuracy: 0.7054\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.9637 - accuracy: 0.7060 - val_loss: 0.9527 - val_accuracy: 0.7125\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.9480 - accuracy: 0.7089 - val_loss: 0.9621 - val_accuracy: 0.7043\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.9406 - accuracy: 0.7153 - val_loss: 0.9186 - val_accuracy: 0.7219\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9183 - accuracy: 0.7199 - val_loss: 0.9437 - val_accuracy: 0.7127\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9133 - accuracy: 0.7231 - val_loss: 0.8960 - val_accuracy: 0.7306\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9057 - accuracy: 0.7224 - val_loss: 0.9050 - val_accuracy: 0.7270\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8888 - accuracy: 0.7308 - val_loss: 0.8962 - val_accuracy: 0.7266\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8794 - accuracy: 0.7318 - val_loss: 0.8734 - val_accuracy: 0.7392\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8857 - accuracy: 0.7295 - val_loss: 0.8563 - val_accuracy: 0.7420\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8662 - accuracy: 0.7365 - val_loss: 0.8864 - val_accuracy: 0.7305\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8560 - accuracy: 0.7411 - val_loss: 0.8586 - val_accuracy: 0.7421\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8463 - accuracy: 0.7450 - val_loss: 0.8670 - val_accuracy: 0.7347\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8466 - accuracy: 0.7415 - val_loss: 0.8530 - val_accuracy: 0.7404\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8433 - accuracy: 0.7410 - val_loss: 0.8298 - val_accuracy: 0.7467\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8200 - accuracy: 0.7515 - val_loss: 0.8346 - val_accuracy: 0.7472\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8161 - accuracy: 0.7524 - val_loss: 0.8113 - val_accuracy: 0.7567\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8085 - accuracy: 0.7551 - val_loss: 0.8242 - val_accuracy: 0.7503\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8037 - accuracy: 0.7566 - val_loss: 0.8455 - val_accuracy: 0.7441\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7932 - accuracy: 0.7571 - val_loss: 0.8017 - val_accuracy: 0.7591\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7853 - accuracy: 0.7622 - val_loss: 0.7856 - val_accuracy: 0.7651\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7786 - accuracy: 0.7612 - val_loss: 0.7961 - val_accuracy: 0.7593\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7754 - accuracy: 0.7671 - val_loss: 0.7748 - val_accuracy: 0.7680\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7770 - accuracy: 0.7634 - val_loss: 0.8239 - val_accuracy: 0.7492\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7739 - accuracy: 0.7648 - val_loss: 0.7602 - val_accuracy: 0.7709\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7485 - accuracy: 0.7750 - val_loss: 0.7455 - val_accuracy: 0.7777\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7448 - accuracy: 0.7738 - val_loss: 0.7963 - val_accuracy: 0.7580\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7415 - accuracy: 0.7754 - val_loss: 0.7614 - val_accuracy: 0.7685\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7313 - accuracy: 0.7795 - val_loss: 0.7732 - val_accuracy: 0.7645\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7170 - accuracy: 0.7823 - val_loss: 0.8316 - val_accuracy: 0.7481\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7243 - accuracy: 0.7842 - val_loss: 0.7151 - val_accuracy: 0.7864\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7047 - accuracy: 0.7888 - val_loss: 0.7228 - val_accuracy: 0.7827\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7060 - accuracy: 0.7846 - val_loss: 0.7021 - val_accuracy: 0.7894\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7026 - accuracy: 0.7851 - val_loss: 0.7281 - val_accuracy: 0.7812\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7007 - accuracy: 0.7880 - val_loss: 0.7015 - val_accuracy: 0.7916\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6976 - accuracy: 0.7890 - val_loss: 0.6917 - val_accuracy: 0.7939\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6896 - accuracy: 0.7902 - val_loss: 0.6935 - val_accuracy: 0.7918\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6802 - accuracy: 0.7946 - val_loss: 0.7015 - val_accuracy: 0.7902\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6849 - accuracy: 0.7952 - val_loss: 0.6994 - val_accuracy: 0.7886\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6595 - accuracy: 0.7972 - val_loss: 0.7010 - val_accuracy: 0.7870\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6676 - accuracy: 0.7991 - val_loss: 0.6755 - val_accuracy: 0.7985\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6639 - accuracy: 0.7993 - val_loss: 0.6614 - val_accuracy: 0.8032\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.8046 - val_loss: 0.7078 - val_accuracy: 0.7848\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6483 - accuracy: 0.8026 - val_loss: 0.6492 - val_accuracy: 0.8065\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6494 - accuracy: 0.8024 - val_loss: 0.6575 - val_accuracy: 0.8049\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6460 - accuracy: 0.8041 - val_loss: 0.6478 - val_accuracy: 0.8078\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6427 - accuracy: 0.8055 - val_loss: 0.6330 - val_accuracy: 0.8123\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6192 - accuracy: 0.8150 - val_loss: 0.6569 - val_accuracy: 0.8029\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6334 - accuracy: 0.8092 - val_loss: 0.6947 - val_accuracy: 0.7925\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6262 - accuracy: 0.8115 - val_loss: 0.6455 - val_accuracy: 0.8072\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6276 - accuracy: 0.8133 - val_loss: 0.6213 - val_accuracy: 0.8165\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6153 - accuracy: 0.8150 - val_loss: 0.6597 - val_accuracy: 0.8019\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6148 - accuracy: 0.8147 - val_loss: 0.6492 - val_accuracy: 0.8044\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6192 - accuracy: 0.8119 - val_loss: 0.6107 - val_accuracy: 0.8193\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6108 - accuracy: 0.8149 - val_loss: 0.6854 - val_accuracy: 0.7957\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5983 - accuracy: 0.8183 - val_loss: 0.6126 - val_accuracy: 0.8178\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5972 - accuracy: 0.8196 - val_loss: 0.6054 - val_accuracy: 0.8199\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5800 - accuracy: 0.8240 - val_loss: 0.6130 - val_accuracy: 0.8181\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5845 - accuracy: 0.8238 - val_loss: 0.6080 - val_accuracy: 0.8199\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5907 - accuracy: 0.8215 - val_loss: 0.6049 - val_accuracy: 0.8204\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5910 - accuracy: 0.8221 - val_loss: 0.6278 - val_accuracy: 0.8122\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5842 - accuracy: 0.8253 - val_loss: 0.6042 - val_accuracy: 0.8191\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5773 - accuracy: 0.8248 - val_loss: 0.7081 - val_accuracy: 0.7878\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5691 - accuracy: 0.8281 - val_loss: 0.6066 - val_accuracy: 0.8211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmj9qK7NLR9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47bb2ea4-4525-4aa5-9283-10575324504d"
      },
      "source": [
        "print('Evaluate NN model with relu activations and changing the number of activators'); print('--'*40)\r\n",
        "results3 = model3.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations and changing the number of activators\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6066 - accuracy: 0.8211\n",
            "Validation accuracy: 82.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZwjV9i7LXRV"
      },
      "source": [
        "**NN model, relu activations, changing number of activators, Adam optimizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wWEkJ6_LawE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dbd183f-21ba-48c2-cab1-32e2a652c9a6"
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\r\n",
        "adam = optimizers.Adam(lr = 0.001)\r\n",
        "model3.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 1.2363 - accuracy: 0.6563 - val_loss: 0.8057 - val_accuracy: 0.7567\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7625 - accuracy: 0.7646 - val_loss: 0.7506 - val_accuracy: 0.7735\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7313 - accuracy: 0.7765 - val_loss: 0.6977 - val_accuracy: 0.7866\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7233 - accuracy: 0.7783 - val_loss: 0.6797 - val_accuracy: 0.7966\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7002 - accuracy: 0.7854 - val_loss: 0.7305 - val_accuracy: 0.7774\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6964 - accuracy: 0.7848 - val_loss: 0.6891 - val_accuracy: 0.7909\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6949 - accuracy: 0.7903 - val_loss: 0.6886 - val_accuracy: 0.7910\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6619 - accuracy: 0.7938 - val_loss: 0.6944 - val_accuracy: 0.7864\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6239 - accuracy: 0.8048 - val_loss: 0.7157 - val_accuracy: 0.7823\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6388 - accuracy: 0.8079 - val_loss: 0.6480 - val_accuracy: 0.8037\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5992 - accuracy: 0.8147 - val_loss: 0.6180 - val_accuracy: 0.8114\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6002 - accuracy: 0.8183 - val_loss: 0.6387 - val_accuracy: 0.8031\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5903 - accuracy: 0.8195 - val_loss: 0.6548 - val_accuracy: 0.8012\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6051 - accuracy: 0.8118 - val_loss: 0.5917 - val_accuracy: 0.8182\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5592 - accuracy: 0.8286 - val_loss: 0.5966 - val_accuracy: 0.8167\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5628 - accuracy: 0.8251 - val_loss: 0.6293 - val_accuracy: 0.8069\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5672 - accuracy: 0.8256 - val_loss: 0.5924 - val_accuracy: 0.8231\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5528 - accuracy: 0.8292 - val_loss: 0.5487 - val_accuracy: 0.8365\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5351 - accuracy: 0.8335 - val_loss: 0.5952 - val_accuracy: 0.8185\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5318 - accuracy: 0.8351 - val_loss: 0.5767 - val_accuracy: 0.8255\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5101 - accuracy: 0.8415 - val_loss: 0.5406 - val_accuracy: 0.8358\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5266 - accuracy: 0.8348 - val_loss: 0.6098 - val_accuracy: 0.8148\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5123 - accuracy: 0.8396 - val_loss: 0.5157 - val_accuracy: 0.8456\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4971 - accuracy: 0.8471 - val_loss: 0.5337 - val_accuracy: 0.8408\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4964 - accuracy: 0.8478 - val_loss: 0.5694 - val_accuracy: 0.8284\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4949 - accuracy: 0.8482 - val_loss: 0.5881 - val_accuracy: 0.8183\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4781 - accuracy: 0.8495 - val_loss: 0.5538 - val_accuracy: 0.8319\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4774 - accuracy: 0.8514 - val_loss: 0.5398 - val_accuracy: 0.8356\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4683 - accuracy: 0.8535 - val_loss: 0.5042 - val_accuracy: 0.8481\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4598 - accuracy: 0.8583 - val_loss: 0.4968 - val_accuracy: 0.8489\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4482 - accuracy: 0.8583 - val_loss: 0.4989 - val_accuracy: 0.8497\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4357 - accuracy: 0.8624 - val_loss: 0.5313 - val_accuracy: 0.8400\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4467 - accuracy: 0.8596 - val_loss: 0.4972 - val_accuracy: 0.8497\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4454 - accuracy: 0.8614 - val_loss: 0.5179 - val_accuracy: 0.8436\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4184 - accuracy: 0.8695 - val_loss: 0.4590 - val_accuracy: 0.8630\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4114 - accuracy: 0.8717 - val_loss: 0.4671 - val_accuracy: 0.8597\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4281 - accuracy: 0.8656 - val_loss: 0.5461 - val_accuracy: 0.8355\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4458 - accuracy: 0.8608 - val_loss: 0.4537 - val_accuracy: 0.8646\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4089 - accuracy: 0.8732 - val_loss: 0.4450 - val_accuracy: 0.8691\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4183 - accuracy: 0.8659 - val_loss: 0.4647 - val_accuracy: 0.8595\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4110 - accuracy: 0.8699 - val_loss: 0.4654 - val_accuracy: 0.8599\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3931 - accuracy: 0.8782 - val_loss: 0.4452 - val_accuracy: 0.8671\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3858 - accuracy: 0.8792 - val_loss: 0.4616 - val_accuracy: 0.8627\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3864 - accuracy: 0.8767 - val_loss: 0.4711 - val_accuracy: 0.8579\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3700 - accuracy: 0.8841 - val_loss: 0.4276 - val_accuracy: 0.8730\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3957 - accuracy: 0.8763 - val_loss: 0.4319 - val_accuracy: 0.8708\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3676 - accuracy: 0.8831 - val_loss: 0.4458 - val_accuracy: 0.8671\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3672 - accuracy: 0.8826 - val_loss: 0.4519 - val_accuracy: 0.8647\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3617 - accuracy: 0.8838 - val_loss: 0.4262 - val_accuracy: 0.8746\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3736 - accuracy: 0.8810 - val_loss: 0.4367 - val_accuracy: 0.8694\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3683 - accuracy: 0.8824 - val_loss: 0.4417 - val_accuracy: 0.8689\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3404 - accuracy: 0.8925 - val_loss: 0.4606 - val_accuracy: 0.8654\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3395 - accuracy: 0.8911 - val_loss: 0.4339 - val_accuracy: 0.8712\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3467 - accuracy: 0.8886 - val_loss: 0.4291 - val_accuracy: 0.8728\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3406 - accuracy: 0.8912 - val_loss: 0.4251 - val_accuracy: 0.8755\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3461 - accuracy: 0.8887 - val_loss: 0.4387 - val_accuracy: 0.8693\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.8940 - val_loss: 0.4083 - val_accuracy: 0.8786\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8933 - val_loss: 0.4490 - val_accuracy: 0.8653\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8930 - val_loss: 0.4110 - val_accuracy: 0.8792\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3303 - accuracy: 0.8940 - val_loss: 0.4169 - val_accuracy: 0.8755\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3106 - accuracy: 0.9012 - val_loss: 0.4143 - val_accuracy: 0.8785\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3192 - accuracy: 0.8977 - val_loss: 0.4256 - val_accuracy: 0.8761\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3281 - accuracy: 0.8976 - val_loss: 0.4186 - val_accuracy: 0.8752\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3258 - accuracy: 0.8947 - val_loss: 0.4148 - val_accuracy: 0.8783\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2975 - accuracy: 0.9068 - val_loss: 0.3954 - val_accuracy: 0.8844\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2947 - accuracy: 0.9067 - val_loss: 0.3874 - val_accuracy: 0.8890\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3104 - accuracy: 0.8995 - val_loss: 0.4014 - val_accuracy: 0.8836\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2958 - accuracy: 0.9042 - val_loss: 0.3935 - val_accuracy: 0.8856\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3017 - accuracy: 0.9028 - val_loss: 0.4059 - val_accuracy: 0.8820\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3014 - accuracy: 0.9002 - val_loss: 0.4097 - val_accuracy: 0.8819\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2937 - accuracy: 0.9045 - val_loss: 0.3796 - val_accuracy: 0.8928\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3084 - accuracy: 0.8995 - val_loss: 0.4286 - val_accuracy: 0.8756\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3129 - accuracy: 0.8979 - val_loss: 0.4093 - val_accuracy: 0.8826\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2856 - accuracy: 0.9086 - val_loss: 0.4190 - val_accuracy: 0.8778\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2907 - accuracy: 0.9059 - val_loss: 0.3983 - val_accuracy: 0.8865\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2882 - accuracy: 0.9036 - val_loss: 0.3616 - val_accuracy: 0.8979\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2738 - accuracy: 0.9098 - val_loss: 0.4103 - val_accuracy: 0.8821\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2758 - accuracy: 0.9109 - val_loss: 0.3948 - val_accuracy: 0.8861\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2677 - accuracy: 0.9118 - val_loss: 0.3735 - val_accuracy: 0.8960\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2780 - accuracy: 0.9087 - val_loss: 0.4189 - val_accuracy: 0.8784\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2674 - accuracy: 0.9136 - val_loss: 0.4094 - val_accuracy: 0.8859\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2698 - accuracy: 0.9101 - val_loss: 0.4429 - val_accuracy: 0.8753\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2623 - accuracy: 0.9141 - val_loss: 0.3822 - val_accuracy: 0.8945\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2810 - accuracy: 0.9077 - val_loss: 0.3826 - val_accuracy: 0.8914\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2609 - accuracy: 0.9146 - val_loss: 0.3810 - val_accuracy: 0.8917\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2600 - accuracy: 0.9135 - val_loss: 0.3942 - val_accuracy: 0.8928\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2462 - accuracy: 0.9212 - val_loss: 0.4062 - val_accuracy: 0.8861\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2675 - accuracy: 0.9111 - val_loss: 0.4371 - val_accuracy: 0.8761\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.9123 - val_loss: 0.3842 - val_accuracy: 0.8953\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2489 - accuracy: 0.9180 - val_loss: 0.4055 - val_accuracy: 0.8878\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2612 - accuracy: 0.9125 - val_loss: 0.4018 - val_accuracy: 0.8894\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2340 - accuracy: 0.9232 - val_loss: 0.3782 - val_accuracy: 0.8990\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2439 - accuracy: 0.9205 - val_loss: 0.4074 - val_accuracy: 0.8877\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2367 - accuracy: 0.9210 - val_loss: 0.3657 - val_accuracy: 0.9018\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2425 - accuracy: 0.9200 - val_loss: 0.4045 - val_accuracy: 0.8874\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2502 - accuracy: 0.9166 - val_loss: 0.3624 - val_accuracy: 0.9010\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2418 - accuracy: 0.9185 - val_loss: 0.3560 - val_accuracy: 0.9057\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2265 - accuracy: 0.9252 - val_loss: 0.3624 - val_accuracy: 0.9056\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2362 - accuracy: 0.9210 - val_loss: 0.4271 - val_accuracy: 0.8837\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2262 - accuracy: 0.9245 - val_loss: 0.3773 - val_accuracy: 0.8978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiFyCFVTLiwp"
      },
      "source": [
        "**Observation 5 - NN model with relu activations and changing activators**\r\n",
        "\r\n",
        "Adding relu activations and changing activators results in improvement of score.\r\n",
        "Best accuracy achieved till now is using relu activations, changing number of activators and Adam optimizers with a learning rate of 0.001\r\n",
        "Next, let's try adding weight initilization.\r\n",
        "\r\n",
        "**With Weight Initializers**\r\n",
        "\r\n",
        "Changing weight initialization scheme can significantly improve training of the model by preventing vanishing gradient problem up to some degree.\r\n",
        "\r\n",
        "**NN model, relu activations, SGD optimizers with weight initializers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9y8XJgfLvcz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f99f55-0c64-4031-c621-c3836c88b7ee"
      },
      "source": [
        "print('NN model with weight initializers'); print('--'*40)\r\n",
        "# Initialize the neural network classifier\r\n",
        "model4 = Sequential()\r\n",
        "\r\n",
        "# Input Layer - adding input layer and activation functions relu and weight initializer\r\n",
        "model4.add(Dense(256, input_shape = (1024, ), kernel_initializer = 'he_normal'))\r\n",
        "# Adding activation function\r\n",
        "model4.add(Activation('relu'))\r\n",
        "\r\n",
        "#Hidden Layer 1 - adding first hidden layer\r\n",
        "model4.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding activation function\r\n",
        "model4.add(Activation('relu'))\r\n",
        "\r\n",
        "#Hidden Layer 2 - adding second hidden layer\r\n",
        "model4.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding activation function\r\n",
        "model4.add(Activation('relu'))\r\n",
        "\r\n",
        "#Hidden Layer 3 - adding third hidden layer\r\n",
        "model4.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding activation function\r\n",
        "model4.add(Activation('relu'))\r\n",
        "\r\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\r\n",
        "model4.add(Dense(10, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding activation function\r\n",
        "model4.add(Activation('softmax'))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with weight initializers\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqfnqhVbL6cF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf80d49-14c3-42dc-dff2-3735267c2345"
      },
      "source": [
        "model4.summary()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 305,962\n",
            "Trainable params: 305,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lngdZHvXL9cS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "316eb96c-1a82-4372-e9c4-bd73df7c39bc"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\r\n",
        "sgd = optimizers.SGD(lr = 0.01)\r\n",
        "# Adding activation function - softmax for multiclass classification\r\n",
        "model4.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model4.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 2.3359 - accuracy: 0.1080 - val_loss: 2.2874 - val_accuracy: 0.1453\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2800 - accuracy: 0.1571 - val_loss: 2.2537 - val_accuracy: 0.1787\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 2.2392 - accuracy: 0.1918 - val_loss: 2.1926 - val_accuracy: 0.2271\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.1764 - accuracy: 0.2411 - val_loss: 2.1094 - val_accuracy: 0.2932\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 2.0844 - accuracy: 0.2944 - val_loss: 1.9983 - val_accuracy: 0.3119\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.9681 - accuracy: 0.3413 - val_loss: 1.8691 - val_accuracy: 0.3757\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.8496 - accuracy: 0.3807 - val_loss: 1.7729 - val_accuracy: 0.3992\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.7575 - accuracy: 0.4127 - val_loss: 1.6542 - val_accuracy: 0.4619\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.6622 - accuracy: 0.4525 - val_loss: 1.5695 - val_accuracy: 0.4985\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.5859 - accuracy: 0.4848 - val_loss: 1.4929 - val_accuracy: 0.5205\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.5063 - accuracy: 0.5084 - val_loss: 1.4699 - val_accuracy: 0.5300\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.4409 - accuracy: 0.5366 - val_loss: 1.3662 - val_accuracy: 0.5777\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.4022 - accuracy: 0.5485 - val_loss: 1.3263 - val_accuracy: 0.5798\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.3153 - accuracy: 0.5854 - val_loss: 1.3352 - val_accuracy: 0.5795\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.2770 - accuracy: 0.5980 - val_loss: 1.2191 - val_accuracy: 0.6206\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.2295 - accuracy: 0.6148 - val_loss: 1.2907 - val_accuracy: 0.5801\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.2180 - accuracy: 0.6190 - val_loss: 1.1843 - val_accuracy: 0.6295\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1879 - accuracy: 0.6287 - val_loss: 1.2946 - val_accuracy: 0.5841\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.1688 - accuracy: 0.6341 - val_loss: 1.1133 - val_accuracy: 0.6576\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1331 - accuracy: 0.6473 - val_loss: 1.1645 - val_accuracy: 0.6281\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0969 - accuracy: 0.6614 - val_loss: 1.0489 - val_accuracy: 0.6784\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0934 - accuracy: 0.6612 - val_loss: 1.0383 - val_accuracy: 0.6825\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0702 - accuracy: 0.6673 - val_loss: 1.0229 - val_accuracy: 0.6925\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.0411 - accuracy: 0.6800 - val_loss: 1.0259 - val_accuracy: 0.6814\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0225 - accuracy: 0.6837 - val_loss: 0.9822 - val_accuracy: 0.7000\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9990 - accuracy: 0.6928 - val_loss: 1.0000 - val_accuracy: 0.6893\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9838 - accuracy: 0.7020 - val_loss: 0.9886 - val_accuracy: 0.6914\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9824 - accuracy: 0.6962 - val_loss: 0.9414 - val_accuracy: 0.7143\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.9572 - accuracy: 0.7054 - val_loss: 0.9249 - val_accuracy: 0.7198\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9417 - accuracy: 0.7124 - val_loss: 0.9184 - val_accuracy: 0.7233\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.9383 - accuracy: 0.7100 - val_loss: 0.9079 - val_accuracy: 0.7257\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9261 - accuracy: 0.7161 - val_loss: 0.9519 - val_accuracy: 0.7039\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8946 - accuracy: 0.7294 - val_loss: 0.9734 - val_accuracy: 0.6910\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8859 - accuracy: 0.7304 - val_loss: 0.8944 - val_accuracy: 0.7247\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8740 - accuracy: 0.7317 - val_loss: 0.8839 - val_accuracy: 0.7300\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8666 - accuracy: 0.7345 - val_loss: 0.8497 - val_accuracy: 0.7430\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8645 - accuracy: 0.7351 - val_loss: 0.8615 - val_accuracy: 0.7371\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8504 - accuracy: 0.7392 - val_loss: 0.8275 - val_accuracy: 0.7509\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8352 - accuracy: 0.7450 - val_loss: 0.8579 - val_accuracy: 0.7392\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8324 - accuracy: 0.7459 - val_loss: 0.8152 - val_accuracy: 0.7534\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8167 - accuracy: 0.7504 - val_loss: 0.8032 - val_accuracy: 0.7565\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8074 - accuracy: 0.7503 - val_loss: 0.7945 - val_accuracy: 0.7613\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7970 - accuracy: 0.7553 - val_loss: 0.7890 - val_accuracy: 0.7628\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7834 - accuracy: 0.7600 - val_loss: 0.8054 - val_accuracy: 0.7520\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7722 - accuracy: 0.7654 - val_loss: 0.7702 - val_accuracy: 0.7671\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7771 - accuracy: 0.7613 - val_loss: 0.7624 - val_accuracy: 0.7704\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7612 - accuracy: 0.7688 - val_loss: 0.9099 - val_accuracy: 0.7245\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7564 - accuracy: 0.7675 - val_loss: 0.7952 - val_accuracy: 0.7595\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7468 - accuracy: 0.7713 - val_loss: 0.7561 - val_accuracy: 0.7700\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7445 - accuracy: 0.7699 - val_loss: 0.7531 - val_accuracy: 0.7715\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7372 - accuracy: 0.7747 - val_loss: 0.7420 - val_accuracy: 0.7762\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7292 - accuracy: 0.7755 - val_loss: 0.7360 - val_accuracy: 0.7759\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7202 - accuracy: 0.7809 - val_loss: 0.7321 - val_accuracy: 0.7788\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7121 - accuracy: 0.7818 - val_loss: 0.7279 - val_accuracy: 0.7810\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7032 - accuracy: 0.7838 - val_loss: 0.7744 - val_accuracy: 0.7635\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7003 - accuracy: 0.7833 - val_loss: 0.7139 - val_accuracy: 0.7830\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6896 - accuracy: 0.7904 - val_loss: 0.7308 - val_accuracy: 0.7780\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6807 - accuracy: 0.7913 - val_loss: 0.7124 - val_accuracy: 0.7845\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6766 - accuracy: 0.7960 - val_loss: 0.6975 - val_accuracy: 0.7889\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6815 - accuracy: 0.7912 - val_loss: 0.7074 - val_accuracy: 0.7869\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6790 - accuracy: 0.7915 - val_loss: 0.6774 - val_accuracy: 0.7950\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6613 - accuracy: 0.8015 - val_loss: 0.6684 - val_accuracy: 0.7994\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6533 - accuracy: 0.7999 - val_loss: 0.7061 - val_accuracy: 0.7862\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6547 - accuracy: 0.8011 - val_loss: 0.6955 - val_accuracy: 0.7854\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6404 - accuracy: 0.8046 - val_loss: 0.6952 - val_accuracy: 0.7878\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6411 - accuracy: 0.8056 - val_loss: 0.7252 - val_accuracy: 0.7798\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6375 - accuracy: 0.8068 - val_loss: 0.6502 - val_accuracy: 0.8063\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6242 - accuracy: 0.8111 - val_loss: 0.6759 - val_accuracy: 0.7927\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6217 - accuracy: 0.8096 - val_loss: 0.6436 - val_accuracy: 0.8062\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6213 - accuracy: 0.8100 - val_loss: 0.6252 - val_accuracy: 0.8109\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6152 - accuracy: 0.8152 - val_loss: 0.6679 - val_accuracy: 0.7976\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6139 - accuracy: 0.8124 - val_loss: 0.6107 - val_accuracy: 0.8168\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6105 - accuracy: 0.8150 - val_loss: 0.6773 - val_accuracy: 0.7926\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6028 - accuracy: 0.8166 - val_loss: 0.6406 - val_accuracy: 0.8083\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6073 - accuracy: 0.8150 - val_loss: 0.6148 - val_accuracy: 0.8146\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5815 - accuracy: 0.8205 - val_loss: 0.6095 - val_accuracy: 0.8160\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5907 - accuracy: 0.8208 - val_loss: 0.6088 - val_accuracy: 0.8137\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5831 - accuracy: 0.8207 - val_loss: 0.6111 - val_accuracy: 0.8159\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5730 - accuracy: 0.8250 - val_loss: 0.6171 - val_accuracy: 0.8142\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5736 - accuracy: 0.8259 - val_loss: 0.5757 - val_accuracy: 0.8295\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5837 - accuracy: 0.8206 - val_loss: 0.5982 - val_accuracy: 0.8213\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5699 - accuracy: 0.8242 - val_loss: 0.5823 - val_accuracy: 0.8254\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5643 - accuracy: 0.8287 - val_loss: 0.6349 - val_accuracy: 0.8061\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5562 - accuracy: 0.8310 - val_loss: 0.5943 - val_accuracy: 0.8197\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5660 - accuracy: 0.8271 - val_loss: 0.5716 - val_accuracy: 0.8295\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5627 - accuracy: 0.8281 - val_loss: 0.6219 - val_accuracy: 0.8104\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5521 - accuracy: 0.8302 - val_loss: 0.5600 - val_accuracy: 0.8336\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5439 - accuracy: 0.8325 - val_loss: 0.6587 - val_accuracy: 0.7959\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5505 - accuracy: 0.8316 - val_loss: 0.5546 - val_accuracy: 0.8349\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5482 - accuracy: 0.8310 - val_loss: 0.5584 - val_accuracy: 0.8341\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5389 - accuracy: 0.8353 - val_loss: 0.5927 - val_accuracy: 0.8194\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5213 - accuracy: 0.8432 - val_loss: 0.5478 - val_accuracy: 0.8375\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5318 - accuracy: 0.8389 - val_loss: 0.5581 - val_accuracy: 0.8337\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5132 - accuracy: 0.8433 - val_loss: 0.5468 - val_accuracy: 0.8371\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5149 - accuracy: 0.8424 - val_loss: 0.5626 - val_accuracy: 0.8332\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5108 - accuracy: 0.8456 - val_loss: 0.5320 - val_accuracy: 0.8427\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5158 - accuracy: 0.8413 - val_loss: 0.5487 - val_accuracy: 0.8354\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5067 - accuracy: 0.8449 - val_loss: 0.5599 - val_accuracy: 0.8312\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5117 - accuracy: 0.8432 - val_loss: 0.5569 - val_accuracy: 0.8329\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5039 - accuracy: 0.8458 - val_loss: 0.5375 - val_accuracy: 0.8407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN-zuvG8MAG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a9ce569-725d-42e3-b89d-5c66c8a59e54"
      },
      "source": [
        "print('NN with weight initializers'); print('--'*40)\r\n",
        "results4 = model4.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results4[1]*100, 2), '%'))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with weight initializers\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5375 - accuracy: 0.8407\n",
            "Validation accuracy: 84.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7IzUTM8MMNx"
      },
      "source": [
        "**NN model, relu activations, Adam optimizers with weight initializers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3MpBeBzMNgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b352c7-dfab-496b-8b11-23d4305f6d3d"
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\r\n",
        "adam = optimizers.Adam(lr = 0.001)\r\n",
        "# Adding activation function - softmax for multiclass classification\r\n",
        "model4.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model4.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 1.2476 - accuracy: 0.6644 - val_loss: 0.7288 - val_accuracy: 0.7766\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7319 - accuracy: 0.7735 - val_loss: 0.8146 - val_accuracy: 0.7491\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7228 - accuracy: 0.7735 - val_loss: 0.7179 - val_accuracy: 0.7834\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6873 - accuracy: 0.7890 - val_loss: 0.7213 - val_accuracy: 0.7815\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6871 - accuracy: 0.7886 - val_loss: 0.7225 - val_accuracy: 0.7759\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6855 - accuracy: 0.7878 - val_loss: 0.6291 - val_accuracy: 0.8106\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6708 - accuracy: 0.7952 - val_loss: 0.6667 - val_accuracy: 0.7928\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6505 - accuracy: 0.7988 - val_loss: 0.6934 - val_accuracy: 0.7929\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6102 - accuracy: 0.8120 - val_loss: 0.6231 - val_accuracy: 0.8111\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5977 - accuracy: 0.8170 - val_loss: 0.6456 - val_accuracy: 0.8051\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6078 - accuracy: 0.8112 - val_loss: 0.6175 - val_accuracy: 0.8110\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5922 - accuracy: 0.8176 - val_loss: 0.6038 - val_accuracy: 0.8163\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5976 - accuracy: 0.8145 - val_loss: 0.6210 - val_accuracy: 0.8096\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5840 - accuracy: 0.8160 - val_loss: 0.5810 - val_accuracy: 0.8257\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5730 - accuracy: 0.8236 - val_loss: 0.5541 - val_accuracy: 0.8325\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5287 - accuracy: 0.8358 - val_loss: 0.6292 - val_accuracy: 0.8076\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5605 - accuracy: 0.8259 - val_loss: 0.5665 - val_accuracy: 0.8278\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5391 - accuracy: 0.8350 - val_loss: 0.5614 - val_accuracy: 0.8301\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5348 - accuracy: 0.8331 - val_loss: 0.5251 - val_accuracy: 0.8378\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5121 - accuracy: 0.8416 - val_loss: 0.5579 - val_accuracy: 0.8316\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5212 - accuracy: 0.8375 - val_loss: 0.6097 - val_accuracy: 0.8125\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5148 - accuracy: 0.8398 - val_loss: 0.5377 - val_accuracy: 0.8370\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5001 - accuracy: 0.8444 - val_loss: 0.5357 - val_accuracy: 0.8348\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4971 - accuracy: 0.8420 - val_loss: 0.5658 - val_accuracy: 0.8243\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4918 - accuracy: 0.8437 - val_loss: 0.5397 - val_accuracy: 0.8364\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4750 - accuracy: 0.8545 - val_loss: 0.4959 - val_accuracy: 0.8482\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4671 - accuracy: 0.8529 - val_loss: 0.5103 - val_accuracy: 0.8435\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4584 - accuracy: 0.8539 - val_loss: 0.5099 - val_accuracy: 0.8432\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4621 - accuracy: 0.8558 - val_loss: 0.4718 - val_accuracy: 0.8565\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4518 - accuracy: 0.8611 - val_loss: 0.4706 - val_accuracy: 0.8579\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4313 - accuracy: 0.8656 - val_loss: 0.5045 - val_accuracy: 0.8485\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4302 - accuracy: 0.8640 - val_loss: 0.4995 - val_accuracy: 0.8460\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4434 - accuracy: 0.8596 - val_loss: 0.4985 - val_accuracy: 0.8492\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4240 - accuracy: 0.8662 - val_loss: 0.4742 - val_accuracy: 0.8549\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4032 - accuracy: 0.8717 - val_loss: 0.4682 - val_accuracy: 0.8554\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4063 - accuracy: 0.8720 - val_loss: 0.4295 - val_accuracy: 0.8711\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4069 - accuracy: 0.8702 - val_loss: 0.4808 - val_accuracy: 0.8548\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4395 - accuracy: 0.8599 - val_loss: 0.4974 - val_accuracy: 0.8459\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3969 - accuracy: 0.8737 - val_loss: 0.4475 - val_accuracy: 0.8649\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4087 - accuracy: 0.8703 - val_loss: 0.4752 - val_accuracy: 0.8562\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4026 - accuracy: 0.8716 - val_loss: 0.4716 - val_accuracy: 0.8565\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3880 - accuracy: 0.8788 - val_loss: 0.4580 - val_accuracy: 0.8615\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3810 - accuracy: 0.8798 - val_loss: 0.4345 - val_accuracy: 0.8687\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3805 - accuracy: 0.8783 - val_loss: 0.4689 - val_accuracy: 0.8583\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3855 - accuracy: 0.8763 - val_loss: 0.4751 - val_accuracy: 0.8571\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3815 - accuracy: 0.8792 - val_loss: 0.4465 - val_accuracy: 0.8655\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3983 - accuracy: 0.8705 - val_loss: 0.4174 - val_accuracy: 0.8760\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3607 - accuracy: 0.8861 - val_loss: 0.4706 - val_accuracy: 0.8578\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3660 - accuracy: 0.8848 - val_loss: 0.4026 - val_accuracy: 0.8821\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3538 - accuracy: 0.8853 - val_loss: 0.4448 - val_accuracy: 0.8658\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3625 - accuracy: 0.8854 - val_loss: 0.4252 - val_accuracy: 0.8734\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3562 - accuracy: 0.8888 - val_loss: 0.4665 - val_accuracy: 0.8614\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3556 - accuracy: 0.8871 - val_loss: 0.4414 - val_accuracy: 0.8688\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3595 - accuracy: 0.8837 - val_loss: 0.4086 - val_accuracy: 0.8771\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3273 - accuracy: 0.8958 - val_loss: 0.4422 - val_accuracy: 0.8669\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3454 - accuracy: 0.8906 - val_loss: 0.4694 - val_accuracy: 0.8605\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3588 - accuracy: 0.8870 - val_loss: 0.4112 - val_accuracy: 0.8794\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3368 - accuracy: 0.8922 - val_loss: 0.4162 - val_accuracy: 0.8760\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3575 - accuracy: 0.8850 - val_loss: 0.3947 - val_accuracy: 0.8840\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3335 - accuracy: 0.8941 - val_loss: 0.4092 - val_accuracy: 0.8790\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3215 - accuracy: 0.8979 - val_loss: 0.3943 - val_accuracy: 0.8852\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3140 - accuracy: 0.9007 - val_loss: 0.3978 - val_accuracy: 0.8838\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3238 - accuracy: 0.8982 - val_loss: 0.4080 - val_accuracy: 0.8814\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3161 - accuracy: 0.8995 - val_loss: 0.4338 - val_accuracy: 0.8709\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3196 - accuracy: 0.8982 - val_loss: 0.3882 - val_accuracy: 0.8873\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3025 - accuracy: 0.9037 - val_loss: 0.4092 - val_accuracy: 0.8803\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3117 - accuracy: 0.8970 - val_loss: 0.3919 - val_accuracy: 0.8863\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3022 - accuracy: 0.9014 - val_loss: 0.4148 - val_accuracy: 0.8793\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3087 - accuracy: 0.9002 - val_loss: 0.4024 - val_accuracy: 0.8835\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3162 - accuracy: 0.8959 - val_loss: 0.3710 - val_accuracy: 0.8937\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2946 - accuracy: 0.9045 - val_loss: 0.3762 - val_accuracy: 0.8923\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2958 - accuracy: 0.9022 - val_loss: 0.4038 - val_accuracy: 0.8835\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3002 - accuracy: 0.9016 - val_loss: 0.4289 - val_accuracy: 0.8747\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2999 - accuracy: 0.9032 - val_loss: 0.3726 - val_accuracy: 0.8935\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2976 - accuracy: 0.9033 - val_loss: 0.3740 - val_accuracy: 0.8930\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2816 - accuracy: 0.9080 - val_loss: 0.4058 - val_accuracy: 0.8805\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2909 - accuracy: 0.9047 - val_loss: 0.4159 - val_accuracy: 0.8805\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2891 - accuracy: 0.9062 - val_loss: 0.4374 - val_accuracy: 0.8720\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2738 - accuracy: 0.9093 - val_loss: 0.3789 - val_accuracy: 0.8936\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2768 - accuracy: 0.9098 - val_loss: 0.3556 - val_accuracy: 0.9014\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2772 - accuracy: 0.9101 - val_loss: 0.3608 - val_accuracy: 0.9003\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2661 - accuracy: 0.9132 - val_loss: 0.4011 - val_accuracy: 0.8850\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2802 - accuracy: 0.9071 - val_loss: 0.3788 - val_accuracy: 0.8925\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2578 - accuracy: 0.9154 - val_loss: 0.4093 - val_accuracy: 0.8842\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2801 - accuracy: 0.9078 - val_loss: 0.3858 - val_accuracy: 0.8918\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2807 - accuracy: 0.9082 - val_loss: 0.4014 - val_accuracy: 0.8839\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2448 - accuracy: 0.9198 - val_loss: 0.3924 - val_accuracy: 0.8894\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2762 - accuracy: 0.9071 - val_loss: 0.3587 - val_accuracy: 0.9024\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.9150 - val_loss: 0.3820 - val_accuracy: 0.8926\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2393 - accuracy: 0.9219 - val_loss: 0.3801 - val_accuracy: 0.8952\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2660 - accuracy: 0.9124 - val_loss: 0.3868 - val_accuracy: 0.8935\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2483 - accuracy: 0.9199 - val_loss: 0.3862 - val_accuracy: 0.8924\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2605 - accuracy: 0.9133 - val_loss: 0.4050 - val_accuracy: 0.8850\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2474 - accuracy: 0.9173 - val_loss: 0.4368 - val_accuracy: 0.8756\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.2392 - accuracy: 0.9216 - val_loss: 0.3968 - val_accuracy: 0.8891\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2358 - accuracy: 0.9226 - val_loss: 0.3518 - val_accuracy: 0.9051\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2247 - accuracy: 0.9263 - val_loss: 0.4064 - val_accuracy: 0.8877\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2627 - accuracy: 0.9118 - val_loss: 0.4255 - val_accuracy: 0.8829\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2363 - accuracy: 0.9236 - val_loss: 0.4713 - val_accuracy: 0.8688\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2582 - accuracy: 0.9148 - val_loss: 0.3546 - val_accuracy: 0.9046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOK0-cSIMWHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c705c65-dcde-49c6-8bf5-f84c3bd59db5"
      },
      "source": [
        "print('NN with weight initializers'); print('--'*40)\r\n",
        "results4 = model4.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results4[1]*100, 2), '%'))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with weight initializers\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3546 - accuracy: 0.9046\n",
            "Validation accuracy: 90.46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKJExQuaMlR4"
      },
      "source": [
        "**Observation 6 - Weight initializers**\r\n",
        "Adding weight initialiers didn't result in improvement of score.\r\n",
        "relu activations, changing number of activators, Adam optimizers gives the best score out of the ones tried as of now.\r\n",
        "Next, let's try batch normalization.\r\n",
        "\r\n",
        "**Batch Normalization**\r\n",
        "Batch Normalization, one of the methods to prevent the \"internal covariance shift\" problem, has proven to be highly effective. Normalize each mini-batch before nonlinearity.\r\n",
        "\r\n",
        "**NN model, relu activations, SGD optimizers with weight initializers and batch normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEFGtJ9yMrQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746cb974-f60c-43af-f9e8-b28454a0d648"
      },
      "source": [
        "print('NN model with batch normalization'); print('--'*40)\r\n",
        "# Initialize the neural network classifier\r\n",
        "model5 = Sequential()\r\n",
        "\r\n",
        "# Input Layer - adding input layer and activation functions relu and weight initializer\r\n",
        "model5.add(Dense(256, input_shape = (1024, ), kernel_initializer = 'he_normal'))\r\n",
        "# Adding batch normalization\r\n",
        "model5.add(BatchNormalization())\r\n",
        "# Adding activation function\r\n",
        "model5.add(Activation('relu'))\r\n",
        "\r\n",
        "#Hidden Layer 1 - adding first hidden layer\r\n",
        "model5.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding batch normalization\r\n",
        "model5.add(BatchNormalization())\r\n",
        "# Adding activation function\r\n",
        "model5.add(Activation('relu'))\r\n",
        "\r\n",
        "#Hidden Layer 2 - adding second hidden layer\r\n",
        "model5.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding batch normalization\r\n",
        "model5.add(BatchNormalization())\r\n",
        "# Adding activation function\r\n",
        "model5.add(Activation('relu'))\r\n",
        "\r\n",
        "#Hidden Layer 3 - adding third hidden layer\r\n",
        "model5.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding batch normalization\r\n",
        "model5.add(BatchNormalization())\r\n",
        "# Adding activation function\r\n",
        "model5.add(Activation('relu'))\r\n",
        "\r\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\r\n",
        "model5.add(Dense(10, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding activation function\r\n",
        "model5.add(Activation('softmax'))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with batch normalization\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIp84U94Mvyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ca6974-8f5d-47ba-e928-85c634a8d105"
      },
      "source": [
        "model5.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 307,882\n",
            "Trainable params: 306,922\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEywczCjM1k0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ae77fcf-ca8f-4663-991d-009d35237830"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\r\n",
        "sgd = optimizers.SGD(lr = 0.01)\r\n",
        "# Adding activation function - softmax for multiclass classification\r\n",
        "model5.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model5.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 2.5127 - accuracy: 0.1414 - val_loss: 2.2163 - val_accuracy: 0.1946\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.9609 - accuracy: 0.3325 - val_loss: 1.8471 - val_accuracy: 0.3794\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.6684 - accuracy: 0.4704 - val_loss: 1.5840 - val_accuracy: 0.4962\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.4676 - accuracy: 0.5481 - val_loss: 1.4100 - val_accuracy: 0.5626\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.3040 - accuracy: 0.6026 - val_loss: 1.2984 - val_accuracy: 0.5990\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 1.1861 - accuracy: 0.6419 - val_loss: 1.1701 - val_accuracy: 0.6413\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 1.0795 - accuracy: 0.6764 - val_loss: 1.1238 - val_accuracy: 0.6569\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.0008 - accuracy: 0.6967 - val_loss: 1.0787 - val_accuracy: 0.6598\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.9289 - accuracy: 0.7197 - val_loss: 0.9832 - val_accuracy: 0.6952\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.8782 - accuracy: 0.7321 - val_loss: 0.9500 - val_accuracy: 0.7036\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8308 - accuracy: 0.7453 - val_loss: 0.9414 - val_accuracy: 0.7028\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.7905 - accuracy: 0.7562 - val_loss: 0.9472 - val_accuracy: 0.7018\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.7690 - accuracy: 0.7628 - val_loss: 0.8873 - val_accuracy: 0.7202\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.7282 - accuracy: 0.7744 - val_loss: 1.0314 - val_accuracy: 0.6755\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.7011 - accuracy: 0.7834 - val_loss: 0.8604 - val_accuracy: 0.7304\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.6807 - accuracy: 0.7898 - val_loss: 0.7790 - val_accuracy: 0.7553\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.6578 - accuracy: 0.7963 - val_loss: 0.7350 - val_accuracy: 0.7712\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.6405 - accuracy: 0.8013 - val_loss: 0.7275 - val_accuracy: 0.7700\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.6266 - accuracy: 0.8049 - val_loss: 0.9822 - val_accuracy: 0.7007\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.6081 - accuracy: 0.8120 - val_loss: 0.7941 - val_accuracy: 0.7505\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.5813 - accuracy: 0.8215 - val_loss: 0.7913 - val_accuracy: 0.7516\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5763 - accuracy: 0.8185 - val_loss: 0.7390 - val_accuracy: 0.7653\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5543 - accuracy: 0.8292 - val_loss: 0.7591 - val_accuracy: 0.7596\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.5437 - accuracy: 0.8296 - val_loss: 0.9086 - val_accuracy: 0.7137\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5225 - accuracy: 0.8393 - val_loss: 0.6992 - val_accuracy: 0.7800\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5141 - accuracy: 0.8392 - val_loss: 0.7695 - val_accuracy: 0.7568\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.5072 - accuracy: 0.8437 - val_loss: 1.0399 - val_accuracy: 0.6872\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.4931 - accuracy: 0.8454 - val_loss: 0.6928 - val_accuracy: 0.7822\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4832 - accuracy: 0.8515 - val_loss: 0.8104 - val_accuracy: 0.7492\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4755 - accuracy: 0.8546 - val_loss: 0.7213 - val_accuracy: 0.7714\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.4651 - accuracy: 0.8565 - val_loss: 0.7583 - val_accuracy: 0.7663\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.4554 - accuracy: 0.8590 - val_loss: 0.8536 - val_accuracy: 0.7370\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4322 - accuracy: 0.8669 - val_loss: 0.9086 - val_accuracy: 0.7349\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4438 - accuracy: 0.8634 - val_loss: 0.6977 - val_accuracy: 0.7813\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4330 - accuracy: 0.8670 - val_loss: 0.7439 - val_accuracy: 0.7716\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.4178 - accuracy: 0.8698 - val_loss: 0.6893 - val_accuracy: 0.7864\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4097 - accuracy: 0.8718 - val_loss: 1.0728 - val_accuracy: 0.6994\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.4092 - accuracy: 0.8710 - val_loss: 0.9074 - val_accuracy: 0.7238\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3958 - accuracy: 0.8793 - val_loss: 0.7254 - val_accuracy: 0.7778\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3908 - accuracy: 0.8785 - val_loss: 0.7266 - val_accuracy: 0.7796\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3922 - accuracy: 0.8762 - val_loss: 0.6004 - val_accuracy: 0.8126\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3703 - accuracy: 0.8858 - val_loss: 0.7007 - val_accuracy: 0.7806\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3675 - accuracy: 0.8846 - val_loss: 0.6702 - val_accuracy: 0.7929\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3620 - accuracy: 0.8872 - val_loss: 0.6282 - val_accuracy: 0.8108\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3518 - accuracy: 0.8921 - val_loss: 0.6588 - val_accuracy: 0.7969\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3632 - accuracy: 0.8883 - val_loss: 1.0536 - val_accuracy: 0.7059\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3430 - accuracy: 0.8937 - val_loss: 0.6830 - val_accuracy: 0.7928\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3437 - accuracy: 0.8939 - val_loss: 0.5431 - val_accuracy: 0.8342\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3347 - accuracy: 0.8984 - val_loss: 0.5410 - val_accuracy: 0.8323\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3261 - accuracy: 0.8969 - val_loss: 0.7958 - val_accuracy: 0.7617\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3251 - accuracy: 0.9012 - val_loss: 0.6089 - val_accuracy: 0.8103\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3259 - accuracy: 0.9005 - val_loss: 0.7691 - val_accuracy: 0.7686\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3138 - accuracy: 0.9031 - val_loss: 0.8071 - val_accuracy: 0.7639\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3148 - accuracy: 0.9024 - val_loss: 0.5316 - val_accuracy: 0.8371\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3109 - accuracy: 0.9029 - val_loss: 0.6301 - val_accuracy: 0.8065\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3051 - accuracy: 0.9059 - val_loss: 0.5816 - val_accuracy: 0.8241\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3024 - accuracy: 0.9088 - val_loss: 0.6453 - val_accuracy: 0.8062\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2885 - accuracy: 0.9122 - val_loss: 0.5839 - val_accuracy: 0.8230\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2871 - accuracy: 0.9115 - val_loss: 0.6700 - val_accuracy: 0.7972\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2874 - accuracy: 0.9116 - val_loss: 1.0192 - val_accuracy: 0.7175\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2910 - accuracy: 0.9083 - val_loss: 0.6870 - val_accuracy: 0.7987\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2787 - accuracy: 0.9162 - val_loss: 0.6817 - val_accuracy: 0.7988\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2739 - accuracy: 0.9162 - val_loss: 0.7231 - val_accuracy: 0.7828\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2751 - accuracy: 0.9152 - val_loss: 0.4755 - val_accuracy: 0.8550\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2615 - accuracy: 0.9218 - val_loss: 0.6595 - val_accuracy: 0.7943\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2573 - accuracy: 0.9224 - val_loss: 0.5953 - val_accuracy: 0.8151\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2493 - accuracy: 0.9261 - val_loss: 0.6553 - val_accuracy: 0.8028\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2556 - accuracy: 0.9199 - val_loss: 0.5226 - val_accuracy: 0.8419\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2488 - accuracy: 0.9232 - val_loss: 0.6033 - val_accuracy: 0.8148\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2410 - accuracy: 0.9256 - val_loss: 0.6033 - val_accuracy: 0.8194\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2434 - accuracy: 0.9247 - val_loss: 0.7072 - val_accuracy: 0.8090\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2352 - accuracy: 0.9293 - val_loss: 1.2253 - val_accuracy: 0.6954\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2418 - accuracy: 0.9261 - val_loss: 0.5406 - val_accuracy: 0.8390\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2329 - accuracy: 0.9290 - val_loss: 0.5653 - val_accuracy: 0.8296\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2327 - accuracy: 0.9293 - val_loss: 0.7000 - val_accuracy: 0.7973\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2293 - accuracy: 0.9304 - val_loss: 0.5941 - val_accuracy: 0.8216\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2230 - accuracy: 0.9317 - val_loss: 0.6482 - val_accuracy: 0.8163\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2209 - accuracy: 0.9329 - val_loss: 0.4637 - val_accuracy: 0.8632\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2157 - accuracy: 0.9352 - val_loss: 0.5135 - val_accuracy: 0.8466\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2111 - accuracy: 0.9360 - val_loss: 0.6771 - val_accuracy: 0.8115\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2171 - accuracy: 0.9350 - val_loss: 0.5064 - val_accuracy: 0.8496\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2089 - accuracy: 0.9377 - val_loss: 0.5182 - val_accuracy: 0.8442\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2054 - accuracy: 0.9394 - val_loss: 0.6326 - val_accuracy: 0.8219\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2081 - accuracy: 0.9368 - val_loss: 0.5144 - val_accuracy: 0.8546\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1956 - accuracy: 0.9399 - val_loss: 0.5097 - val_accuracy: 0.8524\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1974 - accuracy: 0.9404 - val_loss: 0.7128 - val_accuracy: 0.7975\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1962 - accuracy: 0.9409 - val_loss: 0.8390 - val_accuracy: 0.7828\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.1979 - accuracy: 0.9388 - val_loss: 0.4664 - val_accuracy: 0.8644\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9398 - val_loss: 0.6520 - val_accuracy: 0.8098\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1922 - accuracy: 0.9410 - val_loss: 0.7445 - val_accuracy: 0.7952\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1778 - accuracy: 0.9466 - val_loss: 0.6198 - val_accuracy: 0.8234\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1786 - accuracy: 0.9469 - val_loss: 1.0708 - val_accuracy: 0.7354\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1801 - accuracy: 0.9441 - val_loss: 0.6365 - val_accuracy: 0.8191\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1774 - accuracy: 0.9474 - val_loss: 0.7490 - val_accuracy: 0.7952\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1746 - accuracy: 0.9479 - val_loss: 0.5189 - val_accuracy: 0.8517\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1615 - accuracy: 0.9518 - val_loss: 1.2954 - val_accuracy: 0.6970\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1778 - accuracy: 0.9454 - val_loss: 0.5802 - val_accuracy: 0.8379\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1695 - accuracy: 0.9486 - val_loss: 0.5713 - val_accuracy: 0.8382\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1672 - accuracy: 0.9500 - val_loss: 0.5594 - val_accuracy: 0.8411\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1678 - accuracy: 0.9496 - val_loss: 0.7282 - val_accuracy: 0.8030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIIS29prM2RU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b1f07a-f53e-47c7-c93d-e382ec458d18"
      },
      "source": [
        "print('NN with batch normalization'); print('--'*40)\r\n",
        "results5 = model5.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results5[1]*100, 2), '%'))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with batch normalization\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7282 - accuracy: 0.8030\n",
            "Validation accuracy: 80.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiydnL8DM7Ve"
      },
      "source": [
        "**NN model, relu activations, Adam optimizers with weight initializers and batch normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHgmXapRM5Ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b574fce-1bf1-468e-fb16-074ebd73420f"
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\r\n",
        "adam = optimizers.Adam(lr = 0.001)\r\n",
        "# Adding activation function - softmax for multiclass classification\r\n",
        "model5.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model5.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.8943 - accuracy: 0.7473 - val_loss: 2.0959 - val_accuracy: 0.4600\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5799 - accuracy: 0.8148 - val_loss: 1.5290 - val_accuracy: 0.5096\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5020 - accuracy: 0.8380 - val_loss: 1.8730 - val_accuracy: 0.4971\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.4657 - accuracy: 0.8488 - val_loss: 1.1412 - val_accuracy: 0.6554\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.4327 - accuracy: 0.8605 - val_loss: 1.2379 - val_accuracy: 0.6293\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.4330 - accuracy: 0.8605 - val_loss: 1.1713 - val_accuracy: 0.6431\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.4137 - accuracy: 0.8666 - val_loss: 1.1813 - val_accuracy: 0.6318\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3924 - accuracy: 0.8744 - val_loss: 1.5407 - val_accuracy: 0.5460\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3705 - accuracy: 0.8793 - val_loss: 1.0222 - val_accuracy: 0.6845\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3659 - accuracy: 0.8827 - val_loss: 2.0096 - val_accuracy: 0.5212\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3430 - accuracy: 0.8893 - val_loss: 1.1557 - val_accuracy: 0.6518\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3418 - accuracy: 0.8892 - val_loss: 0.8709 - val_accuracy: 0.7254\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3200 - accuracy: 0.8973 - val_loss: 1.0472 - val_accuracy: 0.6979\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3206 - accuracy: 0.8958 - val_loss: 1.0113 - val_accuracy: 0.7024\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2957 - accuracy: 0.9054 - val_loss: 1.3164 - val_accuracy: 0.6378\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2991 - accuracy: 0.9035 - val_loss: 1.1663 - val_accuracy: 0.6503\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.3054 - accuracy: 0.9016 - val_loss: 0.9819 - val_accuracy: 0.7038\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.2772 - accuracy: 0.9102 - val_loss: 1.2879 - val_accuracy: 0.6503\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2842 - accuracy: 0.9076 - val_loss: 0.9289 - val_accuracy: 0.7031\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2759 - accuracy: 0.9109 - val_loss: 1.2526 - val_accuracy: 0.6790\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2679 - accuracy: 0.9118 - val_loss: 1.0954 - val_accuracy: 0.6836\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2581 - accuracy: 0.9160 - val_loss: 0.9526 - val_accuracy: 0.7403\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2512 - accuracy: 0.9186 - val_loss: 0.8913 - val_accuracy: 0.7442\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2383 - accuracy: 0.9234 - val_loss: 1.7455 - val_accuracy: 0.6166\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2375 - accuracy: 0.9237 - val_loss: 0.9879 - val_accuracy: 0.7194\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2262 - accuracy: 0.9260 - val_loss: 1.0640 - val_accuracy: 0.7051\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2271 - accuracy: 0.9272 - val_loss: 1.1409 - val_accuracy: 0.7067\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2176 - accuracy: 0.9288 - val_loss: 0.9883 - val_accuracy: 0.7288\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2169 - accuracy: 0.9303 - val_loss: 1.4384 - val_accuracy: 0.6425\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2126 - accuracy: 0.9304 - val_loss: 1.1751 - val_accuracy: 0.7038\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.2083 - accuracy: 0.9344 - val_loss: 1.5206 - val_accuracy: 0.6544\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2043 - accuracy: 0.9331 - val_loss: 1.1932 - val_accuracy: 0.7162\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1844 - accuracy: 0.9417 - val_loss: 1.0751 - val_accuracy: 0.7298\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2013 - accuracy: 0.9339 - val_loss: 0.9204 - val_accuracy: 0.7483\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1993 - accuracy: 0.9344 - val_loss: 1.2577 - val_accuracy: 0.7062\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1869 - accuracy: 0.9395 - val_loss: 0.7218 - val_accuracy: 0.7930\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1699 - accuracy: 0.9439 - val_loss: 1.0384 - val_accuracy: 0.7271\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1680 - accuracy: 0.9450 - val_loss: 0.8669 - val_accuracy: 0.7618\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1728 - accuracy: 0.9428 - val_loss: 0.8467 - val_accuracy: 0.7594\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1678 - accuracy: 0.9428 - val_loss: 1.6584 - val_accuracy: 0.6791\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1711 - accuracy: 0.9446 - val_loss: 0.7573 - val_accuracy: 0.7866\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1641 - accuracy: 0.9451 - val_loss: 1.7719 - val_accuracy: 0.6523\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1603 - accuracy: 0.9479 - val_loss: 1.0056 - val_accuracy: 0.7486\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1464 - accuracy: 0.9525 - val_loss: 1.0979 - val_accuracy: 0.7534\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1531 - accuracy: 0.9495 - val_loss: 0.9787 - val_accuracy: 0.7470\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1618 - accuracy: 0.9468 - val_loss: 1.2559 - val_accuracy: 0.7046\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1478 - accuracy: 0.9513 - val_loss: 0.9716 - val_accuracy: 0.7507\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1469 - accuracy: 0.9525 - val_loss: 0.7702 - val_accuracy: 0.7969\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1339 - accuracy: 0.9561 - val_loss: 0.9264 - val_accuracy: 0.7737\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1355 - accuracy: 0.9539 - val_loss: 1.1115 - val_accuracy: 0.7572\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1345 - accuracy: 0.9556 - val_loss: 0.8603 - val_accuracy: 0.7827\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1302 - accuracy: 0.9575 - val_loss: 1.1489 - val_accuracy: 0.7310\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1413 - accuracy: 0.9529 - val_loss: 0.7637 - val_accuracy: 0.7966\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1251 - accuracy: 0.9592 - val_loss: 1.4742 - val_accuracy: 0.6758\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1248 - accuracy: 0.9589 - val_loss: 0.6606 - val_accuracy: 0.8312\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1181 - accuracy: 0.9613 - val_loss: 1.2740 - val_accuracy: 0.7194\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1349 - accuracy: 0.9554 - val_loss: 0.8900 - val_accuracy: 0.7903\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1159 - accuracy: 0.9609 - val_loss: 0.9480 - val_accuracy: 0.7711\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1295 - accuracy: 0.9577 - val_loss: 0.7744 - val_accuracy: 0.8206\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1167 - accuracy: 0.9612 - val_loss: 0.8758 - val_accuracy: 0.7818\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1318 - accuracy: 0.9562 - val_loss: 0.9819 - val_accuracy: 0.7659\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1170 - accuracy: 0.9624 - val_loss: 0.8631 - val_accuracy: 0.7975\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1120 - accuracy: 0.9636 - val_loss: 0.9422 - val_accuracy: 0.7714\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1100 - accuracy: 0.9641 - val_loss: 0.6611 - val_accuracy: 0.8390\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0963 - accuracy: 0.9678 - val_loss: 1.2376 - val_accuracy: 0.7601\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1131 - accuracy: 0.9633 - val_loss: 0.8110 - val_accuracy: 0.8089\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1027 - accuracy: 0.9661 - val_loss: 0.7439 - val_accuracy: 0.8159\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1017 - accuracy: 0.9660 - val_loss: 0.7394 - val_accuracy: 0.8173\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0940 - accuracy: 0.9694 - val_loss: 0.9663 - val_accuracy: 0.7975\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1008 - accuracy: 0.9649 - val_loss: 0.8756 - val_accuracy: 0.7915\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.1080 - accuracy: 0.9639 - val_loss: 1.4033 - val_accuracy: 0.7294\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1174 - accuracy: 0.9610 - val_loss: 0.8635 - val_accuracy: 0.8027\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1022 - accuracy: 0.9662 - val_loss: 0.9319 - val_accuracy: 0.7798\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0947 - accuracy: 0.9677 - val_loss: 0.8259 - val_accuracy: 0.8020\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0953 - accuracy: 0.9681 - val_loss: 0.7116 - val_accuracy: 0.8349\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.1002 - accuracy: 0.9663 - val_loss: 1.1350 - val_accuracy: 0.7397\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.1067 - accuracy: 0.9634 - val_loss: 0.9047 - val_accuracy: 0.8009\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.0938 - accuracy: 0.9699 - val_loss: 0.8593 - val_accuracy: 0.8130\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.0930 - accuracy: 0.9687 - val_loss: 1.0302 - val_accuracy: 0.7758\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0887 - accuracy: 0.9709 - val_loss: 0.8416 - val_accuracy: 0.8253\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.0892 - accuracy: 0.9679 - val_loss: 0.9815 - val_accuracy: 0.7883\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.0955 - accuracy: 0.9682 - val_loss: 0.9801 - val_accuracy: 0.7872\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0868 - accuracy: 0.9710 - val_loss: 0.9607 - val_accuracy: 0.7857\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0959 - accuracy: 0.9670 - val_loss: 0.7762 - val_accuracy: 0.8288\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0825 - accuracy: 0.9717 - val_loss: 0.8818 - val_accuracy: 0.8075\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0913 - accuracy: 0.9699 - val_loss: 1.0329 - val_accuracy: 0.7832\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0821 - accuracy: 0.9721 - val_loss: 0.8736 - val_accuracy: 0.8106\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0832 - accuracy: 0.9713 - val_loss: 1.0619 - val_accuracy: 0.7965\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0879 - accuracy: 0.9694 - val_loss: 0.8695 - val_accuracy: 0.8023\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.0819 - accuracy: 0.9721 - val_loss: 0.8531 - val_accuracy: 0.8068\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.0710 - accuracy: 0.9772 - val_loss: 1.0742 - val_accuracy: 0.7782\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0817 - accuracy: 0.9720 - val_loss: 0.6918 - val_accuracy: 0.8437\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0772 - accuracy: 0.9743 - val_loss: 0.9514 - val_accuracy: 0.8008\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0879 - accuracy: 0.9708 - val_loss: 0.7055 - val_accuracy: 0.8435\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.0713 - accuracy: 0.9761 - val_loss: 0.8121 - val_accuracy: 0.8249\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.0703 - accuracy: 0.9754 - val_loss: 1.5469 - val_accuracy: 0.7172\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.0863 - accuracy: 0.9697 - val_loss: 1.1433 - val_accuracy: 0.7805\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0826 - accuracy: 0.9723 - val_loss: 0.8890 - val_accuracy: 0.8090\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0782 - accuracy: 0.9736 - val_loss: 0.9194 - val_accuracy: 0.8027\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.0792 - accuracy: 0.9729 - val_loss: 0.7427 - val_accuracy: 0.8327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe96ApztNF41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a705b9a6-37df-4ccd-ab38-ed7b8d666e22"
      },
      "source": [
        "print('NN with batch normalization'); print('--'*40)\r\n",
        "results5 = model5.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results5[1]*100, 2), '%'))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with batch normalization\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7427 - accuracy: 0.8327\n",
            "Validation accuracy: 83.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7en2uSGNJVK"
      },
      "source": [
        "**Observation 7 - Batch Normalization**\r\n",
        "Batch normalization didn't result in improvement of score.\r\n",
        "Relu activations, changing number of activators, Adam optimizers achieved the best score.\r\n",
        "Next, let's try batch normalization with dropout.\r\n",
        "\r\n",
        "**Dropout**\r\n",
        "**NN model, relu activations, SGD optimizers with weight initializers, batch normalization and dropout**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcNh4INVNRHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ff3d6c-219c-47e1-af53-02a4f9c2ae0a"
      },
      "source": [
        "print('NN model with dropout - sgd optimizer'); print('--'*40)\r\n",
        "# Initialize the neural network classifier\r\n",
        "model6 = Sequential()\r\n",
        "# Input Layer - adding input layer and activation functions relu and weight initializer\r\n",
        "model6.add(Dense(512, input_shape = (1024, ), kernel_initializer = 'he_normal'))\r\n",
        "# Adding batch normalization\r\n",
        "model6.add(BatchNormalization()) \r\n",
        "# Adding activation function\r\n",
        "model6.add(Activation('relu'))\r\n",
        "# Adding dropout layer\r\n",
        "model6.add(Dropout(0.2))\r\n",
        "\r\n",
        "#Hidden Layer 1 - adding first hidden layer\r\n",
        "model6.add(Dense(256, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding batch normalization\r\n",
        "model6.add(BatchNormalization())\r\n",
        "# Adding activation function\r\n",
        "model6.add(Activation('relu'))\r\n",
        "# Adding dropout layer\r\n",
        "model6.add(Dropout(0.2))\r\n",
        "\r\n",
        "#Hidden Layer 2 - adding second hidden layer\r\n",
        "model6.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding batch normalization\r\n",
        "model6.add(BatchNormalization())\r\n",
        "# Adding activation function\r\n",
        "model6.add(Activation('relu'))\r\n",
        "# Adding dropout layer\r\n",
        "model6.add(Dropout(0.2))\r\n",
        "\r\n",
        "#Hidden Layer 3 - adding third hidden layer\r\n",
        "model6.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding batch normalization\r\n",
        "model6.add(BatchNormalization())\r\n",
        "# Adding activation function\r\n",
        "model6.add(Activation('relu'))\r\n",
        "# Adding dropout layer\r\n",
        "model6.add(Dropout(0.2))\r\n",
        "\r\n",
        "#Hidden Layer 4 - adding fourth hidden layer\r\n",
        "model6.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\r\n",
        "# Adding batch normalization\r\n",
        "model6.add(BatchNormalization())\r\n",
        "# Adding activation function\r\n",
        "model6.add(Activation('relu'))\r\n",
        "# Adding dropout layer\r\n",
        "model6.add(Dropout(0.2))\r\n",
        "\r\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\r\n",
        "model6.add(Dense(10, kernel_initializer = 'he_normal',bias_initializer = 'he_uniform'))\r\n",
        "# Adding activation function\r\n",
        "model6.add(Activation('softmax'))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with dropout - sgd optimizer\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rILElNcFNRwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ff0054-ce1f-4b0a-ed7f-55cb216a1b59"
      },
      "source": [
        "model6.summary()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 703,658\n",
            "Trainable params: 701,674\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYLcE18MNUbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c490fb8-c9d4-4468-8b21-e7027139f84e"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\r\n",
        "sgd = optimizers.SGD(lr = 0.01)\r\n",
        "model6.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Adding activation function - softmax for multiclass classification\r\n",
        "history = model6.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 3s 9ms/step - loss: 2.6774 - accuracy: 0.1028 - val_loss: 2.3048 - val_accuracy: 0.1145\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 2.4614 - accuracy: 0.1233 - val_loss: 2.2240 - val_accuracy: 0.1836\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 2.3604 - accuracy: 0.1444 - val_loss: 2.1569 - val_accuracy: 0.2304\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 2.2676 - accuracy: 0.1744 - val_loss: 2.0482 - val_accuracy: 0.2909\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 2.1910 - accuracy: 0.2015 - val_loss: 1.9662 - val_accuracy: 0.3325\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 2.1103 - accuracy: 0.2355 - val_loss: 1.8594 - val_accuracy: 0.3747\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 2.0304 - accuracy: 0.2676 - val_loss: 1.7758 - val_accuracy: 0.4079\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.9622 - accuracy: 0.2872 - val_loss: 1.7055 - val_accuracy: 0.4350\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.9001 - accuracy: 0.3133 - val_loss: 1.6476 - val_accuracy: 0.4602\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.8401 - accuracy: 0.3395 - val_loss: 1.5961 - val_accuracy: 0.4915\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 1.7859 - accuracy: 0.3647 - val_loss: 1.5348 - val_accuracy: 0.5219\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 1.7431 - accuracy: 0.3839 - val_loss: 1.4849 - val_accuracy: 0.5337\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.6899 - accuracy: 0.4006 - val_loss: 1.4233 - val_accuracy: 0.5518\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.6401 - accuracy: 0.4228 - val_loss: 1.3606 - val_accuracy: 0.5883\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.6041 - accuracy: 0.4360 - val_loss: 1.3148 - val_accuracy: 0.5882\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 1.5654 - accuracy: 0.4557 - val_loss: 1.2819 - val_accuracy: 0.6093\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.5127 - accuracy: 0.4739 - val_loss: 1.2481 - val_accuracy: 0.6165\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.4813 - accuracy: 0.4888 - val_loss: 1.1959 - val_accuracy: 0.6236\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 1.4561 - accuracy: 0.4974 - val_loss: 1.1589 - val_accuracy: 0.6234\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.4207 - accuracy: 0.5115 - val_loss: 1.0988 - val_accuracy: 0.6593\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.3825 - accuracy: 0.5286 - val_loss: 1.0745 - val_accuracy: 0.6693\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.3635 - accuracy: 0.5386 - val_loss: 1.0380 - val_accuracy: 0.6798\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 1.3196 - accuracy: 0.5518 - val_loss: 1.0238 - val_accuracy: 0.6842\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.3037 - accuracy: 0.5588 - val_loss: 1.0144 - val_accuracy: 0.6801\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.2653 - accuracy: 0.5754 - val_loss: 0.9939 - val_accuracy: 0.6843\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.2488 - accuracy: 0.5809 - val_loss: 0.9495 - val_accuracy: 0.7076\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 1.2316 - accuracy: 0.5885 - val_loss: 0.9334 - val_accuracy: 0.7162\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.2042 - accuracy: 0.5973 - val_loss: 0.9151 - val_accuracy: 0.7128\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.1963 - accuracy: 0.6076 - val_loss: 0.9298 - val_accuracy: 0.7121\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.1648 - accuracy: 0.6181 - val_loss: 0.8935 - val_accuracy: 0.7224\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 1.1615 - accuracy: 0.6152 - val_loss: 0.8565 - val_accuracy: 0.7406\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.1347 - accuracy: 0.6249 - val_loss: 0.8553 - val_accuracy: 0.7396\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.1086 - accuracy: 0.6362 - val_loss: 0.8399 - val_accuracy: 0.7489\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.0989 - accuracy: 0.6387 - val_loss: 0.8291 - val_accuracy: 0.7457\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.0837 - accuracy: 0.6460 - val_loss: 0.8613 - val_accuracy: 0.7366\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.0641 - accuracy: 0.6543 - val_loss: 0.8224 - val_accuracy: 0.7529\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 1.0540 - accuracy: 0.6568 - val_loss: 0.7851 - val_accuracy: 0.7619\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.0449 - accuracy: 0.6619 - val_loss: 0.7720 - val_accuracy: 0.7655\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.0225 - accuracy: 0.6719 - val_loss: 0.7749 - val_accuracy: 0.7662\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 1.0247 - accuracy: 0.6693 - val_loss: 0.7736 - val_accuracy: 0.7650\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.9977 - accuracy: 0.6813 - val_loss: 0.7390 - val_accuracy: 0.7784\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.9843 - accuracy: 0.6847 - val_loss: 0.7933 - val_accuracy: 0.7596\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.9750 - accuracy: 0.6882 - val_loss: 0.8031 - val_accuracy: 0.7482\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.9699 - accuracy: 0.6896 - val_loss: 0.7483 - val_accuracy: 0.7698\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.9608 - accuracy: 0.6942 - val_loss: 0.7486 - val_accuracy: 0.7680\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.9659 - accuracy: 0.6929 - val_loss: 0.7292 - val_accuracy: 0.7762\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.9357 - accuracy: 0.7043 - val_loss: 0.7043 - val_accuracy: 0.7886\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.9261 - accuracy: 0.7063 - val_loss: 0.7271 - val_accuracy: 0.7743\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.9077 - accuracy: 0.7149 - val_loss: 0.6714 - val_accuracy: 0.7970\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.9077 - accuracy: 0.7103 - val_loss: 0.6515 - val_accuracy: 0.8080\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.9071 - accuracy: 0.7151 - val_loss: 0.6467 - val_accuracy: 0.8045\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.8913 - accuracy: 0.7210 - val_loss: 0.6509 - val_accuracy: 0.8026\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.8829 - accuracy: 0.7207 - val_loss: 0.6720 - val_accuracy: 0.7954\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.8845 - accuracy: 0.7234 - val_loss: 0.6428 - val_accuracy: 0.8071\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.8731 - accuracy: 0.7249 - val_loss: 0.6093 - val_accuracy: 0.8175\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.8627 - accuracy: 0.7277 - val_loss: 0.6240 - val_accuracy: 0.8140\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.8584 - accuracy: 0.7319 - val_loss: 0.6050 - val_accuracy: 0.8199\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.8365 - accuracy: 0.7364 - val_loss: 0.6609 - val_accuracy: 0.7968\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.8418 - accuracy: 0.7386 - val_loss: 0.6291 - val_accuracy: 0.8058\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.8351 - accuracy: 0.7376 - val_loss: 0.6360 - val_accuracy: 0.8039\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.8405 - accuracy: 0.7423 - val_loss: 0.5964 - val_accuracy: 0.8207\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.8186 - accuracy: 0.7443 - val_loss: 0.6192 - val_accuracy: 0.8109\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.8149 - accuracy: 0.7481 - val_loss: 0.5752 - val_accuracy: 0.8273\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.8037 - accuracy: 0.7499 - val_loss: 0.5754 - val_accuracy: 0.8262\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.7979 - accuracy: 0.7544 - val_loss: 0.5687 - val_accuracy: 0.8270\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7949 - accuracy: 0.7557 - val_loss: 0.6038 - val_accuracy: 0.8163\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7885 - accuracy: 0.7585 - val_loss: 0.5850 - val_accuracy: 0.8207\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7716 - accuracy: 0.7636 - val_loss: 0.5949 - val_accuracy: 0.8176\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7759 - accuracy: 0.7591 - val_loss: 0.5483 - val_accuracy: 0.8365\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7729 - accuracy: 0.7637 - val_loss: 0.5805 - val_accuracy: 0.8208\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7720 - accuracy: 0.7589 - val_loss: 0.6355 - val_accuracy: 0.8025\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.7772 - accuracy: 0.7650 - val_loss: 0.6035 - val_accuracy: 0.8159\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7584 - accuracy: 0.7670 - val_loss: 0.5517 - val_accuracy: 0.8339\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.7506 - accuracy: 0.7701 - val_loss: 0.5703 - val_accuracy: 0.8248\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7536 - accuracy: 0.7711 - val_loss: 0.5645 - val_accuracy: 0.8285\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7418 - accuracy: 0.7719 - val_loss: 0.5434 - val_accuracy: 0.8328\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.7259 - accuracy: 0.7757 - val_loss: 0.5531 - val_accuracy: 0.8335\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7347 - accuracy: 0.7765 - val_loss: 0.5863 - val_accuracy: 0.8214\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7355 - accuracy: 0.7745 - val_loss: 0.5132 - val_accuracy: 0.8461\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.7156 - accuracy: 0.7787 - val_loss: 0.5842 - val_accuracy: 0.8222\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.7247 - accuracy: 0.7789 - val_loss: 0.5595 - val_accuracy: 0.8283\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7158 - accuracy: 0.7824 - val_loss: 0.5069 - val_accuracy: 0.8497\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7172 - accuracy: 0.7800 - val_loss: 0.5536 - val_accuracy: 0.8278\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7053 - accuracy: 0.7860 - val_loss: 0.6104 - val_accuracy: 0.8123\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.7052 - accuracy: 0.7848 - val_loss: 0.4868 - val_accuracy: 0.8554\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 7ms/step - loss: 0.7090 - accuracy: 0.7841 - val_loss: 0.5053 - val_accuracy: 0.8490\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.6988 - accuracy: 0.7858 - val_loss: 0.6882 - val_accuracy: 0.7892\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.6888 - accuracy: 0.7910 - val_loss: 0.5092 - val_accuracy: 0.8444\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.6931 - accuracy: 0.7900 - val_loss: 0.5475 - val_accuracy: 0.8312\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.6878 - accuracy: 0.7916 - val_loss: 0.5472 - val_accuracy: 0.8307\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.6751 - accuracy: 0.7956 - val_loss: 0.5574 - val_accuracy: 0.8310\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.6660 - accuracy: 0.7973 - val_loss: 0.5274 - val_accuracy: 0.8379\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.6639 - accuracy: 0.7982 - val_loss: 0.5366 - val_accuracy: 0.8347\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.6555 - accuracy: 0.8010 - val_loss: 0.5067 - val_accuracy: 0.8451\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.6475 - accuracy: 0.8005 - val_loss: 0.4753 - val_accuracy: 0.8565\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.6556 - accuracy: 0.8030 - val_loss: 0.5437 - val_accuracy: 0.8327\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.6594 - accuracy: 0.8003 - val_loss: 0.4639 - val_accuracy: 0.8616\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.6583 - accuracy: 0.8003 - val_loss: 0.4626 - val_accuracy: 0.8602\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.6531 - accuracy: 0.8018 - val_loss: 0.4782 - val_accuracy: 0.8550\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.6470 - accuracy: 0.8031 - val_loss: 0.4882 - val_accuracy: 0.8527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEpAN-cBNW_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aefe4841-a01a-455a-9fdd-b657b930b195"
      },
      "source": [
        "print('NN model with dropout - sgd optimizer'); print('--'*40)\r\n",
        "results6 = model6.evaluate(X_val, y_val)\r\n",
        "print('Validation accuracy: {}'.format(round(results6[1]*100, 2), '%'))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with dropout - sgd optimizer\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4882 - accuracy: 0.8527\n",
            "Validation accuracy: 85.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCydhnJWNj-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89afba17-68b4-49de-a515-cfe032374787"
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\r\n",
        "adam = optimizers.Adam(lr = 0.001)\r\n",
        "model6.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Adding activation function - softmax for multiclass classification\r\n",
        "history = model6.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 3s 8ms/step - loss: 1.1192 - accuracy: 0.6540 - val_loss: 1.4036 - val_accuracy: 0.5228\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.9152 - accuracy: 0.7142 - val_loss: 1.0832 - val_accuracy: 0.6567\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.8435 - accuracy: 0.7404 - val_loss: 1.4289 - val_accuracy: 0.5132\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.8042 - accuracy: 0.7563 - val_loss: 1.0426 - val_accuracy: 0.6532\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.7599 - accuracy: 0.7674 - val_loss: 1.5779 - val_accuracy: 0.5092\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.7407 - accuracy: 0.7754 - val_loss: 0.8996 - val_accuracy: 0.7060\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.7141 - accuracy: 0.7838 - val_loss: 0.9838 - val_accuracy: 0.6737\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.6958 - accuracy: 0.7904 - val_loss: 1.0491 - val_accuracy: 0.6774\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.6609 - accuracy: 0.7998 - val_loss: 0.9508 - val_accuracy: 0.6964\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.6483 - accuracy: 0.8082 - val_loss: 0.8882 - val_accuracy: 0.7160\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.6337 - accuracy: 0.8090 - val_loss: 1.0227 - val_accuracy: 0.6700\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.6297 - accuracy: 0.8112 - val_loss: 1.0227 - val_accuracy: 0.6702\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.6201 - accuracy: 0.8129 - val_loss: 1.0599 - val_accuracy: 0.6474\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5935 - accuracy: 0.8207 - val_loss: 0.9238 - val_accuracy: 0.6811\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5722 - accuracy: 0.8288 - val_loss: 0.8864 - val_accuracy: 0.7146\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5697 - accuracy: 0.8286 - val_loss: 0.9806 - val_accuracy: 0.6829\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5687 - accuracy: 0.8289 - val_loss: 0.8503 - val_accuracy: 0.7440\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.5486 - accuracy: 0.8358 - val_loss: 1.0763 - val_accuracy: 0.6427\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5629 - accuracy: 0.8311 - val_loss: 0.7218 - val_accuracy: 0.7857\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5516 - accuracy: 0.8319 - val_loss: 0.7806 - val_accuracy: 0.7602\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5303 - accuracy: 0.8427 - val_loss: 1.0327 - val_accuracy: 0.6548\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5362 - accuracy: 0.8356 - val_loss: 0.7810 - val_accuracy: 0.7439\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5164 - accuracy: 0.8442 - val_loss: 0.8443 - val_accuracy: 0.7232\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.5121 - accuracy: 0.8465 - val_loss: 0.8686 - val_accuracy: 0.7181\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.5046 - accuracy: 0.8499 - val_loss: 0.7797 - val_accuracy: 0.7684\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4810 - accuracy: 0.8556 - val_loss: 0.8828 - val_accuracy: 0.7064\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4935 - accuracy: 0.8515 - val_loss: 0.7834 - val_accuracy: 0.7572\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4734 - accuracy: 0.8570 - val_loss: 0.9635 - val_accuracy: 0.6984\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4864 - accuracy: 0.8545 - val_loss: 0.9477 - val_accuracy: 0.6858\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4782 - accuracy: 0.8566 - val_loss: 0.8305 - val_accuracy: 0.7285\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4771 - accuracy: 0.8582 - val_loss: 0.8799 - val_accuracy: 0.7245\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4581 - accuracy: 0.8599 - val_loss: 0.7893 - val_accuracy: 0.7398\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4427 - accuracy: 0.8666 - val_loss: 0.7127 - val_accuracy: 0.7855\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.4486 - accuracy: 0.8656 - val_loss: 0.6284 - val_accuracy: 0.8047\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4467 - accuracy: 0.8658 - val_loss: 0.9041 - val_accuracy: 0.7046\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4450 - accuracy: 0.8635 - val_loss: 1.0339 - val_accuracy: 0.6712\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4360 - accuracy: 0.8673 - val_loss: 0.8025 - val_accuracy: 0.7437\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4327 - accuracy: 0.8687 - val_loss: 0.8336 - val_accuracy: 0.7291\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4275 - accuracy: 0.8711 - val_loss: 0.6934 - val_accuracy: 0.7833\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4220 - accuracy: 0.8746 - val_loss: 0.6121 - val_accuracy: 0.8036\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4285 - accuracy: 0.8714 - val_loss: 0.6674 - val_accuracy: 0.7858\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.4049 - accuracy: 0.8775 - val_loss: 0.7741 - val_accuracy: 0.7387\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.4120 - accuracy: 0.8734 - val_loss: 0.7209 - val_accuracy: 0.7666\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4135 - accuracy: 0.8745 - val_loss: 0.7863 - val_accuracy: 0.7410\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.4124 - accuracy: 0.8784 - val_loss: 0.5069 - val_accuracy: 0.8438\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.4037 - accuracy: 0.8791 - val_loss: 0.9062 - val_accuracy: 0.7128\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3927 - accuracy: 0.8811 - val_loss: 0.8150 - val_accuracy: 0.7365\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.3837 - accuracy: 0.8835 - val_loss: 0.6820 - val_accuracy: 0.7751\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3746 - accuracy: 0.8859 - val_loss: 0.7029 - val_accuracy: 0.7786\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3792 - accuracy: 0.8866 - val_loss: 0.5974 - val_accuracy: 0.8068\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3802 - accuracy: 0.8863 - val_loss: 0.7190 - val_accuracy: 0.7684\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3820 - accuracy: 0.8858 - val_loss: 1.1635 - val_accuracy: 0.6688\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3759 - accuracy: 0.8868 - val_loss: 0.7366 - val_accuracy: 0.7650\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3750 - accuracy: 0.8865 - val_loss: 0.6277 - val_accuracy: 0.8058\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3714 - accuracy: 0.8895 - val_loss: 0.6353 - val_accuracy: 0.7950\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3647 - accuracy: 0.8915 - val_loss: 0.6996 - val_accuracy: 0.7685\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3643 - accuracy: 0.8905 - val_loss: 0.5979 - val_accuracy: 0.8103\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3531 - accuracy: 0.8926 - val_loss: 0.5704 - val_accuracy: 0.8261\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3529 - accuracy: 0.8959 - val_loss: 0.7789 - val_accuracy: 0.7528\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3608 - accuracy: 0.8898 - val_loss: 0.7773 - val_accuracy: 0.7488\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3614 - accuracy: 0.8932 - val_loss: 0.5919 - val_accuracy: 0.8090\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3476 - accuracy: 0.8953 - val_loss: 0.5391 - val_accuracy: 0.8280\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3413 - accuracy: 0.8958 - val_loss: 0.6258 - val_accuracy: 0.7985\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.3457 - accuracy: 0.8939 - val_loss: 0.7770 - val_accuracy: 0.7550\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3309 - accuracy: 0.9015 - val_loss: 0.6912 - val_accuracy: 0.7789\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.3473 - accuracy: 0.8943 - val_loss: 0.6224 - val_accuracy: 0.8009\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3278 - accuracy: 0.9003 - val_loss: 0.6959 - val_accuracy: 0.7713\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3331 - accuracy: 0.9012 - val_loss: 0.7427 - val_accuracy: 0.7596\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3205 - accuracy: 0.9024 - val_loss: 0.8675 - val_accuracy: 0.7266\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3371 - accuracy: 0.8976 - val_loss: 0.5684 - val_accuracy: 0.8166\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3290 - accuracy: 0.9034 - val_loss: 0.5063 - val_accuracy: 0.8394\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.3341 - accuracy: 0.9009 - val_loss: 0.5682 - val_accuracy: 0.8178\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3200 - accuracy: 0.9044 - val_loss: 0.6825 - val_accuracy: 0.7794\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3201 - accuracy: 0.9042 - val_loss: 0.5356 - val_accuracy: 0.8302\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3162 - accuracy: 0.9072 - val_loss: 0.5180 - val_accuracy: 0.8337\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3132 - accuracy: 0.9053 - val_loss: 0.5619 - val_accuracy: 0.8194\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3174 - accuracy: 0.9039 - val_loss: 0.5195 - val_accuracy: 0.8305\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.3124 - accuracy: 0.9045 - val_loss: 0.5723 - val_accuracy: 0.8158\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3148 - accuracy: 0.9054 - val_loss: 0.5457 - val_accuracy: 0.8292\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2958 - accuracy: 0.9105 - val_loss: 0.6032 - val_accuracy: 0.8124\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3165 - accuracy: 0.9051 - val_loss: 0.5107 - val_accuracy: 0.8396\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3080 - accuracy: 0.9094 - val_loss: 0.6010 - val_accuracy: 0.8073\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.3017 - accuracy: 0.9078 - val_loss: 0.6927 - val_accuracy: 0.7809\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2995 - accuracy: 0.9086 - val_loss: 0.7257 - val_accuracy: 0.7671\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.3018 - accuracy: 0.9107 - val_loss: 0.8931 - val_accuracy: 0.7328\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2955 - accuracy: 0.9127 - val_loss: 0.6247 - val_accuracy: 0.7947\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2960 - accuracy: 0.9123 - val_loss: 0.5973 - val_accuracy: 0.8036\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2830 - accuracy: 0.9138 - val_loss: 0.5668 - val_accuracy: 0.8195\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.3041 - accuracy: 0.9077 - val_loss: 0.5561 - val_accuracy: 0.8169\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2951 - accuracy: 0.9118 - val_loss: 0.6118 - val_accuracy: 0.7982\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2850 - accuracy: 0.9135 - val_loss: 0.6415 - val_accuracy: 0.7967\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2748 - accuracy: 0.9168 - val_loss: 0.6522 - val_accuracy: 0.8044\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2825 - accuracy: 0.9167 - val_loss: 0.6165 - val_accuracy: 0.8083\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2791 - accuracy: 0.9166 - val_loss: 0.6557 - val_accuracy: 0.7859\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 2s 7ms/step - loss: 0.2733 - accuracy: 0.9190 - val_loss: 0.4662 - val_accuracy: 0.8499\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2728 - accuracy: 0.9178 - val_loss: 0.4614 - val_accuracy: 0.8538\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2789 - accuracy: 0.9167 - val_loss: 0.5705 - val_accuracy: 0.8224\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2750 - accuracy: 0.9160 - val_loss: 0.6085 - val_accuracy: 0.7977\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2836 - accuracy: 0.9135 - val_loss: 0.6782 - val_accuracy: 0.7884\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 2s 8ms/step - loss: 0.2741 - accuracy: 0.9170 - val_loss: 0.5063 - val_accuracy: 0.8417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YDD4FW2Nq6w"
      },
      "source": [
        "**Observation 8 - Batch Normalization and Dropout**\r\n",
        "Didn't result in any improvement of score.\r\n",
        "NN model, relu activations, SGD optimizers with weight initializers and batch normalization is still the best model.\r\n",
        "Next, let's try batch normalization and dropout with adam optimizer.\r\n",
        "\r\n",
        "**Prediction on test dataset using Model 3 - relu activations, Adam optimizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G493B3SHOJnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c010bd8-ab45-48f0-8509-f95f6929a5bc"
      },
      "source": [
        "\r\n",
        "print('NN model with relu activations and changing number of activators'); print('--'*40)\r\n",
        "# Initialize the neural network classifier\r\n",
        "model3 = Sequential()\r\n",
        "\r\n",
        "# Input Layer - adding input layer and activation functions relu\r\n",
        "model3.add(Dense(256, input_shape = (1024, )))\r\n",
        "# Adding activation function\r\n",
        "model3.add(Activation('relu'))\r\n",
        "\r\n",
        "#Hidden Layer 1 - adding first hidden layer\r\n",
        "model3.add(Dense(128))\r\n",
        "# Adding activation function\r\n",
        "model3.add(Activation('relu'))\r\n",
        "\r\n",
        "#Hidden Layer 2 - Adding second hidden layer\r\n",
        "model3.add(Dense(64))\r\n",
        "# Adding activation function\r\n",
        "model3.add(Activation('relu'))\r\n",
        "\r\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\r\n",
        "model3.add(Dense(10))\r\n",
        "# Adding activation function - softmax for multiclass classification\r\n",
        "model3.add(Activation('softmax'))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with relu activations and changing number of activators\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_iwUXd7Nmct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a235f6-2277-4b73-9b97-4c81fd317adf"
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\r\n",
        "adam = optimizers.Adam(lr = 0.001)\r\n",
        "model3.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Fitting the neural network for training\r\n",
        "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 2s 6ms/step - loss: 2.3062 - accuracy: 0.1075 - val_loss: 2.1099 - val_accuracy: 0.1809\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.9415 - accuracy: 0.2953 - val_loss: 1.5750 - val_accuracy: 0.4786\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.4597 - accuracy: 0.5189 - val_loss: 1.2618 - val_accuracy: 0.6003\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 1.2437 - accuracy: 0.6056 - val_loss: 1.1368 - val_accuracy: 0.6423\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.1378 - accuracy: 0.6415 - val_loss: 1.0664 - val_accuracy: 0.6693\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0761 - accuracy: 0.6616 - val_loss: 1.0348 - val_accuracy: 0.6758\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 1.0178 - accuracy: 0.6855 - val_loss: 0.9505 - val_accuracy: 0.7078\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9679 - accuracy: 0.7004 - val_loss: 0.9414 - val_accuracy: 0.7087\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.9160 - accuracy: 0.7143 - val_loss: 0.9544 - val_accuracy: 0.6983\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8869 - accuracy: 0.7264 - val_loss: 0.8956 - val_accuracy: 0.7220\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8520 - accuracy: 0.7342 - val_loss: 0.8240 - val_accuracy: 0.7465\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.8164 - accuracy: 0.7477 - val_loss: 0.8263 - val_accuracy: 0.7473\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.8035 - accuracy: 0.7516 - val_loss: 0.7567 - val_accuracy: 0.7690\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7723 - accuracy: 0.7627 - val_loss: 0.7677 - val_accuracy: 0.7663\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7369 - accuracy: 0.7737 - val_loss: 0.7749 - val_accuracy: 0.7623\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7281 - accuracy: 0.7749 - val_loss: 0.7607 - val_accuracy: 0.7670\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7145 - accuracy: 0.7785 - val_loss: 0.6847 - val_accuracy: 0.7939\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.7080 - accuracy: 0.7851 - val_loss: 0.6769 - val_accuracy: 0.7976\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6899 - accuracy: 0.7890 - val_loss: 0.7006 - val_accuracy: 0.7871\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6688 - accuracy: 0.7958 - val_loss: 0.6830 - val_accuracy: 0.7935\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6548 - accuracy: 0.8010 - val_loss: 0.6805 - val_accuracy: 0.7942\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6506 - accuracy: 0.8025 - val_loss: 0.6538 - val_accuracy: 0.8031\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6392 - accuracy: 0.8037 - val_loss: 0.6385 - val_accuracy: 0.8070\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.8061 - val_loss: 0.6293 - val_accuracy: 0.8103\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.6143 - accuracy: 0.8125 - val_loss: 0.6068 - val_accuracy: 0.8173\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5914 - accuracy: 0.8214 - val_loss: 0.6608 - val_accuracy: 0.7972\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.6038 - accuracy: 0.8112 - val_loss: 0.6474 - val_accuracy: 0.8003\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5830 - accuracy: 0.8196 - val_loss: 0.5778 - val_accuracy: 0.8272\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5720 - accuracy: 0.8229 - val_loss: 0.5730 - val_accuracy: 0.8285\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5582 - accuracy: 0.8315 - val_loss: 0.5792 - val_accuracy: 0.8260\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5607 - accuracy: 0.8287 - val_loss: 0.6307 - val_accuracy: 0.8095\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5662 - accuracy: 0.8228 - val_loss: 0.6181 - val_accuracy: 0.8158\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5465 - accuracy: 0.8321 - val_loss: 0.5683 - val_accuracy: 0.8288\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5293 - accuracy: 0.8373 - val_loss: 0.5697 - val_accuracy: 0.8270\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5307 - accuracy: 0.8386 - val_loss: 0.5253 - val_accuracy: 0.8429\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5183 - accuracy: 0.8413 - val_loss: 0.5684 - val_accuracy: 0.8265\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5287 - accuracy: 0.8364 - val_loss: 0.5902 - val_accuracy: 0.8178\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5209 - accuracy: 0.8388 - val_loss: 0.5436 - val_accuracy: 0.8363\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.5093 - accuracy: 0.8414 - val_loss: 0.5712 - val_accuracy: 0.8276\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.5136 - accuracy: 0.8384 - val_loss: 0.5590 - val_accuracy: 0.8308\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4929 - accuracy: 0.8483 - val_loss: 0.5518 - val_accuracy: 0.8321\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4916 - accuracy: 0.8509 - val_loss: 0.5603 - val_accuracy: 0.8270\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4853 - accuracy: 0.8497 - val_loss: 0.5313 - val_accuracy: 0.8386\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4791 - accuracy: 0.8525 - val_loss: 0.4902 - val_accuracy: 0.8540\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4688 - accuracy: 0.8562 - val_loss: 0.5028 - val_accuracy: 0.8490\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4752 - accuracy: 0.8529 - val_loss: 0.5105 - val_accuracy: 0.8489\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4775 - accuracy: 0.8510 - val_loss: 0.5094 - val_accuracy: 0.8466\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4580 - accuracy: 0.8568 - val_loss: 0.5228 - val_accuracy: 0.8433\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4648 - accuracy: 0.8553 - val_loss: 0.5126 - val_accuracy: 0.8454\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4601 - accuracy: 0.8582 - val_loss: 0.5184 - val_accuracy: 0.8425\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4525 - accuracy: 0.8587 - val_loss: 0.4826 - val_accuracy: 0.8564\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4387 - accuracy: 0.8623 - val_loss: 0.4989 - val_accuracy: 0.8517\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4539 - accuracy: 0.8573 - val_loss: 0.5089 - val_accuracy: 0.8471\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4393 - accuracy: 0.8612 - val_loss: 0.4829 - val_accuracy: 0.8548\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4301 - accuracy: 0.8652 - val_loss: 0.5448 - val_accuracy: 0.8342\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4408 - accuracy: 0.8627 - val_loss: 0.4888 - val_accuracy: 0.8533\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4340 - accuracy: 0.8647 - val_loss: 0.4659 - val_accuracy: 0.8614\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4363 - accuracy: 0.8638 - val_loss: 0.4755 - val_accuracy: 0.8570\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4125 - accuracy: 0.8732 - val_loss: 0.4625 - val_accuracy: 0.8623\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4215 - accuracy: 0.8700 - val_loss: 0.5019 - val_accuracy: 0.8500\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.4276 - accuracy: 0.8666 - val_loss: 0.4841 - val_accuracy: 0.8544\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4175 - accuracy: 0.8680 - val_loss: 0.4615 - val_accuracy: 0.8621\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4091 - accuracy: 0.8719 - val_loss: 0.4783 - val_accuracy: 0.8566\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4058 - accuracy: 0.8737 - val_loss: 0.4760 - val_accuracy: 0.8572\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.4026 - accuracy: 0.8745 - val_loss: 0.4368 - val_accuracy: 0.8700\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3924 - accuracy: 0.8777 - val_loss: 0.4637 - val_accuracy: 0.8622\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3824 - accuracy: 0.8794 - val_loss: 0.4591 - val_accuracy: 0.8647\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3835 - accuracy: 0.8803 - val_loss: 0.4584 - val_accuracy: 0.8639\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3980 - accuracy: 0.8724 - val_loss: 0.4453 - val_accuracy: 0.8680\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3781 - accuracy: 0.8808 - val_loss: 0.4437 - val_accuracy: 0.8686\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3831 - accuracy: 0.8786 - val_loss: 0.4470 - val_accuracy: 0.8681\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3855 - accuracy: 0.8789 - val_loss: 0.4668 - val_accuracy: 0.8622\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3770 - accuracy: 0.8812 - val_loss: 0.4463 - val_accuracy: 0.8675\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3660 - accuracy: 0.8838 - val_loss: 0.4614 - val_accuracy: 0.8641\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3776 - accuracy: 0.8832 - val_loss: 0.4229 - val_accuracy: 0.8747\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3716 - accuracy: 0.8830 - val_loss: 0.4092 - val_accuracy: 0.8801\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3587 - accuracy: 0.8867 - val_loss: 0.4608 - val_accuracy: 0.8644\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3748 - accuracy: 0.8808 - val_loss: 0.4522 - val_accuracy: 0.8668\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3523 - accuracy: 0.8877 - val_loss: 0.4375 - val_accuracy: 0.8721\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3576 - accuracy: 0.8866 - val_loss: 0.4416 - val_accuracy: 0.8693\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3668 - accuracy: 0.8839 - val_loss: 0.4215 - val_accuracy: 0.8765\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3526 - accuracy: 0.8879 - val_loss: 0.4579 - val_accuracy: 0.8640\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3497 - accuracy: 0.8884 - val_loss: 0.4268 - val_accuracy: 0.8748\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3480 - accuracy: 0.8879 - val_loss: 0.4646 - val_accuracy: 0.8619\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3562 - accuracy: 0.8878 - val_loss: 0.4192 - val_accuracy: 0.8781\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3404 - accuracy: 0.8952 - val_loss: 0.4121 - val_accuracy: 0.8803\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3366 - accuracy: 0.8936 - val_loss: 0.4291 - val_accuracy: 0.8758\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3414 - accuracy: 0.8922 - val_loss: 0.4479 - val_accuracy: 0.8683\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3423 - accuracy: 0.8925 - val_loss: 0.4617 - val_accuracy: 0.8617\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3418 - accuracy: 0.8932 - val_loss: 0.4250 - val_accuracy: 0.8753\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8931 - val_loss: 0.4316 - val_accuracy: 0.8734\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3206 - accuracy: 0.8954 - val_loss: 0.4181 - val_accuracy: 0.8776\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3286 - accuracy: 0.8965 - val_loss: 0.4192 - val_accuracy: 0.8786\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3187 - accuracy: 0.8997 - val_loss: 0.4143 - val_accuracy: 0.8782\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3184 - accuracy: 0.8991 - val_loss: 0.4296 - val_accuracy: 0.8749\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3140 - accuracy: 0.8978 - val_loss: 0.4003 - val_accuracy: 0.8848\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.9015 - val_loss: 0.4040 - val_accuracy: 0.8856\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3183 - accuracy: 0.8988 - val_loss: 0.4022 - val_accuracy: 0.8848\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 1s 5ms/step - loss: 0.3150 - accuracy: 0.8974 - val_loss: 0.4552 - val_accuracy: 0.8639\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 1s 6ms/step - loss: 0.3139 - accuracy: 0.8981 - val_loss: 0.4080 - val_accuracy: 0.8838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbdj1CyMO7co",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aaf91e9-a363-4852-87b2-06852a40ce5f"
      },
      "source": [
        "print('Testing the model on test dataset')\r\n",
        "predictions = model3.predict_classes(X_test)\r\n",
        "score = model3.evaluate(X_test, y_test)\r\n",
        "print('Test loss :', score[0])\r\n",
        "print('Test accuracy :', score[1])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing the model on test dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "563/563 [==============================] - 1s 2ms/step - loss: 0.6746 - accuracy: 0.8302\n",
            "Test loss : 0.6745544672012329\n",
            "Test accuracy : 0.8302222490310669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIYFmJqdO-DE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9562d102-3af9-4860-9a88-595122fd9dd6"
      },
      "source": [
        "print('Classification Report'); print('--'*40)\r\n",
        "print(classification_report(y_test_o, predictions))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85      1814\n",
            "           1       0.83      0.85      0.84      1828\n",
            "           2       0.91      0.81      0.86      1803\n",
            "           3       0.78      0.79      0.78      1719\n",
            "           4       0.85      0.87      0.86      1812\n",
            "           5       0.81      0.82      0.81      1768\n",
            "           6       0.85      0.82      0.84      1832\n",
            "           7       0.84      0.89      0.86      1808\n",
            "           8       0.77      0.80      0.78      1812\n",
            "           9       0.85      0.77      0.81      1804\n",
            "\n",
            "    accuracy                           0.83     18000\n",
            "   macro avg       0.83      0.83      0.83     18000\n",
            "weighted avg       0.83      0.83      0.83     18000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ao2KKUgP5pF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "14c8b5ec-3078-452b-bc57-afb642f43954"
      },
      "source": [
        "print('Visualizing the confusion matrix')\r\n",
        "plt.figure(figsize = (15, 7.2))\r\n",
        "sns.heatmap(confusion_matrix(y_test_o, predictions), annot = True)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Visualizing the confusion matrix\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7e64768358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAGlCAYAAACLCgVDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3gU1eLG8e/ZLAQChCJggPhDFCxwBQSkSa+hS7UhoChFQBFp0tVLBxWvhaJYUAwgQgDpobckQEJvAaVDEBEILSSZ3x+JuUEJhEvCzMb38zz7kJyZzLw7zMzOmXPmrLEsCxEREREREXEOl90BRERERERE5EaqqImIiIiIiDiMKmoiIiIiIiIOo4qaiIiIiIiIw6iiJiIiIiIi4jCqqImIiIiIiDiMKmoiIiIiIiIpMMZMNcZEGWN2/qW8hzFmrzFmlzFmTLLyd4wxkcaYfcaY+snKAxLLIo0x/W+7Xn2PmoiIiIiIyM0ZY6oB0cC3lmX9K7GsJjAQaGRZ1jVjTH7LsqKMMcWBH4DyQEFgOfBI4qL2A3WBY0AY8LxlWbtTWq87Pd5MzNFtqv2lgm+xJnZH8Cix8XF2R/AYXi41lqeWy2hbpdb1uFi7I3gMHYOSXtwuL7sjeIx4NUak2pUrh43dGf5X1387lKb/0ZnyPvS3bWFZ1hpjzIN/Ke4KjLIs61riPFGJ5c2AwMTyX4wxkSRU2gAiLcs6BGCMCUycN8WKmj5JRERERERE7swjQFVjTIgxZrUx5qnE8kLA0WTzHUssS6k8RenSoiYiIiIiIpLu7Otx5QbyABWBp4CZxpiH0noFIiIiIiIinseKt2vNx4CfrIQBP0KNMfFAXuA48ECy+fwTy7hF+U2p66OIiIiIiMidmQvUBDDGPAJkBn4D5gHPGWO8jTFFgGJAKAmDhxQzxhQxxmQGnkucN0VqURMREREREc8Un/4tasaYH4AaQF5jzDFgKDAVmJo4ZH8M0D6xdW2XMWYmCYOExALdLMuKS1xOd2AJ4AVMtSxr1y3Xmx7D82vUx9TRqI93RqM+pp5GnEs9jfqYehr1MfV0DEp60aiPqadRH1PPo0d9PLknbUd9LPC4Y7aFWtRERERERMQjWfY9o5buVFETERERERHPdA+6PtpFfTNEREREREQcRi1qIiIiIiLimdT1UURERERExGEy8GBz6vooIiIiIiLiMGpRExERERERz6SujyIiIiIiIg6jUR/vvcFjP6N6q1dp/urbKc4TFrGLVp378EzHXnToNfSu1xkTc53e739Iw3Y9eKH7AI6figJgx95IWnXuQ6vOfWjZqQ/B60Lvel1O4e3tzdq18wgNXczWrcsZPLgXABMnjiE0dDFhYUuYPn0i2bL52JzUeSL3byJ863I2hy1l08aFdsdxFH//AixZMoOI8GDCty6ne7dXAChZsjhrVgcRGrKYDet/ply50jYntV/CMRhESMgitmxZxqBBbwFQo8bTbNjwM5s2LSQ4+EceeqiwzUmd55FHHmZz2NKk1++/7eWNHq/aHcsxUjoOBw16i0MHwwgNWUxoyGIC6te0Oan9tK3unMvlYsPGn/lx9pc3lI8dN5TTUbtsSuVMLpeLjRsXMnv2VAC6dGnPzp2ruXLlMPfdl9vmdOJkxkqHb22PObrtrhe6eftufLJmYeDoT5nzxfi/Tb8QfYmX3hjExJEDKXB/Xs6eO899uXOmatnHT0UxaMxnfPXBsBvKA4OWsP+Xwwzp2YlFK9cTvC6UcYPf4srVa2TK5Mbt5cWZs+do1bkPwTMm4fbyuqv36FusyV39fVrJls2HS5cu43a7WbFiNr17D2PPngNcvBgNwOjRgzlz5izjxn1ma85Yhz0sGrl/ExUqNeDs2XN2R/kbL5e992D8/PLj55efiIidZM+ejU0bF9Kq9auMHzeMjz+ewpKlqwioX5Neb3elXr02tmZ1GfvvV914DP5I797v8sUXH9C69Wvs2xdJp04vUa5cKTp16m1rzutxsbau/1ZcLhdHft1C5SqNOXLkuN1xbD8GIeXjsFWrxlyKvsyHH02yO6JjeNK2crvu7tojrfTo0ZEyZUqSwzc7rVp2BODJMk/Q7fWXadK0PvfnL2FzQohPh2vc/8Ubb7yasK1yZKdly1coVaoE586dZ+nSQJ5+uokjriOuXDls7M7wv7p2cFOa/kd7P1zRMdvitp8kxpjHjDH9jDEfJ776GWMeT+9g5UoWJ2eO7ClOXxi8jtpVKlDg/rwAN1TS5i9fw/Pd3qFV5z68++Fk4uJS1yS6csNmmtarAUDdahUJCd+JZVlkzeKdVCm7FnMdcMz/X5q4dOkyAJkyucmUyY1lWUmVNICsWbOQHhV6ybhOnYoiImInANHRl9i7N5JChfywLIscvjkA8M3py8mTp+2M6RjJj0G3OxOWZWFZFr6+CedAX98c2la3UbtWFQ4dOuyISppTpHQcyt9pW92ZgoX8CAioxddfByaVuVwuhg8fwKBBI21M5jyFErfVV1/9d1tt27aLI0eO2Zgqg4mPT9uXg9yyomaM6QcEklAzCU18GeAHY0z/9I+XssPHT3IhOpqXew2jTdd+zFu6GoBDh4+xZNUGvp3wPj9OGouXy8XPwWtTtcyos7/jl+8+ANxeXmTP5sMfFy4CsH3PAZ7p2IsWr73NkJ6v3XVrmpO4XC5CQhZx9Gg4wcHrCAuLAGDy5HEcPryFRx99mM8++8rmlM5jWRaLFv5AyKZFvNrxRbvjOFbhwv6UKl2C0NBwevcexsiRA4mMDGHUyEEMHjzK7niO4HK52LRpIUeObGXFirWEhUXw+uv9mDPnayIjN/HCCy0YN+5zu2M6Wps2zQicMdfuGI6V/DgE6NK1PZvDljJp0jhy5Updb5R/Cm2r2xszZggDB40kPv6/N3G7dGnPwp+Xc+rUGRuTOc/YsUMZOHAE8Q6rAIhnuF2LWkfgKcuyRlmW9V3iaxRQPnGabWLj4tiz/xc+Hd6fSaMGMun72fx67ASbwney+8AvSS1qIeE7OJZ4J/rNoWNp1bkPrw8Yya79B5OeO5uzeOVt11fy8WLM/fIDAj8dyRc/zOFaTEx6v8V7Jj4+ngoVGvDwwxV46qlSFC/+CACdOvWmSJGn2Ls3ktatndFN00mq12xO+QoBNG7Slq5dO1C1SgW7IzlOtmw+BP4wid69h3HxYjSdOr1Enz7vUrRoBfr0fZdJE8faHdER4uPjqVixIUWLVqRcudIUL/4IPXq8SvPmHShatCLTps1i9OjBdsd0rEyZMtGkcT1+nL3A7iiO9NfjcPLkaTz+eBWeKl+fU6eitG8lo211ewENanHmzFkiwncmlfkVyE/zFg35/POv7QvmQA0a1CIq6izhybaVpAMrPm1fDnK7UR/jgYLA4b+UF0icZpv7895HLt8c+GTNgk/WLJR94nH2HTyMZVk0rVudnq++8Le/mfBuHyDlZ9Ty35eHU2fO4pfvPmLj4oi+dJlcid20/vRQYX98smYh8pejlHj04XR7f3Y4f/4Cq1dvpF69GuzevR9IuICcNWsevXp15dtvZ9mc0FlOnDgFwJkzZwkKWsRTT5Vm7boQm1M5h9vtZkbgZAID5xIUtBiAtm1b0evthIF/Zs9ewMTPx9gZ0XESjsEN1K9fkyeeeDypdfvHH+cTFPStzemcKyCgJuHhO4iK+s3uKI5zs+Mw+XaaOnU6c3762qZ0zqJtlTqVKpajUaM61K9fkyxZvMmRIzubNy8jJiaGHTsTejf5+GRl+45VlHyihr1hbVapUjkaN65DQEANvL298fXNwdSpH/HKKz3tjpaxOGwMg7R0uxa1nkCwMWaRMWZy4msxEAy8mf7xUlarcjnCd+4lNi6OK1evsWNvJA/9XyEqlnmCZWs3cfbceQDOX4jmxOnUNcPXqFyWeUtXAbBszSbKly6BMYZjJ6OIjUvYCU6cPsMvR09Q0C9furyvey1v3jzkzOkLQJYs3tSuXZX9+w/dMMJco0Z12bcv0q6IjuTjk5Xs2bMl/Vy3TnV27dpncypnmTRpLHv3HmDCx1OSyk6ePE21ahUBqFnzaSIjf7ErnmPc7Bjcu/cAvr45KFq0CAC1alXVMXgLzz37jLo9puBmx6GfX/6kn5s1DdC5K5G2VeoMHTqGR4pVovjjVWjfrgerV2/Av1ApHiryFMUfr0Lxx6tw+fKVf3wlDWDIkDEULVqRxx6rQrt2PVi1aoMqaXJHbtmiZlnWYmPMIyR0dSyUWHwcCLMsK12rr32Hf0TYtt38cf4itZ/rQrf2bYiNTRhxrE2TejxU2J+ny5Wm5Wu9cblctGhQi2JF/g+AHh2eo3P/fxMfb+F2ezGwR0cK3n/7ilWLBrV4Z9QnNGzXg5w5sjNmYMLBFL5zL18GzsXt9sJlXAx8oyO5Ey+sPJ2fX36++OIDvLy8cLlczJ69gEWLglmxYjY5cmTHGMOOHbvp0WOg3VEd5f778/HjrIQhid1uLwID57IksZIvULnyU7R9sRU7duwhNCThzvSQIaPp+no/xo8bhtvt5urVa7zezdZHXR3Bzy8/U6Z8gJeXK9kxuIJu3frzww8TiY+P548/ztO5cx+7ozqSj09W6tSuRtfX+9kdxXFSOg7bPNuMUiVLYFkWhw8fo1t3HYfaVnIvvf56B3r16sL99+cjLGwJixev5HWdw/53DuuumJYcOzz/P4FThuf3FE4bnt/JnDA0uKdwwvD8nsLJw/M7jY5BSS9OGZ7fEzhleH5P4NHD8+8KTtvh+UvUdsy20CeJiIiIiIiIw9xuMBERERERERFnysBdH1VRExERERERz5SBv6NOXR9FREREREQcRi1qIiIiIiLikdJ5IHpbqaImIiIiIiKeKQM/o6aujyIiIiIiIg6jFjUREREREfFMGkxERERERERE7hW1qImIiIiIiGfKwM+oqaImIiIiIiKeKT7jjvqoro8iIiIiIiIOoxY1ERERERHxTOr6KCIiIiIi4jAa9VFERERERETulXRpUctRrHF6LDbDubgvyO4IHiVbsSZ2R/AYbpeX3RE8hsHYHcFjXLc7gAexLMvuCB4js1cmuyN4lOvxsXZH8Bg6Dv8h1PVRRERERETEYdT1UURERERERO4VtaiJiIiIiIhnysAtaqqoiYiIiIiIR7IsfeG1iIiIiIiI3CNqURMREREREc+kro8iIiIiIiIOk4GH51fXRxERERERkRQYY6YaY6KMMTtvMu1tY4xljMmb+LsxxnxsjIk0xmw3xpRJNm97Y8yBxFf7261XFTUREREREfFM8fFp+7q5r4GAvxYaYx4A6gFHkhU3AIolvjoBnyfOmwcYClQAygNDjTG5b/XWVFETERERERHPZMWn7etmq7CsNcDvN5n0IdAXsJKVNQO+tRJsAnIZYwoA9YFllmX9blnWOWAZN6n8JaeKmoiIiIiIyB0wxjQDjluWte0vkwoBR5P9fiyxLKXyFGkwERERERER8Uw2jPpojPEBBpDQ7THdqEVNREREREQ80z3o+ngTDwNFgG3GmF8Bf2CrMcYPOA48kGxe/8SylMpTpIqaiIiIiIhIKlmWtcOyrPyWZT1oWdaDJHRjLGNZ1ilgHtAucfTHisB5y7JOAkuAesaY3ImDiNRLLEtRhqyo+fsXYMmSGUSEBxO+dTndu70CQMmSxVmzOojQkMVsWP8z5cqVtjlp2hg8fiLVW3em+Wt9UpwnbNtuWnXpzzOv9abD2+/e9TpjYq7Te/gEGnboyQs9BnH81BkAduyNpFWX/rTq0p+WXfoRvC7srtflFFMmj+f4sW2EhwcnlY0aOYgdO1azdcsyZs36gpw5fW1M6Dwul4sNG3/mx9lfAjBp0jh27V7Lxk0L2bhpISVLFrc5oTPs2rOWkNBFbNj0M2vWBQEweEgvNoUklAXN+xa/AvltTukMUyaP58SxbUQkOw5btmzMtogVxFw9StkyJW1M5yyTJ43j2NEIwrcuTyor+cTjrFkdxNYty5nz01fkyJHdxoTOouMwdby9vVm3dj5hoUsI37qcwYN7AfDggw+wds08du9ay3fTPiNTpkw2J3UGHYf3wD0Y9dEY8wOwEXjUGHPMGNPxFokWAoeASGAK8DqAZVm/A+8DYYmv9xLLUmQsy7rV9P+Jd5YH0n6hd8DPLz9+fvmJiNhJ9uzZ2LRxIa1av8r4ccP4+OMpLFm6ioD6Nen1dlfq1WtjW86L+4LSZDmbt+/BJ2sWBo75jDlTxv5t+oXoS7zUcygTR/SnQP68nD13nvty50zVso+fOsOgcZ/z1bghN5QHzlvK/l+OMOTNV1m0cgPBG8IYN/BNrly9RqZMbtxeXpw5e45WXfoTHPgZbi+vu36f2Yo1uetl3I0qVSpwKfoSU7+awJNP1gagTp1qrFy5nri4OEaMGADAgAEj7IwJQGa3Mz4ge/ToSJkyJcnhm51WLTsyadI4Fi0KZu7cRXZHS2Iwdkdg1561VKvSlLNnzyWV5ciRnYsXowHo2rUDjz1elDffGGRXRACuxsbYun6AqlUqEB19ia++mkDpxOPwsceKEh9v8fmno+jb7322bN1uc0pwGfv3qyp/bqupH/FkmToAbFi/gH79/83atZto3/5Zijz4AMPeHWdrzsxezjhfecpxeD0+1tb1A2TL5sOlS5dxu92sXPETb/ceyptvvMbcoMXMmjWPT/4zgu3b9zB5yjRbc6bHNe6d8pTjMObaMftPWv+jKz9/lKb/0Vkb9XTMtsiQLWqnTkUREZHwfXTR0ZfYuzeSQoX8sCyLHL45APDN6cvJk6ftjJlmypV8nJy3uBuzcMV6aj/9FAXy5wW4oZI2f/lanu8xiFZd+vPuR18QF5e6vrkrN26had1qANStVoGQ8J1YlkXWLN5JlbJrMddxwDVwmlm3LoTfz/1xQ9ny5WuIi4sDICRkK/6FCtgRzZEKFvIjIKAWX38daHcUj/TnxSGAT7asjrjgcIK1NzkO9+6NZP/+gzYlcq5160I495dtVazYQ6xduwmA4OA1NG/e0I5oHkPH4c1dunQZgEyZ3GTK5MayLGrUeJqffvoZgGnf/UjTpvXtjOgYOg7lbvzPFTVjzMtpGSS9FC7sT6nSJQgNDad372GMHDmQyMgQRo0cxODBo+yOd08cPn6SC9GXeLn3e7R5fQDzlq0B4NCR4yxZvYlvPxzGjxNH4eUy/LxiXaqWGfXb7/jluw8At5cX2bP58MeFiwBs3xPJM6/1pkXnvgx549U0aU3zBB06PMfiJSvtjuEYY8YMYeCgkcTH33hhM3RYb0JCFjF69GAyZ85sUzpnsSyLoPnfsnb9PF5+5fmk8qHDerN3/3qefbYZ/37/QxsTSkaxe/f+pAvoli0b4+9f0OZEzqHjMPVcLhehIYs5djSC4OC1HDp0mPPnLyTduDx+/CQFC/rZnNK5dBymMXsGE7kn7qZF7e4fdEpn2bL5EPjDJHr3HsbFi9F06vQSffq8S9GiFejT910mTfx7N8GMKDYunj0HfuHT9/syaWR/Jn0/h1+PnWRT+E52HzjE890TWtRCInZx7GQUAG8OG0+rLv15fdBodu0/lPTc2Zwlq267vpKPF2XulHEEfjKcL2YEcS3G/u5S6a1//zeIjY1l+vSf7I7iCAENanHmzFkiwnfeUD506GieLF2bqlWbkTt3Lnq93cWmhM5St05rqlRuQotnXqZTp5d4+unyALw7bByPPfI0M2YE0blLO5tTSkbQqfPbdO7cjk0bF5Ije3ZiYq7bHckxdBymXnx8POUrBPDQw+Up91RpHn20qN2RPIqOQ0mtW36PmjEmpY7+Brg/7eOkHbfbzYzAyQQGziUoaDEAbdu2otfbQwGYPXsBEz8fY2fEe+b+vHnI5Zsdn6xZ8MmahbJPPMa+Q4exLIumdavRs+Pzf/ubCcPeBlJ+Ri1/3jycOnMWv3z3ERsXR/Sly+RK7Fb6p4f+rxA+WbyJ/PUoJR55OP3eoM3avdSGRg3rUK++fc87Ok2liuVo1KgO9evXJEsWb3LkyM6XX35Ix45vARATE8O0abN4s+drNid1hpMnErphnzlzlvnzl1C2XCnWrw9Nmj4jMIif5kxl+L8/siuiZBD79h2kUaMXAShWrAgNGtS2OZFz6Di8c+fPX2D16g1UrFCGnDl98fLyIi4ujkKFCnDixCm74zmWjsM0ZsP3qN0rt2tRux9oBzS5yets+ka7O5MmjWXv3gNM+HhKUtnJk6epVq0iADVrPk1k5C92xbunalUuR/jOfcTGxXHl6jV27I3koQcKUfHJf7FsbShnz50H4PyFaE6cPpOqZdaoVDapC+WyNSGUL10CYwzHTkYRm9j14cTpM/xy9AQF78+XPm/MAerVq8HbvbvSvEUHrly5anccxxg6dAyPFKtE8cer0L5dD1av3kDHjm/h5/fffaFJk3rs3rXfxpTO4OOTlezZsyX9XKt2VXbv3sfDDz+YNE/jxnXZv/+QTQklI8mX2GXdGMM7/d+0fbAHp9BxmHp58+ZJGuE4S5Ys1K5djb17I1m9egMtWjQC4KW2rZg/f6mdMR1Nx2Eay8BdH2/ZogYsALJblhXx1wnGmFXpkigNVK78FG1fbMWOHXsIDUloTRsyZDRdX+/H+HHDcLvdXL16jde79bc5adroO+Jjwrbv4Y/zF6n9Qje6vdSK2LiEUaHaNK7LQ/9XiKfLlaJl5364jKFFg5oUK5LwfXs9OrSh8zsjibficXu5Gdjj5VRVrFoE1OCd0Z/RsENPcubIzpgBPQAI37WPL4cE4fZy43IZBvZ4hdwZZMj6adM+pXq1SuTNm4dfDm3mvffG0bdvd7y9vVm8KGHAjJCQrXTrnjH2q/QwdeoE8ubNgzGG7dt388YbA+2OZLv8+fPyQ+AkANxuL2bOnMfyZWv4fvpnFCv2EPHxFkeOHudNbSsAvkt2HP56aDPvvjeO38/9wYQP/02+fHmYF/Qt27btomHjF+2Oartp335CtcRtdehgGO+9P57s2bPRtUt7AObOXcQ338ywOaUz6DhMPT+//Hz5xYd4eXnhcrn4cfZ8Fi4KZs/eA0z79lPeHdaHiIidfKWBpAAdh3J3MuTw/J4irYbn/6ewe3h+T+KU4fk9gROG5/cUThie31M4YXh+T+GU4fk9hROG5/cUGqUz9Tx6eP45o9J2eP7m/R2zLW7XoiYiIiIiIuJMDuuumJYy5PeoiYiIiIiIeDK1qImIiIiIiGfKwKM+qqImIiIiIiKeKQNX1NT1UURERERExGHUoiYiIiIiIp4pA4/uqYqaiIiIiIh4JnV9FBERERERkXtFLWoiIiIiIuKZMnCLmipqIiIiIiLimfSF1yIiIiIiInKvqEVNREREREQ8k7o+ioiIiIiIOEwGHp5fXR9FREREREQcRi1qIiIiIiLimdT18c5YGbgJMi3leLSZ3RE8ysV1H9kdwWPkr9nP7ggeIyYu1u4IHsNljN0RPIY+B1Mva6bMdkfwKHExGfeiNM3plPXPkIEraur6KCIiIiIi4jDq+igiIiIiIp4pA3+PmipqIiIiIiLikaz4jNvVXF0fRUREREREHEYtaiIiIiIi4pky8GAiqqiJiIiIiIhnysDPqKnro4iIiIiIiMOoRU1ERERERDyTBhMRERERERGRe0UtaiIiIiIi4pk0mIiIiIiIiIjDZOCKmro+ioiIiIiIOIxa1ERERERExDNZGXcwEVXURERERETEM6nro2fx9y/A0iUz2RaxgojwYLp37wjAyJGD2LF9FVs2L2PWzC/ImdPX5qT28/cvwJIlM4gIDyZ863K6d3sFgJIli7NmdRChIYvZsP5nypUrbXPStDFkyk/UeH0kLfp/fNPpYXsO8XSn92kz8BPaDPyEiXNW3PU6Y67H0ueTQBq//QEvDp3I8TPnANhx8FjSeloP+ITgzbvvel1OsmP3GjaGLmLdxgWsWhuUVN65Szs2b11GSNhi3vt3PxsTOoO3tzdr184jNHQxW7cuZ/DgXjdMHz/+XX77bY9N6ZwnpfN77ty5WLhwOrt2rWXhwunkypXT5qT2mzJ5PMePbSM8PDipbNTIQezYsZqtW5Yxa5Y+B5Pr/Hp71m5awJqN85n05Xi8vTNTpVpFgtf8xJqN8/nk81F4eXnZHdN2CeesIEJCFrFlyzIGDXoLgOrVK7Nhw89s3ryUKVPGa1sl43K52LhxIbNnTwXgq68msG3bCjZvXsrEiWNxu9VuIjeXIStqsbFx9O33HqVK16JK1aZ07dKexx8rRnDwGko/WZuy5epy4MAh+vXtbndU28XGxtGv3/uUfrI2Vas1o0uX9jz2WDFGjhjI8OEfUr5CAO+9N44RIwbYHTVNNKv6JJ/3bX/LeZ589EFmDu/OzOHd6dK8VqqXffzMOToO/+Jv5XNWb8E3W1YWjO9F24DKfDRjCQBF/fMz/b2uzBzenc/6tuf9qUHExsXd2RtyuEYNXqBKpcbUqNoMgKrVKtKwcV0qV2xEhacC+HjC37fXP821a9cICHiO8uUDKF8+gLp1q1O+/JMAlClTkty5VeFILqXze98+3Vi5Yj0lSlRl5Yr19O3Tze6otvvm25k0bvziDWXLg9dQunQtypRN/Bzsp89BAL8C+XmtSzvq1mhJtUpN8PLyomXrJnzy+Shee7kX1So14ejREzz3QnO7o9ou4Zz1PBUqNKBChQbUq1edihXL8sUX42nXrjvlytXjyJHjtG3byu6ojtG9+yvs2xeZ9Htg4FxKlapFuXL1yJrVm5dffs7GdBlAvJW2Lwe5bUXNGPOYMaa2MSb7X8oD0i/W3Tl1KoqIiJ0AREdfYu/eAxQs5Mfy5WuIS7wQDgnZSqFCBeyM6Qh/31aRFCrkh2VZ5PDNAYBvTl9OnjxtZ8w0U/axIvhmy/o//e2C9RG8MPRz2gz8hPemziUulU3tK7fuoWmVhAvvuuVLELrrEJZlkdU7M+7EO47XYq5jzP8Uy6N0fPVFPhw/kZiYGAB+O3PW5kTOcOnSZQAyZXKTKZMby7JwuVyMHDmAAQNG2JzOWVI6vzdpUo9p380CYNp3s2jatL6dMR1h3boQfj/3xw1lf/0c9NfnYBK3lxdZsmbBy46AXJIAACAASURBVMuLrFmzcPnyZWKuX+fQwV8BWL1yPY2b1rM3pEMkP2e53ZmIi4sjJuY6kZG/ALBixVqeeaaBnREdo1AhPwICavHVV4FJZUuWrEz6efPmbboevVtWfNq+HOSWFTVjzBtAENAD2GmMaZZsskdcPRQu7E+pUv8iNDT8hvIOHZ694UCRxG1VugShoeH07j2MkSMHEhkZwqiRgxg8eJTd8e6Z7ZFHaD3gE14f+w2RxxIqqIeOR7Fk0w6+GdyJmcO74+VysXDDtlQtL+r3C/jdl9Aq4vbyIruPN39EX05c11Ga9/+YVgM+YdDLzZIqbhmBZVnMnfcNq9cF0SHxbmHRYkWoXPkpVqz6iYWLf6BMmZI2p3QGl8tFSMgijh4NJzh4HWFhEXTt2oEFC5Zx6lSU3fEcK/n5PX/+vEnb6tSpKPLnz2tzOufr0OE5FutzEIBTJ6P47D9Tidi5kp3713HhQjRzf1qE28uLUk/+C4AmzQIoWMjP5qTO4HK52LRpIUeObGXFirWEhUXgdntRpswTADRv3hB/f1U+AMaOHcrAgSOIv8nNXbfbzfPPt2DZslX3PpjcEWPMVGNMlDFmZ7KyscaYvcaY7caYOcaYXMmmvWOMiTTG7DPG1E9WHpBYFmmM6X+79d6uU+xrQFnLsqKNMQ8CPxpjHrQsawLg+Pv/2bL5MCNwMr17D+Pixeik8v79ehAbG8f0H36yMZ2zZMvmQ+APk5K2VadOL9Gnz7vMnbuIli0bM2niWBo0fMHumOnu8QcLsvjD3vhk8WZtxD7e+mg688e9RcjuQ+z59QQvDv0cgKsxseTxzQZAz4++58SZc1yPjePk2fO0GfgJAC/Ur8Qz1crecn0liz7AnFFvcOh4FIMmz6ZKyWJ4Z86Uvm/yHqlfpw0nT54mb777CJr/Lfv3H8Tt9iJ37pzUqtGCsmVL8vW0/1CyRHW7o9ouPj6eChUakDOnLzNnTqZKlfK0bNmIunXb2B3NsVI6v//JysCjgKWF/v3fIDY2lunT9TkIkDOXLwGNalO2ZG3On7/Il99MoFWbpnR6pRf/HvEOmb0zs2rFeuLjnHW33S7x8fFUrNiQnDl9mTFjMsWLP0K7dj0YM2YI3t6ZWb58bVLL7T9Zgwa1iIo6S3j4TqpWrfi36RMm/Jv160NYvz7MhnQZyL3prvg18AnwbbKyZcA7lmXFGmNGA+8A/YwxxYHngBJAQWC5MeaRxL/5FKgLHAPCjDHzLMtKcZCC21XUXJZlRQNYlvWrMaYGCZW1wji8ouZ2u5kxYzI/BM5hbtCipPKXXmpNw4Z1qB/wrI3pnMXtdjMjcDKBgXMJCloMQNu2rej19lAAZs9ewMTPx9gZ8Z7JnjVL0s9VSz/KiG/mc+7iJSzLokmVJ3nz2b93e/moZ8IzIMfPnGPI5Nl8OfDVG6bnz+PLqbPnuT9PTmLj4oi+fI1c2X1umOehQvnx8c5M5LEoSjxUKB3e2b33Z3fZ386cZcG8pZQtV4oTx08xb17CM3pbtmzHio/nvrx5OPvb73ZGdYzz5y+wevVGqlevzEMPFWb37jUA+PhkZdeuNZQoUc3mhM5ws/N7VNRv+Pnl59SpKPz88nNG3WpT1O6lNjRqWId69XUj4E/Va1TmyOFjnD2bMNjTz/OX8lSFJ/lx5jyaNEg4x9eo9TQPF33QxpTOk3DO2kC9ejX46KPJ1KnTGoDatatSrFgRm9PZr1KlcjRuXIeAgBp4e3vj65uDqVM/4pVXejJgwJvky5eHZ599x+6YHs+6B6M+Wpa1JrHRKnnZ0mS/bgL+fDCzGRBoWdY14BdjTCRQPnFapGVZhwCMMYGJ86ZYUbvdM2qnjTFJw/0lVtoaA3mBJ27zt7aaPGkce/dGMmHClKSyevVq0PvtrrRo+TJXrly1MZ2zTJo0lr17DzDh4/9uq5MnT1OtWsLdn5o1n07qd57R/fbHxaQ78TsOHiPessiV3YcKJR5medguzp5PuHN/PvoyJ347l6pl1njyMeatS+h6uyx0F+WLP4QxhmNRvycNHnLit3P8evI3CubLdatFeQwfn6xkz54t6edatauwZ/d+FsxflrRfFS1ahEyZM/3jK2l58+ZJGnkvSxZvateuytatO3jwwXI8+ujTPPro01y+fEWVtGRudn6fv2AZL7VNuEh8qW1r5s9fmtKf/6PVq1eDt3t3pXmLDvocTObY0ROULVeKrIk366pVr8SBfQfJmzcPAJkzZ6JHz9f4emrgrRbzj3Czc9a+fZHky3cfAJkzZ+btt7syZcr3dsZ0hCFDxlC0aEUee6wK7dr1YNWqDbzySk86dHiOunWr065dD7X+ZxyvAH+2DBUCjiabdiyxLKXyFN2uRa0dEJu8wLKsWKCdMWbS7TPbo3Llp2jbthU7duwhLDTh7v3gIaP54IP38M6cmUULfwAgJHQr3bv/s+9kVK78FG1fTNhWoSEJrWlDhoym6+v9GD9uGG63m6tXr/F6t9t2o/UI/T6dweY9v/BH9GXqvjGGri1qEZvYlaVN7fIsC9vFzOBQ3C4X3pndjH79WYwxPFwoP91a1aHrmK+JtyzcXl4MaN+Egnlz33adzauXZeDEH2n89gf4Zs/KmG4Jrbnh+w8zdcFaMnm5MMYwoH0TcufIlq7v/17Jnz8v3wdOBBKey5s1cx7Ll60hU6ZMfDZxNJvCFhETc50unfrYnNR+fn75+eKLD/Dy8sLlcjF79gIWLQq+/R/+Q6V0fh879hOmT59Ih5ef48iRY7zwQlebk9pv2rRPqV6tEnnz5uGXQ5t5771x9O3bHW9vbxYvSqhwhIRspVv3jHF+vxtbt2xnftASgtfMITY2lh3b9/Dt1zN4Z/Bb1KtfA5fLxddf/sC6NZvsjmo7P7/8TJnyAV5ermTnrBWMGDGABg1q43IZpkz5jtWrN9gd1bH+85/hHDlynFWr5gAQFLSYkSNv/rVBkgo2j9RojBlIQn0pze9OmPSoyWf29tftgVQw/4Rh/tLQ+TUf2B3BY+Svqe8nS62YuNjbzyQAxDtsNCwn013y1MuVNfvtZ5Ik0TFqCZW0d+XKYY+9KL3077ZpesLNNui7m26LxK6PCyzL+leysg5AZ6C2ZVmXE8veAbAsa2Ti70uAYYl/MsyyrPo3m+9mMuT3qImIiIiIiKSXxK8q6ws0/bOSlmge8JwxxtsYUwQoBoQCYUAxY0wRY0xmEgYcmXerdeir0EVERERExDPdg66PxpgfgBpAXmPMMWAoCaM8egPLEnvJbbIsq4tlWbuMMTNJGCQkFuhmWVZc4nK6A0sAL2CqZVm7brVeVdRERERERMQz3ZtRH5+/SfGXt5h/ODD8JuULgYWpXa+6PoqIiIiIiDiMWtRERERERMQz2TzqY3pSRU1ERERERDxTBh6RWF0fRUREREREHEYtaiIiIiIi4pnU9VFERERERMRZrHsw6qNd1PVRRERERETEYdSiJiIiIiIinikDd31Ui5qIiIiIiIjDqEVNREREREQ8UwZuUVNFTUREREREPJO+R01ERERERETuFbWo2chlVE++E75V37I7gsc4P6un3RE8Rq42E+yO4DHyZ8tldwSPcTr6nN0RPEZ0zFW7I3gUL107pNr1+Fi7I8i9oK6PIiIiIiIizmJl4IqabsuIiIiIiIg4jFrURERERETEM2XgFjVV1ERERERExDPFa9RHERERERERuUfUoiYiIiIiIp5JXR9FREREREQcJgNX1NT1UURERERExGHUoiYiIiIiIh7JsjJui5oqaiIiIiIi4pnU9VFERERERETuFbWoiYiIiIiIZ8rALWqqqImIiIiIiEeyMnBFTV0fRUREREREHCZDVtT8/QuwdMlMtkWsICI8mO7dOwLQskUjIsKDuXrlCGXKlLQ5pTN4e3uzdm0QISGL2LJlGYMGvQVA9eqV2bDhZzZvXsqUKePx8vKyOan9UtqvcufOxcKF09m1ay0LF04nV66cNidNG0NnrqbmsGm0HPfjLefbefQMZft9wbLth+56necvX6Xz5IU0GT2DzpMXcuHyNQBW7vyV1uNn0+aD2bwwYQ7hv5y663U5hb9/AZYsmUFEeDDhW5fTvdsrAHw37TNCQxYTGrKYffs2EBqy2OakaWfcf94nYt9qlq+fc9PpzVs1Ytnan1i+7ifmLv6Ox0s8etfrzJw5E599OY51mxcyf9l0/B8oCEDVGpVYuGIGy9f9xMIVM6hctfxdr8sppkwez/Fj2wgPD04qGzVyEDt2rGbrlmXMmvUFOXP62pjQeVwuFxs3LmT27KkAFC78AGvWzGXnztVMm/YJmTJlsjmhM+zas5aQ0EVs2PQza9YFAdC8eUPCNi/hQvRBnizzhM0JnSGl8/ugQW9x6GBY0jk+oH5Nm5N6uHgrbV8OkiErarGxcfTt9x6lSteiStWmdO3SnscfK8au3fto8+xrrF0bYndEx7h27RoBAc9ToUIDKlRoQL161alYsSxffDGedu26U65cPY4cOU7btq3sjmq7lParvn26sXLFekqUqMrKFevp26eb3VHTRNNyj/DZqw1uOU9cfDwTfg6h4iP+d7TssIMnGBy46m/lU1dso0LRgszv9ywVihZk6soIACoUK8TMXi2Y2aslw1pX491Za+5ofU4WGxtHv37vU/rJ2lSt1owuXdrz2GPFaPvS65SvEED5CgHMnbOIuUGL7I6aZmZNn0vb1l1SnH7kyHFaNe5AnSotmDBuImM+GprqZfs/UJBZ8776W/lzbVtw/o8LVCnXkCmfT2PAsF4A/H72HC+/0J06VVrwVreBfPz5yDt/Qw71zbczadz4xRvKlgevoXTpWpQpW5cDBw7Rr193m9I5U/fur7BvX2TS78OH9+c///mSf/2rOufOnadDh2dtTOcsDRu8QOWKjahWpRkAu3fv44Xnu7J+XajNyZwjpfM7wH/+80XSOX7xkpU2J/Vw8Wn8cpDbVtSMMeWNMU8l/lzcGNPLGNMw/aP9706diiIiYicA0dGX2Lv3AAUL+bF3byT799/9Xf+M5tKlywBkyuTG7c5EXFwcMTHXiYz8BYAVK9byzDO3vmD/J0hpv2rSpB7TvpsFwLTvZtG0aX07Y6aZsg8VwNfH+5bz/LB+F7WfKEKebFluKP961TZemDCH1uNn89mSLale56rdh2lS7hEAmpR7hJW7DgPg450JYwwAV2Jik37OCP6+X0VSqJDfDfO0bNWYmTOC7IiXLkI2buGPc+dTnL4lNILz5y8AsDVsOwUK3J80rUXrxixY9gNLVv/IqA+G4HKl7n5jvYa1mBWYsA1/DlpKlWoVANi1Yy+nT50BYN+eSLJkzULmzBmj1WTduhB+P/fHDWXLl68hLi4OgJCQrfgXKmBHNEcqVMiPgIBafPVVYFJZ9eqV+emnhQB8//1smjSpZ1c8x9u37yAHDugaK7nUnN9FbuWWn3DGmKHAx8DnxpiRwCdANqC/MWbgPch31woX9qdUqX8RGhpudxTHcrlcbNq0kCNHtrJixVrCwiJwu70ok9h1oXnzhvj768M8ueT7Vf78eTl1KgpIOCnnz5/X5nT3xunzl1i581faVCp+Q/mGfcc48tsFvn/jGWa81YI9x8+w5dDJVC3z7MUr5PP1ASBvjqycvXgladqKHb/wzJiZ9Ji6hGGtq6XdG3GQwoX9KVW6xA3nqypVKhB1+jciD/5qXzAbPfdSC1YGrwOg6CMP0aR5AM80eIn61VsRFxdP89aNU7UcvwL5OXk8octsXFwcFy5EkztPrhvmadS0Lju27SYm5nravgmH6tDhOd3JT2bs2KEMHDiC+PiEW+r33Zeb8+cvJFVsjx8/ScGCusiGhC8YDpr/LWvXz+PlV563O45H+Ov5vUvX9mwOW8qkSeMyzCMTdrHirTR9OcntRn1sBZQGvIFTgL9lWReMMeOAEGB4Oue7K9my+TAjcDK9ew/j4sVou+M4Vnx8PBUrNiRnTl9mzJhM8eKP0K5dD8aMGYK3d2aWL1+b9EElt9+vLMtZB3l6GTtvI282LI/LdWPr1qb9x9i4/xjPfvgTkNACduS385R9qABtP55LTGwcV2JiOX/5Gm0+mA1Az0blqfzoAzcsxxhD8oazWk8UodYTRdhy6CSfLdnMpM6N0vcN3mPZsvkQ+MOkv+1Xz7ZpxsyZGac17U5UrvIUz7VtQfMGLwFQpVoFnihVnJ+DE1o8smTx5uxvvwPwxbcTeKBwITJlzkShQgVYsjrh2covJ33HzOlzb7uuRx57mHeG9uLFlp3S6d04S//+bxAbG8v06T/ZHcURGjSoRVTUWcLDd1K1akW74zhe3TqtOXniNPny3ce8+dPYv+8g69ery2NK/np+nzx5GiNGTMCyLIYN68Po0YPp3Lm33TE9l8MqV2npdhW1WMuy4oDLxpiDlmVdALAs64oxxmG9OG/kdruZMWMyPwTOyVDPdqSn8+cvsHr1BurVq8FHH02mTp3WANSuXZVixYrYnM4ZbrZfRUX9hp9ffk6disLPLz9nzpy1OeW9sfvoGfp9vwKAPy5dZd3eo3i5XFhAx5qlaVXp8b/9zXdvPAMkPKM2L2w/7z9X44bp9+XIypkLl8nn68OZC5fJkz3r35ZR9qECHPv9IucuXSX3X7pceiq3282MwMkEBs4lKOi/g4Z4eXnRrFkAlSo7urd5uni8+COMmfAeL7XpktRN0hjDj4HzGPX+R3+b/9V2bwIJz6h9+OlwWjd9+Ybpp05GUaCQHydPnMbLywtf3+yc+z2hW2CBgvfzxbcT6Pn6AA7/ejSd35n92r3UhkYN61Cvfhu7ozhGpUrlaNy4DgEBNfD29sbXNwfjxg0jZ05fvLy8iIuLo1ChApw4kXEGMrobJ0+cBuDMmbPMn7+EsuVKqaKWgpud36OifkuaPnXqdOb89LVN6cTpbte5P8YY45P4c9k/C40xOXHc43Y3mjxpHHv3RjJhwhS7ozha3rx5kkb9ypLFm9q1q7JvXyT58t0HQObMmXn77a5MmfK9nTEd42b71fwFy3ipbUKl9qW2rZk/f6ld8e6phQOeZ1Hiq84TRRjQ4mlq/etBKj3iz9ywfVy+ltB97PT5S/wefeU2S0tQvXhh5m/eD8D8zfupUbwwAEd+O5/UUrnn2G/ExMaR6zbPz3mSSZPGsnfvASZ8fOP5qnatquzbf5Djx/9ZF4cFC/kx5duPeLPrO/xy8HBS+bo1m2jUtC735c0DQK5cvhRKZbfsZYtW0vq5hEEPGjWrx/rEQaV8fXPwTeBnjHzvIzaHZPwu8vXq1eDt3l1p3qIDV65ctTuOYwwZMoaiRSvy2GNVaNeuB6tWbeDll99kzZqNtGiRcKPkxRdbsmDBMpuT2s/HJyvZs2dL+rlW7ars3r3P5lTOdbPzu59f/qSfmzUNYNcubb+7koEHE7ldi1o1y7KuAViWlTx6JqB9uqW6S5UrP0Xbtq3YsWMPYaFLABg8ZDTemTPz4Yfvky9fHoLmfsO27bto3LitzWnt5eeXnylTPsDLy4XL5WL27AUsWrSCESMG0KBBbVwuw5Qp37F69Qa7o9oupf1q7NhPmD59Ih1efo4jR47xwgtdbU6aNvp/v4LNB0/wx6Wr1Pv3dLrWK0NsXMJpoPVfnktLrvKj/vwS9QftPknorueTORPDn69509axv3qlZin6fhfMnLB9FMyVnTEv1QYgeMcvzN9yALfLRZZMbsa0rZ1hBhSpXPkp2r6YsF/9OQT/kCGjWbxkJa3bNM1Qg4j86ZMpY6j09FPkuS8XYTuXM37UZ7jdCR9H3309k7f6diVXnpyMGDsISBg5rVHtZzmw7xBjRvyH6bMn43K5uH79OoP6Duf4sds/Axn43U9MmDiSdZsX8se587z+ah8AOrz2PA8WeYCefbrQs0/CSJQvtOyU1KXSk02b9inVq1Uib948/HJoM++9N46+fbvj7e3N4kUJ3UdDQrbSrXt/m5M618CBI5k27ROGDu3Ntm27+PrrGXZHsl3+/Hn5IXASAG63FzNnzmP5sjU0aVqPceOHkTdvHmbPnsr27bt5ppljLxXviZTO722ebUapkiWwLIvDh4/pGJQUmfR4niazt3/G7Syahrxc+m6yOxEXr+fkUuv8rJ52R/AYudpMsDuCx8jnowfeU+t09Dm7I3gMt9ft7hlLcl4mQ36zUrq4Hh9rdwSPce3qUY+9+3mudY00rXfknrXKMdtCZ0cREREREfFMDuuumJZ0W0ZERERERMRh1KImIiIiIiIeyWnffZaWVFETERERERHPpK6PIiIiIiIi/zzGmKnGmChjzM5kZXmMMcuMMQcS/82dWG6MMR8bYyKNMduNMWWS/U37xPkPGGNuOyyqKmoiIiIiIuKRrPi0faXgayDgL2X9gWDLsooBwYm/AzQAiiW+OgGfQ0LFDhgKVADKA0P/rNylRBU1ERERERHxTPfgC68ty1oD/PULNpsB3yT+/A3wTLLyb60Em4BcxpgCQH1gmWVZv1uWdQ5Yxt8rfzdQRU1EREREROTO3G9Z1snEn08B9yf+XAg4mmy+Y4llKZWnSIOJiIiIiIiIR7pFd8V7l8GyLGNMmg8/qRY1ERERERHxTPeg62MKTid2aSTx36jE8uPAA8nm808sS6k8RaqoiYiIiIiI3Jl5wJ8jN7YHgpKVt0sc/bEicD6xi+QSoJ4xJnfiICL1EstSpK6PIiIiIiLike5F10djzA9ADSCvMeYYCaM3jgJmGmM6AoeBNomzLwQaApHAZeBlAMuyfjfGvA+EJc73nmVZfx2g5AaqqImIiIiIiEe6FxU1y7KeT2FS7ZvMawHdUljOVGBqaterro8iIiIiIiIOoxY1ERERERHxSE4Y9TG9pEtFzRiTHovNcDK5vOyO4FHi4uPsjuAxfFt9aHcEj3H58HK7I3gMn8J17I7gMVwudVhJrdi4WLsjeJR47VupltADTTI8K+PWO3S0i4iIiIiIOIy6PoqIiIiIiEdS10cRERERERGHseLV9VFERERERETuEbWoiYiIiIiIR1LXRxEREREREYexNOqjiIiIiIiI3CtqURMREREREY+Ukbs+qkVNRERERETEYdSiJiIiIiIiHikjD8+vipqIiIiIiHgky7I7QfpR10cRERERERGHUYuaiIiIiIh4JHV9FBERERERcZiMXFHLkF0f/f0LsGTJDCLCgwnfupzu3V4B4LtpnxEaspjQkMXs27eB0JDFNid1hh2717AxdBHrNi5g1dogAP71xGMsX/EjG0MXMWPWFHLkyG5zSvv5+xdg6ZKZbItYQUR4MN27dwSgZYtGRIQHc/XKEcqUKWlzSueYMnk8x49tIzw8OKns++8/Z3PYUjaHLeXA/k1sDltqY8K0NWj0f6j2THue6fBGivOEhu+gZceeNOvQgw5vDrzrdcbEXOftd8fS4IUuPN+1D8dPngZgx579tOzYk5Yde9KiY0+Wr9101+tyipvtV3/q2bMz12OOc999uW1I5jwpfRY+8cTjrF41ly2bl/HT7Kk6v6P96k7oGuvOTJ40jmNHIwjfujyprOQTj7NmdRBbtyxnzk9f6RiUFBkrHZ7A887ygK2P9fn55cfPLz8RETvJnj0bmzYupFXrV9m790DSPKNHDeb8hQuMGDHBtpzeXplsW3dyO3avoXrVZvx+9lxS2ao1cxk4YATr14XStl1rHizsz7/f/9DGlHA1NsbW9f91vwrZtIhWrTpiYREfH8+nn4ymX//32bp1u605AdLjuL5TVapU4FL0JaZ+NYEnn6z9t+ljRg/h/IULDB/+kQ3p/uvy4eW3nykVNm/bhU/WLAwYMYG5X3/8t+kXLkbTtnt/Jo0ZSoH783H23B/clztXqpZ9/ORpBo76mK8nDL+hPHDuQvYdPMzQt7uyMHgtwes2MX5oH65cvUYmtxu324szZ3+nZce3WPHjVNxur7t6jz6F69zV36eFlPYrf/+CTJo4lkcfLUqFigGcTXY+s4PLZf990JQ+C7/84kP6v/Nv1q7dRPv2z/Lggw/w7rvjbMsZH2//lyB5yn4F9u9bnnKNBc75LIyOvsRXUz/iyTIJ59AN6xfQr/9/j8EiDz7AMBuPQYCYa8c8tlnql1J10/Q/usi2ZY7ZFvZ/kqSDU6eiiIjYCUB09CX27o2kUCG/G+Zp2aoxM2cE2RHPIzxctAjr14UCsDJ4HU2bBdicyH5/368OULCQH3v3RrJ//yGb0znPunUh/H7ujxSnt2rVhBkZ6BgsV6oEOW9xV3Rh8BrqVK1EgfvzAdxQSZu/dBXPdelDy449eXf8Z8TFxaVqnSvWh9IsoCYA9apXJmTLdizLImsW76RK2bWY6+CYj5y7l9J+NW7cMN4ZMNwRF2ZOkdJnYbFiRVib2MoaHLyG5s80sDOmI2i/Sj1dY92ZdetCOPeXfatYsYduPAabN7QjWoZhxZs0fTnJHVfUjDHfpkeQ9FK4sD+lSpcgNDQ8qaxKlQpEnf6NyIO/2hfMQSzLYu68b1i9LogOLz8HwN49+2nUuC4Az7RoSCH/AnZGdJzChf0pVepfN+xXknpVqlQgKuoMkZG/2B3lnvn16AkuREfT4c2BtOnUi6AlKwE4ePgoi1euY9onI5n95Ue4XC4WLF+TqmVGnfkdv3x5AXC7vcie3Yc/zl8EYPvu/TTr0IPmL7/JkF5d77o1zcmaNKnHieMn2b59t91RHCv5Z+Hu3ftp2qQ+AC1bNMbfv6DN6ZxJ+9Xt6Rrrf7N7936aNk08BlvqGJSU3XIwEWPMvL8WATWNMbkALMtqml7B0kK2bD4E/jCJ3r2HcfFidFL5s22aMXOm7vT8qX6dNpw8eZq8+e4jaP637N9/kNe79mPsuKH07d+dRT8Hcz3mut0xHSNbNh9mBE7+234lqffcs88Q+A+72xoXF8/ufQf54oP3uHYthhe79aNU3Md8+gAAIABJREFU8UcI2bKd3fsP8lzn3gBci4khT66cALwxaCTHT57m+v+zd5/hUVQPG8bvk4QEAqGDIPEFFFRsIE06SO+gFAtVUekKghQpCiJIsyBIky69hdCrQBBIKAk9SEBEAqGp9JZk3g9Z8xcBCZBkZuPz89qLyZnZnWfH2dk5e86ciY7m5KmzNGjVCYCmDevwSo3bu5P+3QvPPMmiyd9y+Nff6DVoBGWLF8bHxztp36QN0qRJTY/uHalR8027ozjWP78LW7fuypdf9qdnz/dZsnQ1N3R8v432q3vTOdaDe691F778sj8f9/yAJUv0GXxYluWsVrDEdK9RH/2B/cD3gEVcRa0oMDyJcz00Ly8vZs8ax6xZASxa9L8LWj09PalXrzolS6mZ+S8nXQMQnD1zjiWBqyhStCDffvM99eu2ACBfvrxUc3Wv+q/z8vJi9uxxzJy1kIBFy+2O45Y8PT2pX78GL5X4b3W3eiRbFjJk8MM3TWp806SmSMFnOHj4KBYWdatVpPN7zW57zogBPYG7X6OWPVtmos6cJUf2rERHx3Dp0hUyZvC7ZZkncj+Gb5rUHPrlGM89nS/p3qBNnngiD3ny/B87tq8G4gY6CAleSanStTh16ozN6ex3p+/Cgz8fplbtJgDkz5eXGtX/vdL/X6T96t/pHOvhHDx4mFq1XJ/B/HmpcY8f3uTfWfZf5ppk7tX1sSiwA+gFnLcsaz1w1bKsDZZlbUjqcA9j7NihhIcf4psR428pr1SxLAd/PkxkZJRNyZzF1zcN6dKljZ+uWKkMB/b/TNZsWQAwxvBR9/ZMmDDDzpiOMW7sMMLDI/jmm/H3XljuqFKlshw8GEFk5Em7oySrl8sUJ3TPfqKjY7h67Tp79h/i8f/zp0ThgqzesJlzrmsYzl+4yImo0wl7zVLFWbQirgvlqg2beanw8xhjOH7yFNHRcde5nYg6zS/HjpMrR/akeWM227s3nFz+Bcn/ZAnyP1mC48dPUvylajqZdrnTd2G2vx3fe/R8n/Hf/2BXPMfSfvXvdI71cP7+GezZ4wPGjZ9mcyJxqn9tUbMsKxb4yhgz1/XvqXs9xwlKlSpG0yYN2bPnQPzwsH37DmbFyh9p1LiuLnD9m+zZszJ91hgAvDw9mTsnkDWrN9K2XUvedf3CHxi4kh+mzrUzpiOUKlWMpk3j9qttISsB6NN3MD7e3nz11Wdky5aZRQFT2LV7H7VrN7U5rf2mTRtF+XIlyZo1M78c2U7//sOYNHkWrzWul6IGEfnLR/2Hsy1sL3+ev0Clhq1o99br8ZWl1+pV54ncj1G6eGFebfUBHsaDBrUqk//x3AB0bNWE97p+SqxlkcrLk14ftObRBFSsXq1ZmZ4Dv6bGm23IkN6PoX27ALBzz34mzFiAl6cnHh4e9O7UmkwZ0yfdm09Gd9uv5HZ3+y7Mly8vbdrE9ZgICFjOlCmz7YzpCNqvEk7nWPdn2tSRlHPtW0cOb6P/Z8NJly4tbfUZTDSxKbjr430Nz2+MqQWUtizr439bzu7h+d2FU4bndxd2D8/vTjRCWcIl1vD8/wVOGJ7fXdg9hLo7ccLw/O5E+1bC6bsw4dx5eP6DT9dI1P/RT4Uvd8y2uK/WMcuylgJLkyiLiIiIiIiI4AbdGEVERERERO7Eafc+S0yqqImIiIiIiFtKyT1c1dFZRERERETEYdSiJiIiIiIibkldH0VERERERBwmJQ/Pr66PIiIiIiIiDqMWNRERERERcUtWCm5RU0VNRERERETckkZ9FBERERERkWSjFjUREREREXFLGkxEREREREREko1a1ERERERExC1pMBERERERERGH0WAiIiIiIiIi/0HGmM7GmH3GmL3GmJnGmNTGmLzGmGBjTIQxZrYxxtu1rI/r7wjX/DwPut4kaVGzUnLVNhFdi75hdwS3ov0q4TKn8bM7gtvwzV3Z7ghu4+KGYXZHcBvZK/eyO4LbuGbpu/B++HimsjuC27gec9PuCJIMknowEWNMLuB94BnLsq4aY+YArwM1ga8sy5pljBkDtAJGu/79w7KsfMaY14HBwGsPsm61qImIiIiIiFuyLJOoj7vwAtIYY7wAX+AkUBGY55o/Bajvmq7n+hvX/ErGmAeqTaqiJiIiIiIicgeWZUUCw4BjxFXQzgM7gD8ty4p2LXYcyOWazgX85nputGv5LA+ybg0mIiIiIiIibikZuj5mIq6VLC/wJzAXqJ6kK3VRi5qIiIiIiLglK5Efd1AZ+MWyrDOWZd0EFgClgYyurpAA/kCkazoSeAzANT8DcO5B3psqaiIiIiIiInd2DChhjPF1XWtWCdgP/Ag0dC3TAljkmg50/Y1r/jrrAUfEU9dHERERERFxS0nd9dGyrGBjzDxgJxANhALjgKXALGPMAFfZBNdTJgDTjDERwO/EjRD5QFRRExERERERt/QvIzUm4jqsT4BP/lF8BCh+h2WvAY0SY73q+igiIiIiIuIwalETERERERG3FGt3gCSkipqIiIiIiLgli6Tv+mgXdX0UERERERFxGLWoiYiIiIiIW4p9oIHv3YMqaiIiIiIi4pZi1fVRREREREREkota1ERERERExC1pMBE34++fk1Ur57ArbB1hoWvp0KEVAJkyZWTZshns2xfEsmUzyJgxg81J7Tdu7DCO/xZG6M418WUvPF+AjRsWsXPHGhYumISfXzobEzrH+HHDiTy+i9DQtfFlXwzqzZ49G9i5YzVz535PhgzpbUzoLO+2acaGLYFs2LqY99o2v2Vemw5vcep8OJkzZ7QpnbPcad/69NOP2LljNdu3rWLZ0hnkzPmIjQkTV98JAVToOIRXe4361+X2Homk8Nv9WL1t30Ov8/ylK7QeOpU63UfQeuhULly+CsCPO8Np2Ps7GvcZzRufjmXnz78+9LqcZM/+jWwJWc6mLUtYH7QIgOdfKMDaH+fHlxUp8oLNKe13p+/Cgi88Q9DGQLaFrGTL5qUULVrIxoTOkiGDH1N/GMX2navZtmMVxYu/SKZMGQhYPJXQXesIWDyVjBn1fejvn5OVK2cTFrqW0J1r6ND+bQB69+7MkcPbCAleQUjwCqpXe9nmpO4tNpEfTpIiK2rR0TF0696fgoUqUqZsXdq2aUGBp/PT7aP2/LjuJ559tiw/rvuJbh+1tzuq7aZOm0vtOk1vKRszZii9eg+icJHKBCxaQZcP29iUzlmmTJ1D7dpNbilbs3YjhQpVpHCRKhw6dITu3TvYlM5Zni6Qn6YtGlG9YmMqlq5PleoVyPP4/wHwaK4cVKhYmt+ORdqc0jnutG8NHz6awkWqULRYVZYtW0PvXp1tSpf46pUpxOguTf91mZjYWL6eu5qSzz1xX6+97cAv9Bm/8LbyiUs3UbxAXhYPfp/iBfIyYekmAF56Ji9zP2vLnM/a0q9VPfpNDLyv9bmDWjXepEzJ2lQoWw+Azwb04ItBIyhTsjYDB3xF/wE9bE5ovzt9Fw4c1IsBn39FseLV6Nd/OIMG9rIpnfMMHtqXNas3ULRwFUqVqMXBgxF07tKGDes382LBimxYv5nOXdraHdN20dExdO/+GYVerETZcvVo06YFTz+dH4Bvv/2e4i9Vp/hL1Vmx8kebk4pT3VdFzRhTxhjzoTGmalIFSgxRUacJC9sLwKVLlwkPP8SjuXJQp05Vpv0wF4BpP8ylbt1qdsZ0hE2bgvnjjz9vKcuf/3GCgrYCsHbtRl55paYd0Rxn06Zgfv/HtlqzZiMxMTEABAfvxD9XTjuiOU7+px5n547dXL16jZiYGDZv2katOlUA6D+oJ/37DsVKwaM03a877VsXL16Kn/ZN64uVgjZYkafykD5tmn9dZubqYCoXeYbMfmlvKZ+87Cfe7DeOhr2/47uFCT+5+TH0IHXLxLWI1C1TiB93hgPgm9oHY+K6zVy9fjN+OiWzLCu+p0T69H5ERZ22OZH97vRdaFkW6V3bKUN6P06ePGVHNMdJn96PUqWLM3XKHABu3rzJ+fMXqVWrCjOmzwdgxvT51K5dxc6YjnD7+WgEuXLlsDlVymNhEvXhJP9aUTPGhPxt+l1gJOAHfGKMcYuf4HLn9qdgwecICQkle/as8V9IUVGnyZ49q83pnGn//p/jK7ENGtTG3/9RmxO5h5YtX9evYi7h+w/xUsmiZMqUkTRpUlO5anly5cpJ9ZoViTpxiv17D9od0S3079+dI4e38cYbr/Bpv6F2x0k2p/64wLqd4TSuWPSW8s17Izh26hzT+77LnP5t2H/0BDsOHk3Qa/5+/hLZMvoBkDVDOn4//7+K8NodB6jX41s6fDWdfq3qJdr7cALLsggInMKGTYto+dbrAHTv9hmffd6T/Qc3MWBgTz7tO8TmlM7UteunDBrUm8MRIXzxRR969xlkdyRHyJ3Hn3Nnf2f02CEEbV7Mt6MG4eubhmzZs3Iq6gwAp6LOkE3nWLfIndufgoWeJSQkFIA2bVuwfdsqxo4dpktxHtJ/uetjqr9NvwdUsSyrH1AVaHLnpzhH2rS+zJ41jq5dP73l1+m/pKRfqBPTe6270Lp1c7ZuWYZfunTcuHHT7kiO16PH+0RHRzNjxgK7ozjCoZ+PMPLr8cwOmMDM+ePZu+cA3j7efNClNYMHjrA7ntvo23cwjz9RjJkzF9Ku3Vt2x0k2Q6evoFOjynh43PoVtWXvYbbsPcxrfcfw+idjOXryLL9G/Q5Ak/7jadxnNP0mBbI+7CCN+4ymcZ/R/LQn4rbXN8bA31rOKhUpwKIvOvL1+68zasG6pH1zyaxa5caUK12XBq+8zbutm1GqdDHeeacJPbsP4JmnytCz+wBGjh5sd0xHeu+95nz0UT+eyFecjz76lLFjh9kdyRG8PL0oWOhZJoyfTtlSdbhy5Qofdrn9EgmdY/1P2rS+zJo5Nv58dNy4aRQoUIZixasRFXWawYP72B1RHOpeoz56GGMyEVehM5ZlnQGwLOuyMSY6ydM9BC8vL2bPHsfMWQsJWLQcgNOnz5IjR3aiok6TI0d2zpw5Z3NKZzp48DC1asXVw/Pnz0uNGpVsTuRszZs1plbNylSt1tjuKI4yY9p8ZkyL6wbzcd/OnDl9lhq1KrFuU9yABo/meoTVGxdQvWJjzpw+a2dUx5s5cwGBgdPo33+43VGSxb6jJ+g+eh4Af1y6QtDuQ3h6eGBZ8HbtsjR6uehtz5ne910g7hq1wE1hfPbuK7fMz5whHWf+vEi2jH6c+fMimdOnve01ijyVh+NnAvjj4mUy+d0+3x391V3v7JlzLAlcRZGiBXmjSQO6fdQfgIULlvHtKLUU3Umzpg358MO+AMybv4QxY/47rdr/JvLESSIjo9i+fRcAAQtX8GGXNpw5fZZHcmTjVNQZHsmRjbM6xwJc56OzxjFrVgCLFq0A4s5H/zJx4gwWLphsU7qUwWmtYInpXi1qGYAdwHYgszEmJ4AxJh04rBPnP4wbO4zw8Ai++WZ8fNniJatp1rQRAM2aNmLx4lV2xXO0bNmyAHG/Ovfs8QHjxk+zOZFzVa1agS5d2/LKqy25evWa3XEcJWvWzADk8s9JzTpVmD0zgGfzlabYC5Uo9kIlTkSeokq5V1VJu4t8+fLGT9etU42DBw/bmCZ5LR/WieXDO7N8eGeqFH2GXs1rUbFIAUo9/wQBQaFcuXYdiOsiee7C7b0l7qRCoacI3BQGQOCmMF5+8SkAjp06F//L/4GjJ7hxM4aM6XyT4F0lP1/fNKRLlzZ+umKlMhzY/zNRJ09RpuxLAJSvUIrDh4/amNK5Tp48RblyJQF4+eXSRET8YnMiZzh96iyRx0+SL3/cMapChVKEhx9i2bI1vNmkAQBvNmnA0qWr7YzpGGPHDiU8/BDfjPjf+WiOHNnjp+vVrc6+fbocQO7sX1vULMvKc5dZscArd5lnu1KlitG0aUP27DnAtpCVAPTpO5ihQ0cyY8YYWr71OseOHefNNzUi0bSpIylXriRZs2bmyOFt9P9sOOnSpaVtmxYABAQsZ8qU2TandIZp00ZR3rWtfjmynf79h9GtWwd8fHxYsXwWEDegSPsObnH5ZpKbMG0EmTJnJPpmND279ufC+Yt2R3KsO+1b1WtU5Mknn8CKjeXXY5G0b59y9qvuo+exPfwof166QpXOw2lb/2WiXYPyNK5Y7K7PK/VcPn45cZZmAyYA4OvjzcDWr5IlAaOAv127DB+NmktAUCg5s2RgaLu4H+3WbD/A4p92kcrTAx/vVAxp1zDFDCiSPXtWps8aA4CXpydz5wSyZvVGLl36mMFD++Dl5cX1a9f5oINGM7zTd2Gbtt34cng/vLy8uHbtOm3bdbc7pmN81PVTvp/4Nd7eqTj6yzHatemGh4cHk6eNpHnzxhz7LZKWzTQKcqlSxWjaJO58NCQ4rjWtb9/BNH6tHgVfeBbLsvj11+M6b3hIThsAJDGZpOhD7O3jr47JkujU3z3hMqfxszuC2/j9qiqQCXVxg67RSajslVX5Sahr0TfsjuBWUnt52x3BbVyP0TX2CXX92m9uW9tZnOONRD1BrBM10zHbIkXeR01ERERERMSd3WswEREREREREUeKTcFdH1VRExERERERt5SSL4xR10cRERERERGHUYuaiIiIiIi4pZR8HzVV1ERERERExC3FppBbqtyJuj6KiIiIiIg4jFrURERERETELaXkwURUURMREREREbeUkq9RU9dHERERERERh1GLmoiIiIiIuKXYlDuWiCpqIiIiIiLinmJJuTU1dX0UERERERFxGLWoiYiIiIiIW9Koj/cplafqfwkRHRtjdwS3Emul5I9i4vrz+mW7I7gNLx2vEixnlT52R3AbZ4LH2h3BbaQr3NLuCG7Fy8PT7ghu4+rN63ZHkGSQkq9RU9dHERERERERh9FPySIiIiIi4pZS8n3UVFETERERERG3lJIvjFHXRxEREREREYdRi5qIiIiIiLillDyYiCpqIiIiIiLillLyNWrq+igiIiIiIuIwqqiJiIiIiIhbik3kx50YYzIaY+YZY8KNMQeMMSWNMZmNMauNMYdc/2ZyLWuMMSOMMRHGmN3GmMIP+t5UURMREREREbdkmcR93MU3wArLsp4GCgIHgB7AWsuy8gNrXX8D1ADyux7vAaMf9L2poiYiIiIiInIHxpgMQDlgAoBlWTcsy/oTqAdMcS02Bajvmq4HTLXibAUyGmNyPsi6VVETERERERG3lAxdH/MCZ4BJxphQY8z3xpi0wCOWZZ10LRMFPOKazgX89rfnH3eV3TdV1ERERERERO7MCygMjLYs60XgMv/r5giAZVkWSXDvbVXURERERETELSVDi9px4LhlWcGuv+cRV3E79VeXRte/p13zI4HH/vZ8f1fZfVNFTURERERE3JKVyI/bXt+yooDfjDFPuYoqAfuBQKCFq6wFsMg1HQg0d43+WAI4/7cukvdFN7wWERERERG5u47AdGOMN3AEeIu4Bq85xphWwK9AY9eyy4CaQARwxbXsA0mxLWoeHh5s3rKUefMnALBq9Ry2bF3Glq3LiDgczKzZ42xO6Az+/jlZuXI2YaFrCd25hg7t3wagd+/OHDm8jZDgFYQEr6B6tZdtTuo8HTu0Iix0LbvC1vF+x3fsjuMo2q/un4eHB1u2LGP+/IkA5M79GBs3BrB37wamTRtJqlSpbE7oHOkz+DH5h5EE71zJ1h0rKFb8RZ57vgCr1s1j4+ZA1m1cSOEiL9gdM1H0HfUD5d/qwSudPr/j/G17f6ZUs6406jKIRl0GMWbO8ode542bN/lo+ERqtf+UN3sMJfL0OQD2HDoav56GHw5ibfCuh16XU4wfN5zI47sIDV17S3n7dm+xZ88GwsLWMWhQL5vSOUu+/HnZ8FNg/OPXyFDatGvJs889zcq1c9i0dQkz5ozFzy+d3VEdQftW0os1ifu4E8uywizLKmpZ1guWZdW3LOsPy7LOWZZVybKs/JZlVbYs63fXspZlWe0ty3rCsqznLcva/qDvLcW2qLVv/xYHwyPwSx93oKhapXH8vOkzRrN0yWq7ojlKdHQM3bt/RljYXtKlS8vWLctYszYIgG+//Z6vvh5rc0JnevbZp2jV6k1KlqrFjRs3WbZkOkuXreHw4aN2R3ME7Vf3r0OHtzl4MCL+5Obzz3vw7bcTmDt3MSNGfE7Llq8xfvwPNqd0hi+G9GHt6o20bNqBVKlSkcY3NZOmfsuQQSNYs3ojVaqWp9+A7tSp0cTuqA+tboUSvF6jPL1GTL3rMoULPMHIj9ve92tHnj5Hn5HTmNi/0y3lC9ZuIX26NCwd9SnLN23n62mLGNrlbfL936PMHNINL09PzvxxnoYfDqJ80efw8vS873U7zZSpc/juu0lMnPRNfFn58qWoU6caRYpU4caNG2TLlsXGhM4RcegXypeuC8T9wLTv500sWbyKydO+pW+vwWz+KYQmzRrS8YN3GDjga5vT2k/7VtK7202qU4J/bVEzxrxkjEnvmk5jjOlnjFlsjBnsuqeAIz2aKwfVq1dk8uRZt83z80tH+fKlWLx4lQ3JnCcq6jRhYXsBuHTpMuHhEeTKlcPmVM739NP5CQkJ5erVa8TExLAxaCuv1K9hdyzH0H51f3K5jlmTJv3vmFW+fCkWLFgGwPTp86lTp6pd8Rwlffp0lCpdjGlT5gBw8+ZNLpy/iGVZ8T/Mpc/gR9TJU3bGTDRFn81HhnS+D/TcJRtCeLP7UBp1GUT/MTOJiUnY6cz6kN3UrfASAFVKvkjwnoNYlkUaH+/4Stn1Gzcx5u53hnU3mzYF8/sff95S1rp1c4YMHcWNGzcAOHPmnB3RHK18hVIc/eUYx387Qb58edn8UwgA69dtok69ajancwbtW/Iw7tX1cSJxfSsh7o7cGYDBrrJJSZjroQwZ0pdevQcRG3v7JYF16lRl/fqfuHjxkg3JnC13bn8KFnqWkJBQANq0bcH2basYO3YYGTM6tl5ui337wilT5iUyZ85EmjSpqVG9Iv7+j9ody5G0X93b0KGf0KvXQGJj406ks2TJxPnzF4iJiQEgMvIkjz6qii7A/+V+jLNnf2fUmMFs+CmQb0YOxNc3DR93H0D/AT3YGx5E/8970P+TYXZHTTa7Dv5Cww8H0XbAd0Qci7te/cjxKFb8tJMpn3/I3OE98fDwYGnQtgS93qnfz/NI1kwAeHl6ks43DX9evAzA7p+P8soHA2jw4UD6tH49RbSm3c2T+R+nTJni/LRpMWvXzKNokYJ2R3KcVxvWYv7cJQCEhx+iZu3KANR7pQaP6se5u9K+lbiSYdRH29yrouZhWVa0a7qoZVmdLMvaZFlWP+DxJM72QKrXqMiZM+cIC917x/mNGtdl7pzAZE7lfGnT+jJr5li6dv2UixcvMW7cNAoUKEOx4tWIijrN4MF97I7oKOHhEQwdOorly2awbMl0wnbtS/Cv1f8l2q/urUaNipw+fY7Quxyz5FZeXp4ULPQsE7+fQfnSdbly5QqdurTm7Xfe5OMen/Pc02Xp1WMgI74bZHfUZFHg8cdYOeYz5n3ZkzdrlKfT4Ljrr4N3H+TAkWO82X0IjboMInjPQY6fOgtAp8HjaNRlEO0/H82+w8firzsLWLflnut74ck8LPymNzMHd2PCglVcv3EzSd+fnTy9PMmcKSOly9ShR48BzJgxxu5IjpIqVSqq16zIooVx10V2bNeTVu80Yd3GhaRLl5abN1PuvvGwtG8lrqQe9dFO97pGba8x5i3LsiYBu4wxRS3L2m6MeRJw5CewZImi1KpVmWrVXiZ1ah/8/NIxYcJXtGrVmSxZMlGkSEFef6213TEdxcvLi9mzxjFrVgCLFq0A4PTps/HzJ06cwcIFk21K51yTJs9ikqt77YDPenD8+AONvJpiab9KmJIli1K7dmWqV6+Aj48P6dP7MWzYp2TIkB5PT09iYmLIlSsnJ05E2R3VEU5ERnEiMood2+MGsggMWEGnD1tTomRRenz0GQABC5bxzciBdsZMNul808RPly3yLJ+Pn80fFy5hYVG3wkt80LTebc/5uvt7wN2vUXskcwZOnf2DHFkyER0Tw6UrV8nol/aWZR73z0Ga1D5EHDvBs/lyJ8E7s1/k8ZMsDIirhGzbHkZsbCxZs2bm7NnfbU7mDJWrlmN32P74bnuHfj5Cg/pxg9s9kS8PVapVsDGds2nfkoS6V4vaO0B5Y8xh4BlgizHmCDDeNc9xPvlkCE/mL8kzBcrQonlHNmzYTKtWnQGo/0pNVixfx/Xr121O6Sxjxw4lPPwQ34wYH1+WI0f2+Ol6dauzb99BO6I52l8X/z722KPUr1+DmbMW2pzIWbRfJUzfvkPIl68ETz9dhubNO7J+/WbeeusDNm7cwquv1gSgSZMGLNEASEBcZT8y8iT58ucFoFyFUhwMj+Bk1ClKl33JVVaSI/+RgX3O/nEBy4r7DXjPoaPEWhYZ/dLy0vNPsXpLGOfOXwTg/MXLnDidsJPACsWeJ3B93H1dV28JpfhzT2KM4fips0S7uuOeOP07RyOjeDR7yh0EITBwJRUqlAIgf/7H8fb21on03zRoWJv585bE/501a2YAjDF0+agdkyfePk6AxNG+lbiSY9RHu/xri5plWeeBlq4BRfK6lj9uWZZbXqXdsGEdvhw+2u4YjlKqVDGaNmnInj0HCAmOa/Xo23cwjV+rR8EXnsWyLH799TjtO/SwOanzzJ09nsxZMnHzZjTvv9+L8+cv2B3JMbRfPbxevQYxbdpIPvmkK7t27WPy5Nl2R3KMbl36M27Cl3h7p+LoL7/Rvm13li1dw6AhffDy8uTatet06pgyhrvu9uUktu87xJ8XL1H53d60e61mfGWpcbWyrN4SypyVQXh6euLjnYohnd/CGMMTj+Wkw5u1adN/JLGxFl5ennz8bmMezZ75nut8pVIpPh4xlVrtPyUQB6ViAAAgAElEQVRDurQM6RzXShJ64AgTF67Cy8sTYwy93n2NTOlTxhDs06aNony5kmTNmplfjmynf/9hTJo8i+/HDyc0dC03b9zk7Vad7v1C/xG+vmmoULE0nT/4X/f1Bo3q0Oq9uJFWlwSuYvq0eXbFcxTtW0kvJV94Yv76JS4xpfXN47Quno4UHRtjdwS3EhObkj+KicvTI8XeIjHReRhtq4RK7al7uSVU1Fb9KJhQ6Qq3tDuCW/HzebBRQP+LLl6/cu+FBICbNyId1paUcF/kbpqo9Y4ev/7gmG2RYu+jJiIiIiIiKVtKbh1SRU1ERERERNxSbAquqqnPj4iIiIiIiMOoRU1ERERERNxSSh7BQBU1ERERERFxSym346O6PoqIiIiIiDiOWtRERERERMQtqeujiIiIiIiIw8Q65q5niU9dH0VERERERBxGLWoiIiIiIuKWUvJ91FRRExERERERt5Ryq2nq+igiIiIiIuI4alETERERERG3lJJHfVSLmoiIiIiIiMMkSYvazZjopHjZFMfbM5XdEdyM9itJfNE6XiXYJW2rBEtbuKXdEdzG1RNBdkdwK2lzlbM7goijaDARERERERERh0m51TR1fRQREREREXEctaiJiIiIiIhbSsmDiaiiJiIiIiIibiklX6Omro8iIiIiIiIOoxY1ERERERFxSym3PU0VNRERERERcVMp+Ro1dX0UERERERFxGLWoiYiIiIiIW7JScOdHVdRERERERMQtqeujiIiIiIiIJBu1qImIiIiIiFtKyfdRU0VNRERERETcUsqtpqnro4iIiIiIiOOoRU1ERERERNxSSu76mCJb1Pz9c7Jq5Rx2ha0jLHQtHTq0AqDBq7UIC13LtavHKFz4BZtTOse+A0EEhyxn89albNy06JZ5Hd9/h0tXfiFLlkw2pXMOf/+crFw5m7DQtYTuXEOH9m8D8PzzBdiwPoAd21ezYP5E/PzS2ZzUfnfbVgDt2rZk964fCd25hoGff2xjSucYP244kcd3ERq69pby9u3eYs+eDYSFrWPQoF42pXOWO22rLwb1Zs+eDezcsZq5c78nQ4b0NiZ0jvHjhnPi+C7C/ratMmXKyIplMzmwbxMrls0kY8YMNiZMXL0Hfkm5Wq9Tv2mbuy4TsnM3DVq0p16T1rRs/9FDr/PGjRt06TOIGo3f5o13OxF58hQAe/YfpEGL9jRo0Z5XW7RjzYafHnpdTjFu7DCO/xZG6M418WUFX3iGoI2BbAtZyZbNSylatJCNCZ3lbsd3gE6dWnPzRqTOsR5SbCI/nCRFVtSio2Po1r0/BQtVpEzZurRt04ICT+dn3/6DNH7tXYKCgu2O6Dg1a7xJqRK1KFemXnxZrlw5qVSpLMeORdqYzDmio2Po3v0zCr1YibLl6tGmTQuefjo/Y0YPpXefLyhStAqLAlfy4Yd3P0n4r7jbtipfviR16lSlaLFqvFi4Ml99PdbuqI4wZeocatducktZ+fKlqFOnGkWKVKFQoYp8+eUYm9I5y5221Zq1GylUqCKFi1Th0KEjdO/ewaZ0zjJ16hxq/WNbde/WnnU/bqLAs2VY9+Mmundrb1O6xFe/ZhXGfDngrvMvXLzEgOEjGTn4ExZNH8vwAQn/8SPy5Claduh2W/mCJatI75eO5XMm0uy1+nz53UQA8j2em9kTRjB/yijGDh9A/yHfEh0dc/9vyoGmTptL7TpNbykbOKgXAz7/imLFq9Gv/3AGDdQPS3+50zELwN//UapULsevvx63IZW4i3+tqBlj3jfGPJZcYRJLVNRpwsL2AnDp0mXCww/xaK4chIdH8PPPR2xO5z4GD+lD795fYFkpt0n5fty+X0WQK1cO8ufPS1DQVgDWrt3IK/Vr2BnTEe62rd57txlDh33HjRs3ADhz5pydMR1j06Zgfv/jz1vKWrduzpCho7St/uFO22rNmo3ExMSdBAcH78Q/V047ojlO0B22VZ061Zg6bS4Qd8Jdt251O6IliaKFnidDer+7zl+2ej2Vy5cmZ47sAGTJlDF+3uKV63j9nQ9o0KI9/YaMiN+f7mVd0Bbq1awMQNUKZQneEYZlWaRJnRovL08Art+4AcY86NtynE2bgvnjH/uVZVmkd/UmyZDej5OulkW58zELYNiwT+n58ec6x0oEViL/dzfGGE9jTKgxZonr77zGmGBjTIQxZrYxxttV7uP6O8I1P8+Dvrd7tah9BgQbY4KMMe2MMdkedEV2yZ3bn4IFnyMkJNTuKI5lWRaLFk8l6KdA3nr7DQBq1a7CiRNR7N1zwOZ0zpQ7tz8FCz1LSEgo+/f/TN061QBo8Gpt/P0ftTmds/x9W+XP/zilSxcnaGMgq1fPpUiRgnbHc6wn8z9OmTLF+WnTYtaumUdRbasEadnydVas/NHuGI71SPasREWdBuJ+UHkke1abEyWfo8eOc+HiJVp26EbjtzuyaHlc173DR4+xYu0Gpo0Zzvwpo/Dw8GDJqoTtQ6fPnCOHaxt6eXmSLq0vf56/AMDufeHUa9KaV5q3pe9HHeIrbilR166fMmhQbw5HhPDFF33o3WeQ3ZEcrU6dqpyIPMnu3fvtjpIiJGPXxw+Av58YDwa+siwrH/AH0MpV3gr4w1X+lWu5B3KvwUSOAEWAysBrQD9jzA5gJrDAsqyLD7ri5JA2rS+zZ42ja9dPuXjxkt1xHKtK5UacPHGKbNmyELh4Gj8fPEzXj9pRr05zu6M5Utq0vsyaOTZ+v2rduitfftmfnj3fZ8nS1dy4cdPuiI7xz23l5eVF5kwZKVuuLkWLFmLG9O946unSdsd0JE8vTzJnykjpMnUoVrQQM2aM4cmnStody9F69Hif6OhoZsxYYHcUt/Ff+jU/JiaW/eGH+H7EF1y/fp0mrT+k4LNPE7w9jP3hEbze6gMArl+/TmZXa9v7PfsTeeIUN6NvcvLUGRq0iOsq2rRxPV6pVfVf1/fCs0+zaPpYDh89Rq8Bwylbohg+Pt5J+yZt8t57zfnoo34sDFhGwwa1GTt2GDVqvGF3LEdKkyY1Pbp3pEbNN+2OIvfBGOMP1AI+Bz40xhigIvDX/8gpwKfAaKCeaxpgHjDSGGOsBzjg3quiZlmWFQusAlYZY1IBNYA3gGGAY1vYvLy8mD17HDNnLSRg0XK74zjayRNxXRTOnDnH4sUrKVP2JfLk9mdL8DIAcuXKwabNiylfrj6nT521M6rtvLy8mD1rHLNmBbBo0QoADv58OP46kPz58lKjeiU7IzrGnbZVZOTJ+M/j9u1hxMZaZM2ambNnf7czqiNFHj/JwoC4bbVtexixsbHaVv+iebPG1KpZmarVGtsdxdFOnT5LjhzZiYo6TY4c2Tn9H+pS+0j2rGTI4IdvmtT4pklNkULPcTDiFyzLom6NynRu+9ZtzxkxqC8Qd41ar8+HM3nkkFvmZ8+WhajTZ8mRPRvR0TFcunyFjP8YzOaJPP+Hb5o0HDpylOcKPJl0b9BGzZo25MMP47bVvPlLGDNmqM2JnOuJJ/KQJ8//sWP7aiBu8K2Q4JWUKl2LU6fO2JzOPf1bd8VE9DXQDfirf3UW4E/LsqJdfx8HcrmmcwG/AViWFW2MOe9a/r5Pou/V9fGWTtWWZd20LCvQsqw3gNz3u7LkNG7sMMLDI/jmm/F2R3E0X980pEuXNn66YqWy7Nixi7x5ivFsgbI8W6AskZFRlClV5z9fSQMYO3Yo4eGH+GbE//arbNmyAGCMoUfP9xn//Q92xXOUO22rwMCVlC9fCoir1KbyTqWKx10EBq6kQgXXtsr/ON7e3tpWd1G1agW6dG3LK6+25OrVa3bHcbQli1fRvFkjAJo3a8TixSttTpR8Xi5bgtDd+4iOjuHqtWvs2XeQx/M8RomihVi9fhPnXNcRnb9wkRNRCbvG6uUyJVi0LK4L5ar1QbxUpCDGGI6fiIofPORE1Cl++fU3cuV8JGnemAOcPHmKcuXiWvxffrk0ERG/2JzIufbuDSeXf0HyP1mC/E+W4PjxkxR/qZoqaQ8hqbs+GmNqA6cty9qRpG/kDu7Vovba3WZYlnUlkbMkmlKlitG0aUP27DnAtpC4L6E+fQfj4+3NV199RrZsmVkUMIVdu/dRu3bTe7xaypY9e1Zmzoobec/Ly5M5cwJZs3qjzamcqVSpYjRtErdfhQTHtRD17TuYfPny0qZNCwACApYzZcpsO2M6wt221eQpsxk3bhg7d6zhxo0bvPNOZ5uTOsO0aaMoX64kWbNm5pcj2+nffxiTJs/i+/HDCQ1dy80bN3m7VSe7YzrCnbZVt24d8PHxYcXyWUDcgCLtO/SwOan9fvjbtjp6ZDv9+g9j8NBRzJoxhrdavsGxY8d5/c2UM0rtR598wbbQ3fz55wUq1W9Ku1bNiI6O+7H7tVdq8USe/6P0S0V5tUVbPIwHDepUI//jeQDo+G5z3uvUi1grllReXvT6sB2P5rh3xerV2tXo+dlQajR+mwzp/RjaL26/27l7HxOmzcHLywsPD0Pvru3JlEJuhTBt6kjKufarI4e30f+z4bRp240vh/fDy8uLa9eu07Zdd7tjOsbdju/iVkoDdY0xNYHUQHrgGyCjMcbL1armD/w1THok8Bhw3BjjBWQAHqj7gkmK/unePv7/nU7vD8HbM5XdEdzKzdjoey8kcp9iY5121xRJCfQlmHBXTwTZHcGtpM1Vzu4IbuO/dA3mw7p5I9JthyZtlvvVRP0fPe3XBXfdFsaYCkBXy7JqG2PmAvMty5pljBkD7LYs6ztjTHvgecuy2hhjXgdetSzrgfrlp8j7qImIiIiIiCSh7sQNLBJB3DVoE1zlE4AsrvIPgQfu4nGvro8iIiIiIiKOlJztppZlrQfWu6aPAMXvsMw1oFFirE8VNRERERERcUuxKbizubo+ioiIiIiIOIxa1ERERERExC0l033UbKGKmoiIiIiIuKWUPHazuj6KiIiIiIg4jFrURERERETELaXkwURUURMREREREbeUkq9RU9dHERERERERh1GLmoiIiIiIuKWUPJiIKmoiIiIiIuKWLEtdH0VERERERCSZqEVNRERERETckkZ9FBERERERcRhdo3afUnmq/pcQMVZK3rUSX2ystldCeXulsjuC24i2O4Ab8fLwtDuC20jj5W13BLeRNlc5uyO4lQvrh9odwW1kqdTT7ggiD0U1KhERERERcUsp+T5qqqiJiIiIiIhbSsnXqGnURxEREREREYdRi5qIiIiIiLillHwfNVXURERERETELaXkoebU9VFERERERMRh1KImIiIiIiJuSaM+ioiIiIiIOIxGfRQREREREZFkoxY1ERERERFxSyl51Ee1qImIiIiIiDiMWtRERERERMQtpeRr1FRRExERERERt5SSR31U10cRERERERGHSbEVNQ8PDzZvWcq8+RPiyz75tCthu9axY+ca2rZtaV84B/Hx8SEoaBHBwcvZsWM1vXt3BqB8+VJs3ryU7dtXMX78cDw9PW1Oar/x44YTeXwXoaFr48umTx/N9m2r2L5tFYd+3sr2batsTOg8//wcjh07jH37g9iydRlbti7jhReesTmh/fz9c7Jy5WzCQtcSunMNHdq/DUDv3p05cngbIcErCAleQfVqL9uc1Dm0XyVcm/Yt2RS8lKCtSxg38Ut8fLxp9V5TQsJWc/bCz2TOnMnuiI7g75+TVSvnsCtsHWGha+nQoRUAgwb1Zs/u9ezYvpq5c74nQ4b0NidNHH0nBFCh4xBe7TXqX5fbeySSwm/3Y/W2fQ+9zvOXrtB66FTqdB9B66FTuXD5KgA/7gynYe/vaNxnNG98OpadP//60OtyGh2zklasZSXqw0lSbNfH9u3f4mB4BH7p0wHQrFkj/HPl5MVClbAsi2zZstic0BmuX79O9epvcPnyFby8vFi3bh5r1mzk+++HU6PGm0RE/EKfPh/StGlDpkyZbXdcW02ZOofvvpvExEnfxJc1adI2fnrI4L6cv3DBjmiO9c/PIUCvjwcSELDcxlTOEh0dQ/funxEWtpd06dKydcsy1qwNAuDbb7/nq6/H2pzQebRfJUyOnI/wbutmlC5ek2vXrvP95K95pUEtQrbuYNWKH1m0dJrdER0jOjqGbt37x38Og7cuZ+2ajaxdu5HevQcRExPDwM8/pnu3Dnzca6DdcR9avTKFeKNScXqNX3jXZWJiY/l67mpKPvfEfb32tgO/ELgpjM/efeWW8olLN1G8QF5a1S7LhCVBTFi6ic6Nq/DSM3mp8GJbjDH8/FsUH42ay6IvOj7Q+3IqHbOSlrOqVonrX1vUjDHexpjmxpjKrr/fNMaMNMa0N8akSp6I9+/RXDmoXr0ikyfPii97590mDBo0In4IzzNnztkVz3EuX74CQKpUXnh5pSImJoYbN24SEfELAOvWBVG/fg07IzrCpk3B/P7Hn3ed37BhHWbPXpSMiZztTp9DuV1U1GnCwvYCcOnSZcLDI8iVK4fNqZxL+9X98fLyInWa1Hh6euLrm4aoqNPs2X2A345F2h3NUW7/HB7i0Vw5WLNmIzExMQAEB+8kV66cdsZMNEWeykP6tGn+dZmZq4OpXOQZMvulvaV88rKfeLPfOBr2/o7vFv6Y4HX+GHqQumUKAVC3TCF+3BkOgG9qH4wxAFy9fjN+OqXQMUsexr26Pk4CagEfGGOmAY2AYKAY8H0SZ3tgQ4b0pVfvQcTG/q+OnTdvbho0rE3QpkAWBkzmiSfy2BfQYTw8PNi6dRnHju1k3bogtm0Lw8vLk8KFnwfglVdq4u+fMr6ckkqZMi9x+vSZ+Mqt3PlzCHFdkIODlzN4cB+8vb1tSudMuXP7U7DQs4SEhALQpm0Ltm9bxdixw8iYMYPN6ZxB+1XCRZ08xahvJxC2bz37Dv3EhQsXWb/uJ7tjOV7u3P4ULPhc/OfwLy1bvsbKlQmvmLizU39cYN3OcBpXLHpL+ea9ERw7dY7pfd9lTv827D96gh0HjyboNX8/f4lsGf0AyJohHb+fvxQ/b+2OA9Tr8S0dvppOv1b1Eu19OIGOWUkvFitRH05yr4ra85ZlvQa8AlQFGlqWNQ14C3gxqcM9iOo1KnLmzDnCQvfeUu7j4831a9cpW6YukybNZPSYITYldJ7Y2FhKlKhJvnwlKFq0EM888yTNm3dkyJC+BAUt4uLFy/G/KMqdvf5afWapNS3e3T6Hn3wymBcLVaJs2XpkypSRD7u0sSmh86RN68usmWPp2vVTLl68xLhx0yhQoAzFilcjKuo0gwf3sTui7bRf3Z8MGdNTo2YlijxfkeeeLIOvry+NXqtrdyxHS5vWl9mzxsV/Dv/So3tHoqNjmDFzgY3pks/Q6Svo1KgyHh63niZu2XuYLXsP81rfMbz+yViOnjzLr1G/A9Ck/3ga9xlNv0mBrA87SOM+o2ncZzQ/7Ym47fWNMfC3lrNKRQqw6IuOfP3+64xasC5p31wy0jEreaTkitq9rlHzMMZ4A2kBXyAD8DvgAziy62PJEkWpVasy1aq9TOrUPvj5pWPChK+IjIxi0aIVAAQuWsmYMUNtTuo8589fYMOGzVStWoGvvx5H5cqNAKhUqSz58+e1OZ1zeXp6Ur9+DV4qoe6hf7nb57BVq7jBam7cuMG0aXP5oNO7Nid1Bi8vL2bPGsesWQHxx6nTp8/Gz584cQYLF0y2KZ1zaL+6P+UrlOLXX49z7twfACxZvIpiL73I3NmBNidzJi8vL2bPHsfMWQsJWPS/a4eaNWtEzZqVqVb9NRvTJa99R0/QffQ8AP64dIWg3Yfw9PDAsuDt2mVp9HLR254zvW/c5+5u16hlzpCOM39eJFtGP878eZHM6dPe9hpFnsrD8TMB/HHxMpn8bp/vbnTMkod1rxa1CUA4EAb0AuYaY8YD2wBHdrb95JMhPJm/JM8UKEOL5h3ZsGEzrVp1ZsniVZQvXxKAsmVLqIuaS9asmeNHsUqd2odKlcpy8GBE/GAr3t7edOnSlvHjp9sZ09H+2maRkSftjuIYd/sc5siRLX6ZOnWqsn/fzzamdI6xY4cSHn6Ib0aMjy/LkSN7/HS9utXZt++gHdEcRfvV/Tl+/ARFixUiTZrUAJQrX5KfDx6xOZVzjRs7jPDwCL755n+fw6pVK9C1S1tebfAWV69eszFd8lo+rBPLh3dm+fDOVCn6DL2a16JikQKUev4JAoJCuXLtOhDXRfLchUv3eLU4FQo9ReCmMAACN4Xx8otPAXDs1Ln48QMOHD3BjZsxZEznmwTvKvnpmJU8LMtK1IeT/GuLmmVZXxljZrumTxhjpgKVgfGWZYUkR8DEMnz4aCZO+poOHVpx6fIV2rfrYXckR8iRIzvjx3+Jp6cHHh4ezJ+/hOXL1zFw4MfUqFEJDw/D+PE/sGHDZruj2m7atFGUL1eSrFkz88uR7fTvP4xJk2fxWuN6GkQkgSZO/IasWTNjjGH37v28/34vuyPZrlSpYjRt0pA9ew4QEhzXmta372Aav1aPgi88i2VZ/Prrcdp30DHrbrRf3dnO7btZvGgl64ICiI6OZs/uA0ydNIt32zSj4wfvkv2RrGzcEsiaVRvp1PG/vc1KlSpG06Zxn8NtISsB6NN3MF9+2R8fb2+WL5sJQHDITjp06Gln1ETRffQ8tocf5c9LV6jSeTht679MtOsSh8YVi931eaWey8cvJ87SbEDcMPO+Pt4MbP0qWRJw14K3a5fho1FzCQgKJWeWDAxtF9drZ832Ayz+aRepPD3w8U7FkHYNU9yAIv+kY1biclp3xcRkkqLmmNY3T8rdYonIafdqcLromGi7I7gNby9H9kx2pOhYXX+ZUF4eup9iQqXx0uAACXXh+hW7I7iVC+t16UZCZank/pXq5HL5ylG3rR0Xf7R8op5Qh5zY4JhtkWLvoyYiIiIiIimblYJb1FRRExERERERt+S068oS070GExEREREREZFkpoqaiIiIiIi4paS+j5ox5jFjzI/GmP3GmH3GmA9c5ZmNMauNMYdc/2ZylRtjzAhjTIQxZrcxpvCDvjdV1ERERERExC0lw/D80UAXy7KeAUoA7Y0xzwA9gLWWZeUH1rr+BqgB5Hc93gNGP+h7U0VNRERERETkDizLOmlZ1k7X9EXgAJALqAdMcS02Bajvmq4HTLXibAUyGmNyPsi6NZiIiIiIiIi4peS8j5oxJg/wIhAMPGJZ1knXrCjgEdd0LuC3vz3tuKvsJPdJFTUREREREXFLyTU8vzEmHTAf6GRZ1oW/35jdsizLGJPoQdT1UURERERE5C6MMamIq6RNtyxrgav41F9dGl3/nnaVRwKP/e3p/q6y+6aKmoiIiIiIuKVYy0rUxz+ZuKazCcABy7K+/NusQKCFa7oFsOhv5c1doz+WAM7/rYvkfVHXRxERERERcUvJ0PWxNNAM2GOMCXOVfQx8AcwxxrQCfgUau+YtA2oCEcAV4K0HXbEqaiIiIiIiIndgWdYmwNxldqU7LG8B7RNj3aqoiYiIiIiIW7pTd8WUQteoiYiIiIiIOIxa1ERERERExC0l1/D8dkiSipqPZ6qkeNkU58rN63ZHcCseHmoATihPo22VULGJf9sTES5cv2J3BLfx93sRyb35V/vE7ghu42xAN7sjSDJQ10cRERERERFJNur6KCIiIiIibkldH0VERERERBxGXR9FREREREQk2ahFTURERERE3JK6PoqIiIiIiDiMZcXaHSHJqOujiIiIiIiIw6hFTURERERE3FKsuj6KiIiIiIg4i6VRH0VERERERCS5qEVNRERERETckro+ioiIiIiIOIy6PoqIiIiIiEiyUYuaiIiIiIi4pVi1qLmXfPnzsuGnwPjHr5GhtGnXku49O7L3YFB8eeWq5e2OajsfHx+CggIJCVnBzp1r6NPnw1vmDx/ej7NnD9iUzln8/XOycuVswkLXErpzDR3av33L/E4fvMf1a7+RJUsmmxI6y579G9kSspxNW5awPmgRAM89/zRr1s1jS8hyZs8dj59fOptT2i/uM7iI4ODl7Nixmt69OwNQoUJpNm9eytaty1i7dh6PP57b5qTO4eHhweYtS5k3f0J82SefdiVs1zp27FxD27Yt7QvnIP7+OVm1cg67wtYRFrqWDh1aAZApU0aWLZvBvn1BLFs2g4wZM9ic1H53O74//3wBNqwPYMf21SyYP1HHLJc27VuyKXgpQVuXMG7il/j4ePP1yM9Z/1MgGzYHMnHqCNKm9bU7ZqL4ZPpqXu45jgYDf/jX5fb+GkWRD0awOvTQQ6/z/OVrtB65gDr9J9N65AIuXLkGwI+7D9No0A80/mI6bw6ZSejhyIdeV0pgJfJ/TmKSol9nZr/8jnmXHh4e7Pt5E1VebkiTpg24fPkKI0dMuPcTk8GVm9ftjgBA2rS+XL58BS8vL9atm0/Xrp8SEhJK4cIv0KHD29StW42sWQvYHdP2D0+OHNnJkSM7YWF7SZcuLVu3LKNho3cIDz+Ev39OxoweypNPPUHJkjU5d+4PW7P6eKaydf0QV1ErX7Yev/9tW6zfGECvjwfy06YQmjZvRJ7c/gz47CsbU8LN2Bhb1w///AzOo2vXfnz//Zc0avQuBw9G8N57zShatCDvvdfV1pwexti6/r907NiKwoVfwC99Oho2aEWzZo0oV64E773XFcuyyJYtC2fOnLM1482YaFvXD7cfs4K3Lqdhw1Y0b96Y33//k6HDRvFR1/ZkypSBj3sNtC2nccB+dbfj+4Tvv6JHzwEEBW2lRYvXyJPnMfr1G2ZrVj/vNLauP0fOR1i6cgali9fk2rXrfD/5a9as2sCSxau4dPEyAJ8N7MmZM+cY8dU4W7P+Nq/TQ7/GjohIfH1S0XvaKuZ/3PSOy8TExtJm5EK8U3lSv8SzVHkxf4Jee9uh4wRu3c9nzareUv5VwCYy+PrwdtViTFy1jQtXr9X3lPUAAA9USURBVNOpXhmuXL9BGu9UGGP4OfIM3SYuJ6BP84d+jwBpqraz/4P4gHJkLJCoJ4hRfx5wzLa4Z4ua+f/27jy6yvLa4/h3J6GEhMEBKAoKSJSLijiAYEBQEJmpc62idUKchy7q1OvU4aKgtra3igqKKBAECgEqioIiVBkUUSCJGKcQNAQHkIAVMfv+cY4RpSB3GXmeE36ftc7i5CSEH2flfc/Zz7ufHbODzGyomd1vZveZ2WVmVn93hKsO3U7I5f33Sihd/WHoKNHatGkzALVqZVCrVgbuTlpaGsOG3cItt4R78Y5NWVk5y5atAKCiYhNFRcU0bdoEgBHDb+fmW/5Uoze0VodWOS3514LFALwwZwEDf9E7cKI4bHsMZmTUwt1xd+rXT6ze169fj48+WhsyYjT2b9qE3r27M2ZMXtVjlww+l2HD/lp1/IUu0mKx/TnrbfZv2oQBA07miScnAfDEk5MYOLBXyJhR2NH5/eCDWzJ//kIA5sx5iVNP6RMyZjQyMjLIrJNJeno6WVl1KCsrryrSADIza9eY18NjcppSPytzp18zYd4b9Dgyh33qfvcq4pjnX+OcERM4c9iTPPDPV3b533xx+TsM6HgoAAM6HsoLb74DQFbtn1UtbHyxZSsRrHFE4ZvXzOq6xWSnhZqZXQOMBDKBDkBt4ABgoZmd8JOnqwanndGPKZNmVn18yaWDmP/KDP72wDAa7JUy9eZPKi0tjUWLZrF69evMmbOAJUuWcfnlFzBz5nOUlZWHjhel5s2b0e7Iw1i8+HUG9D+ZDz8sY/lytYhuy92ZNv1x5i3I54ILzwagqHAV/fr3BOCU0/rStNl+ISNGIy0tjYULn6akZClz585nyZJlXHHFjUydOobi4oWcc85p3HPPg6FjRmH48Nv43X8Po7Ly2xfTli2bc/oZ/Zm/YDpTp42hVasW4QJGqnnzZrRrdziLF79O48YNq87tZWXlNG7cMHC6uGx7fi8oWMXAAYlC9vTT+tOs2f6B04VX9tFa/v630Sxb+SIr3/4Xn3++kRfn/guAvz4wjILilzn4kIMY9dATgZPuHmvXV/DCm+9wVpcjvvP4y4UfULJuPeOGns3EG8+lcHU5rxXvWqviJxs306hBNgAN62fxycbNVZ+b+0Yxp/xhLFePzOeOc3tW338khVXi1XqLyQ9dURsM9HH3PwInAYe5+++A3kDYfqVdUKtWLXr37U7+1FkAPDpqPEcf0YOuuQMpKyvnj/9zc+CEcaisrKRjxz60atWRDh3a0aXLsZx+ej8eeGBM6GhRys7OIm/CQwwdegdbt27lhhuu4s7f3xs6VnR6nXQWXTsP5PRTL2LwkPPI7dyBKy6/kcGXDmLegnzq1c3mqy1fhY4ZhcrKSjp16ktOTifatz+SQw89hKuvvoRTT72AnJxOPPHEJO6++9bQMYPr3ac769Z9wrLXV3zn8dq1f8aX//6S47sM5LHHJvDgyOGBEsYpOzuLiXkPM3ToHWzcWLHd52NbQQ5p2/P7xo0VDBkylCFDzueVl/9J3XrZbNE5iwZ71adP3x4c07Y7hx/ShaysLM785UAArrniZg4/pAurVr3DKaf1DZx09xgxZR7XDuxMWtp3L28tLCrhlaIP+OXd4zl7+HjeX/sZJesSWwEG3ZPHWXeN4/fjn2feinc5665xnHXXOF4u/GC7729mGN9+7+7tcph26/n8efAAHpi561fpJDXtytTHDOBrElfT6gK4e4mZhd8E8wNOOrkrby4rqGqD2bYdZuyYp8ibFLZ3OjYbNnzOvHmv0K1bLgcd1JyCgpcAyMqqw8qVL3HYYV0DJwwvIyODiXkPk5c3jfz8ZzjssP+iRYsDWLLkWQCaNd2PhQtn0aXLANauXRc4bVjftOp9vO4TZk6fzTHt2/G3+0dxysBfA5CT05JevU8MGTE6iWPwZXr1OpG2bduwZMkyACZPnkF+/tjA6cI7rlN7+vU7iV69TiQzszb16tVl9Og/s2ZNGfn5zwAwPf9ZRo4cEThpPDIyMpg48WEm5E1lWn5i0bK8/GOaNGlMWVk5TZo0Vqto0vfP7wBvrXqHfv3PBeDgnJb06d0jZMQodDshlw8+KK3aiz1zxmw6dDyKSROnA4mFp6mT/8nV1w1mwrh/hIy6WxSUlHPjmMSxtb7i3ywoeJ/0tDTcnYt7duCMLm23+ztPDk10mexoj9q+9bJYt2ETjRpks27DJvapt/2+xGNymlL6yQY+q/iCveuG3bcYWk1ebPqhK2qjgCVm9gjwCvB3ADNrBHz6E2f70U4/oz9TJn/b9vjznzequt9/QE8KC1aFiBWVhg33oUGDRAtoZmZtevQ4nqVLl9OiRXtat+5M69ad2bz5CxVpSQ89NIKiore5/6+PALByZREHHHgUrVvn0rp1LqVrPqJTpz57fJGWlVWHunWzq+5379GFwoJVNGy0L5BYIfztjVcyevT4kDGj8J+OwaKit6lfvx45OS0B6N79eN56qzhkzCjcfvtwDjn4OA5t04Vfn3818+a9zMUXX8/MGbPp1u04AI4/vhPFxe8FThqPhx+6h6KiYu6//5Gqx2bMfI7zBp0JwHmDzmTGjNmh4kXl++d3gEbbnLNuuvkaHhm188l/e4LS0g9p3+FI6tRJ7Nvq2u04Vr31Li0POrDqa3r37cHbq94NFXG3evrOC5l150XMuvMiTjoyh1vOOpHu7VpxXJvmTFu4ks1fbgESLZKfbtPCuDPd2h7EjEUFAMxYVMAJbVsBULJufVVRUri6nC1bv2av7J3vn9sTVLpX6y0mO72i5u73m9nzQBvgXncvSj6+Doj6nXtWVh1O6N6Z66/9tl3ojj/cQNsj2uDulJSs4TfXqJWoSZPGjBp1H+np6aSlpTFlykxmzZoTOlaUcnM7MOjcM1i+vJDFixKrrbfddjfPPPtC4GTxady4IePyRgKQkZ7OpKem8/xzL3H5FRcw+NLzAJg+/VmeHDspZMwoNGnSmEceuY/09LRtjsG5XHnlTUyYMJLKykrWr9/AkCG/DR01Wvfe+yCPPvYXrrrqYio2bebKK24KHSkKubkdGDQocc5asjhx1f/W2+5mxIj/Zfz4kVxw4dmUlJRyzjmXB04a3o7O7zk5LbnsskQXwLRps3j88YkhY0Zh6atvMiP/WebOn8bWrVtZ/mYhYx/LY+rMsdSrVxczY+WKIoZef3voqNXipsdm8WpxKesr/s3Jt47m8r4d2fp1JQBnfm9f2rZy2zTnvbWfcv69TwGQVbsWfzq/F/vU++FfW3BRz/bc8OjTTF24kv33rs/wixJtpHOWFTNjcSEZ6Wlk1spg+IV9opiaKj+dGj+eP2axjOdPFaHH86eSGMbzp4oYxvOniljG86eCGMbzpwq90fz/CT2eP5VUx3j+PUUqj+ffu25Otb5B/KyiOJrnokb+wmsREREREZFUtivDRERERERERKIT20j96qRCTUREREREUtKePPVRREREREREdjNdURMRERERkZQU20j96qRCTUREREREUlJNngqu1kcREREREZHI6IqaiIiIiIikJLU+ioiIiIiIREZTH0VERERERGS30RU1ERERERFJSTV5mIgKNRERERERSUlqfRQREREREZHdRlfUREREREQkJdXkK2oq1EREREREJCXV3DINrCZXoSIiIiIiIqlIe9REREREREQio0JNREREREQkMirUREREREREIrNHFGpm1tvM3jKzYjO7KXSeWJnZo2ZWbmYrQmeJnZkdYGYvmFmBma00s2tDZ4qVmWWa2WIzeyP5XN0ZOlPszCzdzF43s5mhs8TOzN43s+VmtszMXg2dJ2ZmtpeZTTazIjMrNLPjQmeKkZm1Tv48fXP73MyuC50rVmZ2ffLcvsLMJphZZuhMsTKza5PP00r9TMmuqPHDRMwsHVgF9ARKgSXAr9y9IGiwCJlZV6ACGOvuh4fOEzMz2w/Yz92Xmlk94DXgFP1cbc/MDMh29wozqwUsAK5194WBo0XLzH4DtAfqu3v/0HliZmbvA+3d/ePQWWJnZo8D8919lJn9DMhy9/Whc8Us+R5iDdDR3T8InSc2ZtaUxDn9UHf/wsyeAp529zFhk8XHzA4H8oBjgS3AM8Bl7l4cNJhEbU+4onYsUOzu77r7FhIHyS8CZ4qSu78EfBo6Rypw94/cfWny/kagEGgaNlWcPKEi+WGt5K1mrxD9CGbWDOgHjAqdRWoOM2sAdAVGA7j7FhVpu6QH8I6KtJ3KAOqYWQaQBXwYOE+s2gCL3H2zu28F5gGnBc4kkdsTCrWmwOptPi5Fb6ilGplZC+AoYFHYJPFKtvItA8qB59xdz9WO/QW4AagMHSRFODDbzF4zs0tDh4lYS2Ad8FiyrXaUmWWHDpUCzgYmhA4RK3dfA9wDlAAfARvcfXbYVNFaARxvZvuaWRbQFzggcCaJ3J5QqIn8ZMysLjAFuM7dPw+dJ1bu/rW7Hwk0A45NtoDI95hZf6Dc3V8LnSWFdHH3o4E+wJXJFm7ZXgZwNPCgux8FbAK0Z3snku2hA4FJobPEysz2JtGl1BLYH8g2s0FhU8XJ3QuBu4HZJNoelwFfBw0l0dsTCrU1fHfFolnyMZEfJbnfagowzt3/ETpPKki2Wr0A9A6dJVKdgYHJfVd5QHczezJspLglV/Rx93JgKol2d9leKVC6zdXsySQKN9mxPsBSd18bOkjETgLec/d17v4V8A8gN3CmaLn7aHc/xt27Ap+RmKEgskN7QqG2BDjYzFomV8fOBqYHziQpLjkgYzRQ6O73hc4TMzNrZGZ7Je/XITHYpyhsqji5+83u3szdW5A4V811d61O74CZZSeH+ZBs4zuZRHuRfI+7lwGrzax18qEegIYf7dyvUNvjDykBOplZVvJ1sQeJPdvyH5hZ4+SfB5LYnzY+bCKJXUboAD81d99qZlcBzwLpwKPuvjJwrCiZ2QTgBKChmZUCt7v76LCpotUZOA9Yntx7BXCLuz8dMFOs9gMeT05PSwOecneNnZfq8HNgauL9IRnAeHd/JmykqF0NjEsuWr4LXBg4T7SShX9PYEjoLDFz90VmNhlYCmwFXgceDpsqalPMbF/gK+BKDfSRH1Ljx/OLiIiIiIikmj2h9VFERERERCSlqFATERERERGJjAo1ERERERGRyKhQExERERERiYwKNRERERERkcioUBMREREREYmMCjUREREREZHIqFATERERERGJzP8BPc1rQFMxZUwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x518.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "iQbk_UlmcmRR",
        "outputId": "5cc23ca7-d261-4ad4-ed91-295554942c3c"
      },
      "source": [
        "history.describe()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-93856ca0fa1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'describe'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsTZHvtLQIe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d55f914-1e11-4690-a3e6-3634e304dcc0"
      },
      "source": [
        "model3.predict_classes(X_test)[5]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "u-aq-4wmdiGO",
        "outputId": "b8fcc2eb-0877-4ff1-8f37-ab2bc214e86c"
      },
      "source": [
        "#Showing the image\r\n",
        "plt.imshow(X_test[5].reshape(32, 32), cmap = 'gray')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7e5c7036d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO6-w176QL0h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "0e3ed7e1-d8a6-4157-d79c-5df6a44c4e96"
      },
      "source": [
        "#Showing the image\r\n",
        "plt.imshow(X_test[20].reshape(32, 32), cmap = 'gray')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7e5c7a6c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkutb9zcQbS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "767fc87e-d0a1-4543-8495-202311bb0fbe"
      },
      "source": [
        "model3.predict_classes(X_test)[20]"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZOb4VFVQedM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "fab6ca90-40c2-4d64-c513-4f5a2f94187d"
      },
      "source": [
        "plt.imshow(X_test[10].reshape(32, 32), cmap = 'gray')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7e5c72b668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ODXp4b6QgcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325ae4ad-e651-4e6f-ed12-49137cd1900e"
      },
      "source": [
        "model3.predict_classes(X_test)[10]"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NplvtTtzQljc"
      },
      "source": [
        "**Conclusion**\r\n",
        "Evaluated the accuracy using two methods i.e. baby sitting the NN and NN through API. Followed all the required steps starting with loading the datasets to performing hyperparameter optimization and running a finer search by using a finer range. Explored different options in optimizers, number of activators, learning rate and activation methods in NN through API. Found that baby sitting process achieved the best accuracy of 21% using hyper parameter optimization. It might have been further improved but that's the trade off vs time taken to run the script. NN through API method achieved best accuracy of 90% on validation set. Also printed the classification report, visualized the confusion matrix and summarized history for accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--8hHdq7Qk6U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}